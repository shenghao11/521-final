Query,Key
What is NLP?,NLP stands for Natural Language Processing. It's a field of artificial intelligence that focuses on teaching machines to understand human language.
What are some applications of NLP?,"NLP has many practical applications, including speech recognition, sentiment analysis, language translation, and chatbots."
What is the difference between NLP and AI?,NLP is a subfield of AI that focuses on teaching machines to understand and process human language.
What is a chatbot?,"A chatbot is an AI program designed to simulate conversation with human users, often for customer service or support purposes."
What is sentiment analysis?,"Sentiment analysis is the process of using NLP techniques to identify and extract subjective information from text, such as opinions, attitudes, and emotions."
What is machine translation?,Machine translation is the process of using NLP algorithms to automatically translate text from one language to another.
What is named entity recognition?,"Named entity recognition is the process of identifying and classifying specific entities in text, such as names of people, organizations, or locations."
What is the difference between syntax and semantics?,"Syntax refers to the rules governing the structure of language, while semantics refers to the meaning of language."
What is a corpus?,"A corpus is a large, structured collection of text used for linguistic analysis and research."
What is a part-of-speech tagger?,"A part-of-speech tagger is an NLP tool that assigns a grammatical category (such as noun, verb, or adjective) to each word in a sentence."
What is text normalization?,"Text normalization is the process of converting text to a standardized format, such as converting all text to lowercase or removing punctuation."
What is lemmatization?,"Lemmatization is the process of reducing a word to its base or dictionary form (such as converting ""running"" to ""run"")."
What is stemming?,"Stemming is the process of reducing a word to its base or root form (such as converting ""running"" to ""run"")."
What is text classification?,"Text classification is the process of assigning categories or labels to text, such as classifying news articles by topic."
What is a named entity?,"A named entity is a specific object, person, or location referred to in text, such as ""New York City"" or ""Barack Obama""."
What is topic modeling?,Topic modeling is an NLP technique that identifies the underlying themes or topics present in a collection of text.
What is sentiment classification?,"Sentiment classification is the process of determining the sentiment or emotion expressed in text, such as identifying whether a review is positive or negative."
What is word sense disambiguation?,"Word sense disambiguation is the process of identifying the correct meaning of a word with multiple meanings, such as ""bank"" (which can refer to a financial institution or the side of a river)."
What is a dependency parser?,A dependency parser is an NLP tool that identifies the grammatical relationships between words in a sentence.
What is chunking?,Chunking is the process of grouping together adjacent words in a sentence based on their part of speech or other linguistic features.
What is a co-reference resolution?,"Co-reference resolution is the process of identifying when two or more words or phrases in a sentence refer to the same entity, such as ""he"" and ""John""."
What is a tokenization?,Tokenization is the process of breaking a sentence or text into individual words or tokens.
What is a stopword?,"A stopword is a common word (such as ""the"" or ""and"") that is often removed from text during preprocessing."
What is a vector representation of words?,A vector representation of words is a mathematical representation of words that allows NLP algorithms to perform mathematical operations on text.
What is a word embedding?,A word embedding is a type of vector representation of words that is learned from a large corpus of text using deep learning techniques.
What is a bigram?,"A bigram is a pair of adjacent words in a sentence, often used in NLP for language modeling or text analysis."
What is a trigram?,"A trigram is a sequence of three adjacent words in a sentence, often used in NLP for language modeling or text analysis."
What is perplexity in NLP?,Perplexity is a metric used to measure the effectiveness of a language model in predicting the likelihood of a sequence of words.
What is a language model evaluation metric?,"A language model evaluation metric is a way of quantitatively measuring the performance of a language model, such as accuracy or F1 score."
What is a bag-of-words model?,"A bag-of-words model is an NLP technique that represents text as a set of individual words, ignoring grammar and word order."
What is a n-gram model?,An n-gram model is an NLP technique that uses sequences of n adjacent words (such as bigrams or trigrams) to model language.
What is a language generation model?,"A language generation model is an NLP tool that generates text based on a given input, such as a prompt or a set of keywords."
What is a text similarity measure?,"A text similarity measure is a metric used to compare the similarity or dissimilarity of two pieces of text, often used in NLP for information retrieval or recommendation systems."
What is a text summarization tool?,"A text summarization tool is an NLP tool that summarizes a longer piece of text into a shorter summary, often used in news or article summarization."
What is named entity linking?,"Named entity linking is the process of connecting named entities (such as ""New York City"" or ""Barack Obama"") to their corresponding knowledge bases, such as Wikipedia or Freebase."
What activation function is used for the forget gate of the LSTM,Sigmod Function
What is TF-IDF?,"TFIDF or Term Frequency-Inverse Document Frequency indicates the importance of a word in a set. It helps in information retrieval with numerical statistics. For a specific document, TF-IDF shows a frequency that helps identify the keywords in a document. The major use of TF-IDF in NLP is the extraction of useful information from crucial documents by statistical data. It is ideally used to classify and summarize the text in documents and filter out stop words."
Explain stop words in NLP,"Stop words in NLP are those that text processing removes. Articles and prepositions are examples of stop words. Removing stop words is a typical pre-processing step in NLP, which even search engines follow. Some common stop words are a, an, the, why, how, am, is and at."
Explain the term frequency-inverse document frequency,"TF-IDF helps ascertain the importance of a word in a data set. In NLP, TF-IDF indicates the frequency of different keywords, which helps extract, classify and summarise the text. It also helps remove stop words. When the TF-IDF is high, the frequency of the term is low."
What is the NLG Natural Language Generation,"Natural Language Generation is a part of AI and generates natural language texts from structured data to produce an output. It can be seen as NLPÈà•Ê™ö reverse process, where NLP is used to understand and interpret the natural language to form data, and NLU is used to generate outputs in natural language from structured data."
What is the order of steps in natural language understanding,The order of steps that are to be followed in Natural Language Understanding is as follows:
List any real-world application of NLP,"The most used real-world application of NLP is speech recognition. Examples of speech recognition applications are Amazon Alexa, Google Assistant, Siri, HP Cortana."
What are the tools used for training NLP models,"The tools used to train NLP models are NLTK, spaCY, PyTorch-NLP, openNLP."
List the models to reduce the dimensionality of data,"The commonly used models are TF-IDF, Word2vec/Glove, LSI, Topic Modelling, Elmo Embeddings."
List some open-source libraries for NLP,"The popular libraries are NLTK (Natural Language ToolKit), SciKit Learn, Textblob, CoreNLP, spaCY, Gensim."
Explain the masked language model,"Masked modeling is an example of autoencoding language modeling. Here the output is predicted from corrupted input. By this model, we can predict the word from other words present in the sentences."
What is the bag of words model,"The Bagofwords model is used for information retrieval. Here the text is represented as a multiset, i.e., a bag of words. We don‚Äôt consider grammar and word order, but we surely maintain the multiplicity."
What is CBOW,"CBOW or continuous bag of words is a model that tries to predict the target word from the available source context words, i.e., the surrounding words. Here the context words are taken into account as multiple words for a given target word."
What is TF-IDF and what are its uses,TF-IDF is an abbreviation for the term frequency-inverse documentary frequency. It is used to provide a numeric value to a word to show how important it is in the document or a corpus.
What are POS and tagging,"Parts Of Speech (POS) are the functions of the word, like a noun, verb, etc., and tagging is labeling the words present in the sentences into different parts of speech."
What is n-gram,N-grams are the continuous sequence (similar to the power set in number theory) of n-tokens of a given text.
What is skip-gram,Skip gram is an unsupervised learning technique used to find the most related words to a target word. It is a reverse process of the continuous bag of words model.
What is the corpus,"Corpus or corpora (plural), is a collection of the text of a similar type, for example, movie reviews, social media posts, etc."
What is normalization,"Normalization is the process of mapping similar terms to a canonical form, i.e., a single entity."
What is keyword normalization,Keyword normalization is an NLP technique where we apply normalization on a word to condense it to its most basic form.
What is lemmatization,"Lemmatization is a type of normalization used to group similar terms to their base form-based on the parts of speech. For example, talking and talking can be mapped to a single term, walk."
What is stemming,"Stemming in NLP is also a type of normalization and is similar to lemmatization, but the difference here is that it segregates the words without the parts of speech tags. It is faster than lemmatization and can also be more accurate in some cases."
What is ambiguity,"Ambiguity can be referred to as a condition when a word can have multiple interpretations and results in being misunderstood. Natural languages are ambiguous and can make it difficult to process NLP techniques on them, resulting in the wrong output."
What is tokenization,Tokenization is the process of breaking down large sets of text into small parts for easy readability and understanding. Each small part is referred to as Èà•Ê¶ØextÈà•?and provides a piece of meaningful information.
What are stop words,"Stop words are the unwanted text that is present in the input. It is the process of removal of unwanted text from further processing of text, for example, a, to, can, etc."
How to find word similarity,Word similarity in NLP is done by calculating the word vectors of the words in the vector space and then calculating the similarity on a scale of 0 to 1.
How to find sentence similarity,Sentence similarity is done in NLP by finding the cosine similarity between the two sentences. It can be done by finding the cosine angle between the vectors of two sentences in the inner product space.
How to find document similarity,Document similarity is done in NLP by converting the documents into the TF-IDF vectors form and finding their cosine similarity.
What are transformers,Transformers are deep learning architectures that can parallelize computations. They are used to learn long-term dependencies.
What is named entity recognition NER,"Named Entity Recognition is a part of information retrieval, a method to locate and classify the entities present in the unstructured data provided and convert them into predefined categories."
What is NLTK,"NTLK, an abbreviation of Natural Language Toolkit, is one of NLP most popular libraries. It was written in Python and contained libraries for tokenization, classification, tagging, stemming, parsing, and semantic reasoning."
What is spaCY,spaCY is an open-source library for natural language processing on an advanced level. It is mostly used for production-level usage and uses convolutional neural network models.
What is openNLP,"openNLP is a java based library used for Natural Language Processing, and it supports most of the NLP tasks such as tokenization, language detection, etc."
What is the difference between NLTK and openNLP,"There is a small difference between NTLK and openNLP, i.e., NLTK is written in python, and openNLP is based on java. One other difference is that NTLK has an option of downloading corpora by an in-built method."
What is parsing,Parsing is the method of analyzing the sentence automatically based on the syntactic structure.
What is dependency parsing,"Dependency parsing, also called syntactic parsing, recognizes a dependency parse of a sentence and assigns a syntax structure to a sentence. It focuses on the relationship between different words."
What is semantic parsing,Semantic parsing is a method of conversion of natural language into machine-understandable form.
What is constituency parsing,Constituency parsing is a method of division of sentences into sub-parts or constituencies. It aims to extract a constituency-based parse tree from the constituencies of the sentences.
What is shallow parsing,"Shallow parsing, also known as light parsing and chunking, identifies constituents of sentences and then links them to different groups of grammatical meanings."
What are the differences between dependency parsing and shallow parsing,"The difference between shallow parsing and dependency parsing is that shallow parsing is the parsing of limited parts of the information. In contrast, dependency parsing is the process of finding relations between all the different words."
What is language modeling,Language modeling is the process of creating a probability distribution of a sequence of words. It is used to provide probability to all the words present in the sequence.
What is topic modeling,Topic modeling is a method of finding abstract topics in a document or set of documents to find hidden semantic structures.
What is text summarization,"Text summarization in NLP is the process of conversion of large pieces of text to short text. It is intended to summarize the given text, keeping the main contents and overall meaning intact."
What is perplexity,Perplexity is the condition when the system encounters something unaccountable or which is not meaningful.
What is the Naive Bayes algorithm and where is it used,Naive Bayes algorithm is used to predict tags of text by calculating the probability for each tag for the text and then providing the one with the highest probability.
What is noise removal,"Noise removal is one of the NLP techniques i.e., used to remove pieces of text from the corpus that is not necessary as they can hinder our text analysis."
What is word embedding,Word embedding is the process of mapping words from the vocabulary to vectors of real numbers.
What are the word embedding libraries,The libraries that provide word embedding features are spaCY and genism.
What is word2vec,Word2vec is a collection of models that are used to generate word embeddings. These models are trained to reconstruct the linguistic context of the words in the corpus.
What is doc2vec,Doc2vec is one of the unsupervised algorithms used to generate vectors of sentences or documents irrespective of their length.
What is a document-term matrix,"The document-term matrix, also called the term-document matrix, is the matrix that describes the frequency of terms occurring in a document."
What is wordnet,Wordnet can be described as a database created to store words from different languages connected by their semantic relationships.
What is GloVe,The gloVe based on their pronunciation.
What is a flexible string matching,Flexible string matching or fuzzy string matching is a method to find strings that are likely to match a specific pattern. It is also called approximate string matching as it uses an approximation to find patterns between strings.
What is cosine similarity,Cosine similarity is the measure of cosine difference between two non-zero vectors in the inner product space. It is used to find the similarity between documents irrespective of their size.
What is information extraction,Information extraction is the process of extracting useful data in a structured way from a given unstructured set of data.
What is object standardization and when is it used,Object standardization is the process of extracting useful information from abbreviations and other acronyms that can not be meaningful in lexical dictionaries.
What is text generation and when is it done,Text-generation is the process of generating natural language texts automatically in response to the communication. It uses artificial intelligence and computational linguistic knowledge to perform this task.
How can we estimate the entropy of the English language,N-grams can estimate the entropy of the English language. The entropy of a letter is calculated by knowing the entropy of all the previous letters.
What is Latent Dirichlet Allocation,"Latent Dirichlet Allocation is a topic modeling method where each topic represents a set of words, and every document is made of various words."
What is PAC learning,Probably Approximately Correct learning is a mathematical analysis framework. It is used for the analysis of generalization error of the learning algorithms.
What is sequence learning,Sequence learning is a method of learning where both input and output are sequences.
What is an ensemble method,The ensemble method uses multiple learning algorithms to get enhanced and more accurate performance compared to the performance of an algorithm alone.
What is vanishing gradient problem,"In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, during each iteration of training each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0,1], and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the early layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the early layers train very slowly."
How to solve the vanishing gradient problem,"Avoid using activation functions that have very low or zero derivatives, such as the sigmoid function. Instead, consider using activation functions that have a wider range of derivatives, such as the ReLU function or its variants."
This kind of probability considers an event after another has occurred.,conditional probability
"In traditional statistical modeling, the basic strategy we use to estimate probabilities is this.",counting
The three axioms of probability is this,The three axioms of probability are: (1) Probabilities are nonnegative. (2) Probabilities are additive (3) Probabilities sum to 1 over their sample space
Zipf's Law is relevant in NLP because of this/these reason(s).,What is: Zipf's Law is an empirical finding about the frequency distribution of words in language. It's relevant in NLP because it tells us that: (1) many words occur with low frequency (hard to estimate statistics) (2) words with low frequency can be more informative than those with highfrequency.
"As we increase the value of the smoothing hyperparameter in Lidstone sommthing, this is what happens in terms of the bias-variance tradeoff.",What is increase the bias of our model?
The name of the type of distribution we wish to find when we use Bayes Theorem is this.,posterior distribution
This performance measure can be misleading when working with unbalanced data,accuracy
This class of words often carries stylistic information because its use has been observed to be stable across documents for a particular author,function words
we use smoothing in Naive Bayes for this reason.,What is to deal with avoiding zero probabilities because of having seen words in training data that do not appear in testing conditioned on a class?
"This is why we call an inverted index ""inverted""","What is because the relationship between document and term is inverted: given a term, what documents are associated with it?"
"Thsi term weighting approach improves on TF-IDF weightings by accounting for document lengths, for instance.",BM25
These are the two data structures we use for a basc inverted index,vocabulary and inverted file
This kind of retrieval model would be a great choice if we know exactly the terms that appear in relevant documents.,boolean retrieval model
This is the name of the algorithm we use to find the parameters of IBM Model 1.,Expectation Maximization
These are the parameters of the IBM Model 1 approach,translation table probabilities
We decompose directly modeling p(e|f) into these two subproblems.,language modeling and translation modeling
"Given n English target words nd m Russian source words, this is the total number of possible alignments.",(m+1)^n
This is the difference between sparse and dense vectors,"What is sparse vectors are mostly zero-valued, and dense vectors are mostly non-zero-valued."
This is the name of the probability function used to model p(context words | center words) in the Skip-gram model.,softmax function
This is a strategy we can use to speed up training the Skip-gram model by only updating some context word representations,negative sampling
This is an assumption made by a feedforward network that is not appropriate for processing text.,What is the assumption that the order of the words does not matter?
Understand the interpretability of word embeddings,"""Interpretable"" is not very precise in this context.

In the case of deleting a dense layer, the embedding layer is more likely can learn the nontask dependent co-occurrences of words in the dataset.

In the second case of adding more data, the embedding layer would learn more signals because there is an increased opportunity to ""average out"" the noise.

In other words, word embeddings are more generalizable by reducing the complexity of the architecture and training on more data."
How should I evaluate writing quality to compare two articles(which article is better suited/written for a topic ) according to their content?,"""Source credibility"" of Internet articles is best calculated through the Page Rank algorithm.

Algorithmically determining writing quality might be intractable. However Page Rank could be a proxy. If an article is a hub then it is the authority on the topic and can be assumed well written (or at least very useful)."
Group unstructured chat logs into conversations,"""The end goal would be to take the discussion from the beginning of time to the start of time and be able to isolate different chunks of text into a logical conversation group.""

I would start by clarifying this. A 'conversation' on a messaging app can be defined in a million ways. The day ended and people left the app after a sustained discussion, or the users changed the topic... but maybe came back to it later. One could argue that one conversation = one thread but the way people use threads might depend on the server.

If you don't have a definition yet, I think a conversation could be defined by some threshold that was crossed in terms of time elapsed since the last message and or change in topic. For the former, you have the data and I would look at the distribution of time between messages. It's unlikely that your distribution will be bimodal, but if it is, it might be useful.

For the latter, it will be harder but you could use embeddings (for each message) from a pre-trained model, and look at the distribution of cosine similarity between consecutive messages (or over some window) and find when that value hopefully drops, due to a topic change. You might have to filter out short messages that are very generic and build some attribution logic to assign them back to the conversation later.

Mostly speculating for this one, but maybe another way would be to use a Transformer model that can handle large chunks of text, spanning multiple messages, and look at how the attention moves as you feed the text into the model with a rolling window. I wonder if when the window is across conversations, you can see some sort of clustering."
Survey papers on Natural Language Understanding,"‚ÄúGrammar as a foreign language‚Äù (2015) by Vinyals et al. is not a review paper but is a significant breakthrough by applying sequence-to-sequence deep learning.

""Advances in natural language processing"" (2015) by Hirschberg and Manning provides a general overview, including parsing.

""Recent Trends in Deep Learning Based Natural Language Processing"" (2018) by Young et al. covers popular deep learning approaches and lists current benchmarks for parsing."
Categorizing Customer Emails,"(1) Data quality. The single best way to improve your accuracy. Garbage in garbage out. You already said your data is suspect. Some data was mis-classified; data only has single label, when multiple labels are possible. This is the biggie - this will improve your accuracy more than any other technique: improve the quality of your training data. One way to go about this. Recruit expert labellers (these could be workers in your own company who do this task, or they could be external people, e.g., mturkers who have been trained to do this labelling task). In general, the more labellers, the better. At a minimum I would have 3 labellers, label all n emails in your train/test set, using either majority label wins, or require all-three consensus depending on your confidence needs. You will want to measure your inter-rater reliability to ensure the integrity of your data using Cohen's kappa or similar. Importantly, this data will become your gold standard data. That is to say the average agreement upon labels provided by your original labellers is the ground truth to which your algorithm attempts to approach.

Obviously, this approach to data quality is costly in terms of resources (time to label all the emails; cost of hiring mturkers, etc.). But depending upon your specific needs it needn't be... basically it depends upon the needed level of ""expertise"" to do the task. Sometimes, it is safe to consider you (the researcher/developer) as the ""expert"" (e.g., you yourself said that some emails were mis-classified... how do you know that? You must have some sort of expert knowledge.). However, using a single labeller for the task makes your data less reliable due to individual processing differences introducing a bias into the data.

(2) Including stop words. Yes, you read that correct including stop words. Grammatical function words (e.g., the, and, of, etc.) can often be highly indicative of specific text types or genres. It is premature to eliminate stop words without first ensuring that they are an uninformative feature. Similarly, try including punctuation, try including capitalization features. Time and time again, I see people blindly pre-processing text without empirically determining whether these features are significant predictors of class.

(3) Capture spelling variation. As the OP stated, the original texts have many ""spelling mistakes."" You can improve accuracy by reducing this variance - basically map variants to their canonical form before extracting your word features (e.g., ""colour"" -> ""color""). I often rely on external corpora to rapidly create these spelling maps.

(4) Balance your training data - The OP mentions that his classes are severely imbalanced. Try over- and/or under-sampling to balance your classes.

(5) Describe your problem. It is also important to understand what the underlying purpose of your classifier. If your classifier is intended to run on historical data, perhaps to flag incorrectly human-classified emails, then you would probably want to keep auto-response and correspondence thread info. On the other hand, if the goal is to classify new customer emails so that they get properly routed, for example, then you probably want to remove these correspondence features from your training and test data."
BPE vs WordPiece Tokenization - when to use / which?,"(This answer was originally a comment)

You can find the algorithmic difference here. In practical terms, their main difference is that BPE places the @@ at the end of tokens while wordpieces place the ## at the beginning. The main performance difference usually comes not from the algorithm, but the specific implementation, e.g. sentencepiece offers a very fast C++ implementation of BPE. You can find fast Rust implementations of both in Hugginface's tokenizers."
How to improve accuracy of Named entity recognition (NER) tagger on local data?,"@Ravikm, excellent question. In Spacy, you can assign a word manually. For example, ""Tesla"" to ORG. Source: screenshot from Jose Portilla's NLP course on Udemy."
Features of word vectors in Word2Vec,"1- The number of features: In terms of neural network model it represents the number of neurons in the projection(hidden) layer. As the projection layer is built upon distributional hypothesis, numerical vector for each word signifies it's relation with its context words.

These features are learnt by the neural network as this is unsupervised method. Each vector has several set of semantic characteristics. For instance, let's take the classical example, V(King) -V(man) + V(Women) ~ V(Queen) and each word represented by 300-d vector. V(King) will have semantic characteristics of Royality, kingdom, masculinity, human in the vector in a certain order. V(man) will have masculinity, human, work in a certain order. Thus when V(King)-V(Man) is done, masculinity,human characteristics will get nullified and when added with V(Women) which having femininity, human characteristics will be added thus resulting in a vector much similar to V(Queen). The interesting thing is, these characteristics are encoded in the vector in a certain order so that numerical computations such as addition, subtraction works perfectly. This is due to the nature of unsupervised learning method in neural network.

2- There are two approximation algorithms. Hierarchical softmax and negative sampling. When the sample parameter is given, it takes negative sampling. In case of hierarchical softmax, for each word vector its context words are given positive outputs and all other words in vocabulary are given negative outputs. The issue of time complexity is resolved by negative sampling. As in negative sampling, rather than the whole vocabulary, only a sampled part of vocabulary is given negative outputs and the vectors are trained which is so much faster than former method."
What are the exact differences between Word Embedding and Word Vectorization?,"A ‚Äûbag of words‚Äú usually describes encoding of text where one word (or ngram) is represented as one variable (column). This can be done as binary encoding or as count of words, often called one-hot encoding. Alternatively, you can introduce weights to represent the frequency of words in a document, such as TFIDF. See also here for a sklearn implementation. Hashing essentially is a ‚Äûbag of words‚Äú using the hashing trick to cope with previously unseen words in a corpus and a large (or growing) corpus.

In word2vec, each word is represented by a vector, which indicates how close one word is to another (this is the result of a pre-trained model). You can use a pre-trained word2vec model and assess the proximity of words by comparing two (word) vectors e.g. based on the Euclidean distance. These vectors help models to better understand the (semantic) structure of some text via understanding the empirical co-occurance of words (which is not possible with one-hot encoding etc.)

BERT goes even one step further. In BERT pre-training a word in a sentence is ‚Äûmasked‚Äú, where the model tries to predict the masked word in a sentence. Also ‚Äûnext sentence prediction‚Äú is used to pre-train BERT models. By doing so, BERT has a even better ability to understand semantic relations in a text."
"NLP Question: Where can I find a list of all open compound words (with spaces in them), like ""peanut butter"" or ""high school""? [close","A common way to frame this type of task is as collocation extraction, i.e., extracting bigrams or trigrams and then filtering them by the mutual information between their constituent tokens (this filtering solves the issue of noise you described when talking about the Google n-gram dataset). You can find more information on collocation extraction using nltk here.

Note, though, that choosing the right mutual information cut-off threshold to separate collocations from non-collocations (=n-grams) is not trivial, will require tweaking, and you will probably never be fully happy with the results :D

Generally speaking, you want it to be as high as possible, as long as that doesn't cause too many candidates to be discarded. You will probably only manage to keep about 5%-20%, which may or may not be enough for your particular needs. Note, though, that some equally-highly-collocated non-compound-noise will persist, e.g. n-grams containing verbs, phrasal verbs, some frequent adverbial phrases, and so on.

So, the options below are probably better, either on their own or combined with the information-theoretical approach (they are not mutually exclusive, for the most part):

Part of speech tagging: Run a part-of-speech tagger on your data and retain only candidate n-grams whose tokens are tagged as sequences of nouns or adjectives (NN(S) and JJ tags), or where the n-gram, at the very least, starts and ends with one such part of speech (if you're really interested only in compounds, then I think that you'll be fine with sequences like {(""NN"", ""NN""), (""NN"", ""NNS""), ...} only). With this additional condition, you could lower the mutual information threshold substantially and still get informative terms.

Stopword removal: t's always easier to compile a list of all stopwords (or use/extend nltk's) than a list of all compounds, so you could also try generating all n-grams, then filtering those that i) only contain stopwords, ii) start with one, iii) or end with one. You would still end up with collocations containing verbs, so this option is not as good as #1, but it might be a working compromise.

Using gensim's phrases phrase extractor. The documentation is great so I won't expand on it here.

""Good old scraping"": download a knowledge base, e.g. a dump of Wiktionary (https://en.wiktionary.org/wiki/peanut_butter) and parse it to get all dictionary lemmas a) that contain an underscore (=blank) and b) at least one dictionary entry listed as a Noun. In this way, you can filter out verbs, while at the same time validating any n-gram hypothesis as a noun against the external ground truth. Of course, this approach is limited to the coverage of the ground truth dataset (at least initially), but this may already be enough for your purposes.

For a comprehensive overview, motivation and how-to of methods #1 and #3, you can check this excellent post: https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf

Hope any of this helps!"
What is the best approach to deploy N number of ML models as a scalable service in the Cloud?,"A couple of ideas:

Reduce the number of models.
Reduce the size of the models through distillation, quantization, and pruning.
Reduce the size of the machine type within the cluster.
Confirm the system downscales to zero when not being used."
Topic classification on text data with no/few labels,"A feasible approach would be to take a pre-trained model, like BERT, and fine-tune it on a small labeled dataset.

For that, you may use Huggingface's Transformers, which makes all the steps in the process relatively easy (see their tutorial on doing exactly that: https://huggingface.co/docs/transformers/training)"
How to know the state-of-the-art recommended approaches for data science?,"A few comments:

There is no scientific domain of ""data science"", instead there are multiple fields which are related to data science: statistics, ML, NLP, computer vision, signal processing... and a lot of other fields which overlap and/or focus on specific applications, for instance bioinformatics. All of these domains are highly active and specialized, so it would just be impossible to monitor every possible advance.
There is no unique recommended way: first, people disagree all the time about the best way to do X. Second, it's very rare that a method would become completely obsolete. For example TFIDF still makes sense in many use cases, with low-resource languages or when there are efficiency constraints for instance.
In order to comprehensively follow the state of the art, one would have to follow the research publications. At best it's doable for a specific domain, for example one can more or less get an idea of what happens in NLP by browsing through the main conferences. A more realistic option is to wait for the advances to reach the mainstream professionals, for example by browsing regularly through DataScienceSE and/or CrossValidated.
Final suggestion: old books are very useful to fully understand why/how things are done a certain way. We often see errors here on DataScienceSE which are due to people trying to apply methods without understanding them."
Dataset - Sample pdfs for text processing?,"A GitHub repo with ~1,000 pdfs is here.

Another GitHub repo has a corpus of pdf examples, including edge-cases, is here."
Agglomerative Clustering without knowing number of clusters,"A minimum cluster size will not generally be satisfiable in hierarchical clustering. Instead, you have to expect many clusters with just a single point.

ELKI has some fairly interesting techniques to cut a dendrogram. Check the clustering.hierarchical.extraction (or so) package. If I remember correctly, some allow you to set a minimum size (but there will be a ""noise"" cluster with all the leftovers)."
N-grams for RNNs,"A neural language model tries to predict a conditional probability 
P(
w
i+1
|
w
1
,‚Ä¶,
w
i
)
ùëÉ
(
ùë§
ùëñ
+
1
|
ùë§
1
,
‚Ä¶
,
ùë§
ùëñ
)
. It approximates the probability with the following 
P(
w
i+1
|s(
w
1
,‚Ä¶,
w
i
))
ùëÉ
(
ùë§
ùëñ
+
1
|
ùë†
(
ùë§
1
,
‚Ä¶
,
ùë§
ùëñ
)
)
, where 
s
ùë†
 is a state function. After that an LSTM looked at all the words 
w
1
,‚Ä¶,
w
i
ùë§
1
,
‚Ä¶
,
ùë§
ùëñ
, it has an updated state, so now it contains some useful information about all previous words. You've got an error in your code: you should take all words of a sentence, but the last. But you've taken all, but the last sentence.

In language modeling a normal sentence 
w
i
,‚Ä¶
w
n
ùë§
ùëñ
,
‚Ä¶
ùë§
ùëõ
 is usually augmented with 2 special tokens: -- begin of sequence, -- end of sequence. So your example ""Hello my name is"" should transform into "" Hello my name is "". Now your source tokens are all except the last i.e. "" Hello my name is"" and the targets you want to predict are all expect the first i.e. ""Hello my name is "". You feed tokens in your LSTM one at a time and try to predict the next token."
Neural Networks for Predictive typing,"A neural network is in principle a good choice when you have A LOT of similar data and classification tasks. Predicting the next character (or word... which is just multiple characters) is such a szenario. I don't think it really matters which kind of language you have, as long as you have enough training data of the same kind.

See The Unreasonable Effectiveness of Recurrent Neural Networks for a nice article where a recurrent neural network (RNN) was used as a character predictor to write complete texts. They also have code on github.com/karpathy/char-rnn ready to train / go. You can feed it with a start string and ask for the next characters / words."
Is it possible to use word2Vec to derive hyperonymy (hyponymy or ISA relation)?,"A paper entitled ""Dependency-Based Word Embeddings"" found that word embeddings that defined context using dependency-based parsing generated cohyponyms. For example, ""florida"" was embedding near ""texas"", ""louisiana"", ""georgia"", ""california"", and ""carolina""."
Topic modeling for short length sentences,"A possible approach might be to use the most predictable and predictive word(s) in each cluster as its name(s). The following is inspired by Fisher's category utility used by the COBWEB algorithm.

An attribute is highly predictable in a cluster 
C
l
ùê∂
ùëô
 if most elements in that cluster have the same value 
a
ùëé
 for it, thus a word represented by attribute 
A
i
ùê¥
ùëñ
 has a predictability of 
P(
A
i
=a|
C
l
)
ùëÉ
(
ùê¥
ùëñ
=
ùëé
|
ùê∂
ùëô
)

An attribute is highly predictive for a cluster 
C
l
ùê∂
ùëô
 if knowing it's value 
a
ùëé
 implies you can say with high certainty to which cluster it belongs, expressed as 
P(
C
l
|
A
i
=a)
ùëÉ
(
ùê∂
ùëô
|
ùê¥
ùëñ
=
ùëé
)

Assume now you process each label org.java.somepackage.validateLogin as a sentence: ""org java somepackage validate Login"" and apply one-hot encoding to all your sentences in the dataset. The occurrence of a word is now represented by a value of 1 for it's corresponding attribute.

The task of representing each cluster by a word can now be formulated as finding for each cluster 
C
l
ùê∂
ùëô
 the word represented by attribute 
A
i
ùê¥
ùëñ
 that equals 1 and has the highest predictability and predictiveness, weighted by the total probability of this word being in a sentence.

P(
A
i
=1|
C
l
)√óP(
C
l
|
A
i
=1)√óP(
A
i
=1)
ùëÉ
(
ùê¥
ùëñ
=
1
|
ùê∂
ùëô
)
√ó
ùëÉ
(
ùê∂
ùëô
|
ùê¥
ùëñ
=
1
)
√ó
ùëÉ
(
ùê¥
ùëñ
=
1
)

Which can be calculated by counting occurrence of a word per cluster and cluster per word."
How can I run Labeled LDA over one textual document?,A python package that you can check. It's an online max-margin topic model.
Does Fasttext use One Hot Encoding?,"A quick answer would be No.

Let's walk through how FastText works internally:

For representation purposes, FastText internally initializes a dictionary. Dictionary contains all the collection of words. Besides words, it also maintains the count of all the words in the dictionary (and other information). Every time a new word is added to the dictionary its size is increased and word2int_ is updated to size_(which is increased after assignment).

The code below adds a word into the dictionary.

// adding of new word
void Dictionary::add(const std::string& w) {
  int32_t h = find(w);
  ntokens_++;
  if (word2int_[h] == -1) {
    entry e;
    e.word = w;
    e.count = 1;
    e.type = getType(w);
    words_.push_back(e);
    word2int_[h] = size_++; //  word2int_[h] is assigned a uniuqe value 
  } else {
    words_[word2int_[h]].count++; // word's count is being updated here
  }
}

// funciton used to access word ID (which is the representation used)
int32_t Dictionary::getId(const std::string& w) const {
  int32_t h = find(w);
  return word2int_[h];
}



As mentioned in this medium article, word2int_ is indexed on the hash of the word string, and stores a sequential int index to the words_ array. The maximum size of word2int_ vector can be 30000000.

For embeddings matrix of M x N is created where M = MAX_VOCAB_SIZE + bucket_size. Where, M is total vocabulary size including bucket_size corresponds to the total size of array allocated for all the n-gram tokens and N is the dimension of the embedding vector, which means the representation for one word requires the size of 1.

The code below shows how to compute the hash and calculates the ID of the subword. A similar logic is used to access the subword vector. Note here h is an integer value which is calculated using dict_->hash(). This function returns the same h value which is used when adding a word in the dictionary. This makes the process of accessing word IDs only dependent on the value of h.


int32_t FastText::getSubwordId(const std::string& subword) const {
  int32_t h = dict_->hash(subword) % args_->bucket;
  return dict_->nwords() + h;
}

void FastText::getSubwordVector(Vector& vec, const std::string& subword) const {
  vec.zero();
  int32_t h = dict_->hash(subword) % args_->bucket;
  h = h + dict_->nwords();
  addInputVector(vec, h);
}



Long story short, FastText makes use of integer IDs assigned at the beginning and use those IDs to access the embeddings.

I hope this helps. All the code samples are taken from FastText repo. Feel free to dive in to understand more."
Preprocessing for Text Classification in Transformer Models (BERT variants),"A quick experiment you can do is to once do the preprocessing steps that you usually do and then feed it to the model and get the results. And once feed the dataset as it is to the model to compare the difference.

In my experience doing the preprocessing won't make any difference, based on the dataset it gave me 1 more or less percent difference in accuracy (not a considerable change).

When these models are trained, no preprocessing is done, as they want to learn the context of all sorts of sentences.

A reason that your results are not good enough might because of your label distribution. Most of the time the datasets are populated with only one or two labels and other labels are only a small portion of the dataset. If that's the case you might want to look into oversampling solutions."
Classifying job titles,"A reasonable assumption to make, is that titles that share indicative words, are in the same sector.

Examples:

Positive example: ""senior IOS developer"" and ""principal web developer"" seem to be both in the IT sector, and the word ""developer"" is our giveaway.
Negative example: ""chief operations officer"" and ""chief data officer"" share 2 words, but those words (chief,officer) are not indicative

So, I would maintain a list of non-indicative words, such as ""senior"",""chief"",...

Filter them out, and then apply an hierarchical clustering algorithm on the word-distance between titles."
Extract key phrases from a single document,"A related keyword to your case can be Single Document Keyword Extraction. A good paper about this is:

We present a new keyword extraction algorithm that applies to a single document without using a corpus. Frequent terms are extracted first, then a set of cooccurrence between each term and the frequent terms, i.e., occurrences in the same sentences, is generated. Co-occurrence distribution shows importance of a term in the document as follows. If probability distribution of co-occurrence between term a and the frequent terms is biased to a particular subset of frequent terms, then term a is likely to be a keyword. The degree of biases of distribution is measured by the 
œá
2
ùúí
2
-measure. Our algorithm shows comparable performance to tfidf without using a corpus.

You can find the paper here.

In sum, this paper gives a rank on keywords based on the defined 
œá
2
ùúí
2
-measure."
How to implement your own word list for sentiment analysis?,"A relatively straightforward approach is counting up the number of occurrences of positive and negative words. Then see which tally is larger.

Creating a sentiment word list dataframe is not the best approach. Dataframe are large in memory and strings are large in memory. It is better to create a hash table, aka Python dict, to reduce the data size in memory.

The most common options for assigning positive and negative metric are:

Use an existing metric, such as TextBlob's sentiment.
Assign your own metric score."
Is summing a cosine similarity matrix a good way to determine overall similarity?,"A similar but more advanced approach would be BERTScore. It also computes pairwise cosine similarity between (BERT) embeddings but uses greedy matching by only accounting for similarity between the closest tokens:  (based on Figure 1 from the BERTScore paper)

However, it should be noted that BERTScore is designed to be used for paragraphs and not documents.

Another more traditional approach would be doc2vec."
Non-brute force approach to finding permissible English word anagrams,"A simple solution is store the words in a dictionary (since you have to store them in some data structure) whose key is the character distribution; e.g., Counter(""hello"") = {h: 1, e: 1, l: 2, o: 1}. Querying the dictionary would give you the anagrams.

For storing the immutable key (character distribution) can either use a tuple list, incurring a sort, or you can use a vector the length of your alphabet (26). So you can make a speed-space trade-off in the preparation stage; the look-ups are constant time, not counting the time it takes to calculate the character distribution of the query word. If you go the latter, fixed-width route, you can make another trade-off by hashing the key, since you know the cardinality of the input (number of unique words)."
Why does averaging word embedding vectors (exctracted from the NN embedding layer) work to represent sentences?,"A simple, intuitive explanation- think of each latent dimension as a measure of some (very abstract) quality or property of a word. The value a word's coordinate has in that dimension describes how aligned or opposed that word is to that property. Words have a higher similarity when they are aligned with the same abstract concepts and opposed to the same ones.

When you average the coordinates over the words in a sentence, what happens to the coordinates that many words agree upon? In a very, very simple example of a two word sentence and a two dimensional embedding, say the first word has the embedding 
(1,‚àí1)
(
1
,
‚àí
1
)
 and the second 
(1,1)
(
1
,
1
)
. The average of the two is 
(1,0)
(
1
,
0
)
. Since the words agree on the first dimension, the average is large. Since they disagree on the second, they cancel each other out. Adding in a third word with embedding 
(1,0)
(
1
,
0
)
 results in the same average. All 3 words are positively associated with the first abstract concept. For the second abstract concept, one word is positively associated, one is negatively, and the last is neutral. These all combine to be neutral. So the embedding average in a sense captures the associations that words agree on, and when words disagree the average becomes more neutral to the concept.

For tasks where the ordering of the words is not important but the collective associations are, this is very convenient- it allows representing sentences or collections of words of arbitrary length in the same exact way as individual words, and the embeddings are directly comparable. Two sentences are similar when the average of the similarities between the individual words is large. Take a sentence with words 
(
x
1
,
x
2
)
(
ùë•
1
,
ùë•
2
)
 and one with 
(
y
1
,
y
2
)
(
ùë¶
1
,
ùë¶
2
)
. Computing their average embeddings followed by the dot product of the embeddings can be written as

<
x
1
+
x
2
2
,
y
1
+
y
2
2
>
<
ùë•
1
+
ùë•
2
2
,
ùë¶
1
+
ùë¶
2
2
>

But since inner products are linear, we can rewrite this as

1
4
(<
x
1
,
y
1
>+<
x
1
,
y
2
>+<
x
2
,
y
1
>+<
x
2
,
y
2
>)
1
4
(
<
ùë•
1
,
ùë¶
1
>
+
<
ùë•
1
,
ùë¶
2
>
+
<
ùë•
2
,
ùë¶
1
>
+
<
ùë•
2
,
ùë¶
2
>
)

Which shows that sentence similarity calculated in this way is equivalent to the average similarity when selecting a word in the first sentence and one from the second. This gives another way to think about the averaged embeddings in a way that directly relates to the similarities between the individual words.

An interesting and important connection pops up when we consider the distance between the embeddings- this takes into account the pairwise similarity between words within the sentences as well as between. This becomes equivalent to something called Maximum Mean Discrepancy which has a great number of important properties when examining whether two samples came from the same distribution or not."
NLP text autoencoder that generates text in poetic meter,"A starting point that comes to mind is creating a cost function for a sentence being in IP. Now, while normally this is a binary affair (either a sentence is in IP or not - or so I would assume), this does not lend itself readily to the task. You should devise a cost function that measures how close your sentence is to IP (so I assume that sentence with 11 syllables would have a lower cost than one with 12. Verbal stresses and their associated costs are left to you to figure out ;) ).

After you have a cost function, you should take some sort of pretrained deep neural network that can translate from one language to another, such as those that use Word2Vec, set it to translate from English to English, and train it with two costs - one for keeping the meaning of the text (probably with another instance of word2vec or something to that effect), the other for being in IP. The relative weights of the two costs should be determined experimentally (you may even want to change them while training).

This is of course a shot in the dark. More research and experimentation will be required, but I hope this may serve you as a starting point in your endeavor.

Be advised that while this is an intriguing project, in my opinion it might not be the best choice for beginners."
Comparing one small dataset with a big dataset for similar records,"A way to speed up this process is to preprocess the large dataset, the goal being to store the documents from A in a way which avoids a lot of useless comparisons.

Store each document from A in an inverted index 
m
ùëö
, so that for any word 
w
ùë§
 
m[w]
ùëö
[
ùë§
]
 is the list of all documents in A which contain word 
w
ùë§
 (note that a document can appear several times in this data structure).
When comparing a new query against 
A
ùê¥
, instead of iterating through all documents in 
A
ùê¥
 just compare against the subsets which have at least one word in common, i.e. 
m[w]
ùëö
[
ùë§
]
 for every word 
w
ùë§
 in the query.

Couple remarks:

Normally stop words would be excluded from the keys since they appear everywhere and they are not relevant for matching.
The key doesn't have to be a single word, it could also be an n-gram (or several n-grams) or even several words in case it fits in memory.

This kind of problem is frequent in the task of record linkage."
"How to find the datasets for skill test (like java, python, c c++, etc.) [closed]","a) You may try to find an anonymized resume dataset in the openbase of Kaggle datasets

b) You may do some web scraping on professional social networks like LinkedIn (take the description of a profile as resume, and the LinkedIn skills as supervised training set) to build your own sample database (do not forget to anonymize it).

Hope this helps!"
How can i get the vector of word using BERT?,"About the first piece of code you posted:

At least from the apparent behavior, I would say your code computes the average of all subword vectors in a sentence, not for each word.

To compute word-level representations, you should average only the subwords belonging to a specific word, not all subwords in the sentence.

As a side note, I would suggest not to reuse variable names, as it makes the code confusing. In your code, you reuse i.

About the second piece of code you posted:

It seems to add up the subword embeddings of each word (only the last BERT layer) and concatenate each resulting vector into a tensor for the whole sentence (whose length would be the number of words)."
Pytorch: understanding the purpose of each argument in the forward function of nn.TransformerDecoder,"About the need for tgt_key_padding_mask

While padding is usually applied after the normal tokens (i.e. right padding), it is perfectly fine to apply it before normal tokens (i.e. left padding). For instance, fairseq supports parameter left_pad to specify precisely this.

For left padding to be handled correctly, you must mask the padding tokens, because the self-attention mask would not prevent the hidden states of the normal token positions to depend on the padding positions.

About the meaning of each argument

I think the best documentation is in MultiHeadAttention.forward:

key_padding_mask ‚Äì if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored
attn_mask ‚Äì 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.

With that information and knowing where keys, values and queries come from in each multi-head attention block, it should be clear the purpose of each parameter in nn.TransformerDecoder.forward. Also, the documentation of MultiheadAttention.forward contains info about the expected shapes."
Detecting grammatical errors with BERT,"Absolutely, you could use bert pre trained model to suggest corrected sentences without annotated data as long as you are using a pre trained model. Here is a good repo that shows it https://github.com/sunilchomal/GECwBERT Hope it helps"
Is CRF suitable for multi-words Named Entity Recognition?,"Absolutely. If you look at the training tutorial, it implies that this isn't an issue at all. When using multi-word entities, you typically need to use a IOB or BILUO tagging schemes, which helps your model train better.

But from a mathematical perspective, there aren't any restrictions for a CRF, as CRF models the likelihood of particular sequences/transitions. Often, people set restrictions for particular transitions if you know in advance that they aren't possible. But by default, all transitions are allowed. In sklearn-crf, allowing all transitions is done by using the all_possible_transitions=True argument."
How to learn common sense constants? Look in body for detail,"Absolutely. You can use this gist by Yuchen Lin to get the correct answers from BERT:

predict_masked_sent(""There are [MASK] days in a week."", top_k=1)
> [MASK]: 'seven'  | weights: 0.1132921576499939

predict_masked_sent(""A chicken has [MASK] legs."", top_k=1)
> [MASK]: 'four'  | weights: 0.25219154357910156

predict_masked_sent(""1 km = [MASK] m"", top_k=1)
> [MASK]: '500'  | weights: 0.08255643397569656

predict_masked_sent(""1 day = [MASK] hours"", top_k=1)
> [MASK]: '24'  | weights: 0.06566877663135529
```"
"Machine translation transformer output - ""unknown"" tokens?","According Hugging Face documentation for BERT, the default vocab_size is 30522. The token 'jean' is not in those top 30522 tokens."
Which is better: GPT or RelGAN for text generation?,"According to [Caccia et al., 2018], in general textual GANs are no rival for LMs regarding several quality measures. These are the conclusions of the paper:

This research demonstrates that well-adjusted language models are a remarkably strong baseline and that temperature sweeping can provide a very clear characterization of model performance. A well-adjusted language model outperforms the considered GAN variants as evaluated on both local, and more surprisingly, global metrics of quality and diversity. Our temperature sweeping framework shares characteristics with a Receiver Operating Curve. Analogously, if one needed a single scalar to compare NLG models, one could compute area under the curve and seek the model with the smallest value (lower is better for our considered metrics).

GAN-based generative models have been proven effective on real-valued data, however, but there exist many difficult pernicious issues of moving to discrete data. These issues must be overcome before they will improve over the strong MLE baselines. On the datasets and tasks considered, potential issues caused by exposure bias were less than the issues of training GANs in discrete data. GAN training may prove fruitful eventually, but this research lays forth clear boundaries that it must first surpass.

This way, OpenAI's GPT and GPT-2 may be considered superior in text generation quality to current textual GANs."
Mixed language OCR,"According to https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image you are doing the correct thing by trying eng+rus. I.e. that is the answer to the question.

You could also try rus+eng (though according to those docs that only affects the time taken?).

If that is not giving good enough quality one possibility is to fine-tune on your own data. See https://tesseract-ocr.github.io/tessdoc/#training-for-tesseract-5 and https://tesseract-ocr.github.io/tessdoc/tess5/TrainingTesseract-5.html#introduction where it mentions fine-tuning might give good results even with a relatively limited amount of training data.

Another approach is to run OCR twice, as each English and Russian, then analyze the resulting strings for likeliness. E.g. match against a dictionary, or run through a character language model. The thinking here is that ""Xonogunbxuk"" does not look at all English. And ""KncnenAR"" (sorry, can't type Cyrillic here) doesn't look like valid Russian. Even ""60150R"" vs. ""601508"" can be done, if you know that you are expecting codes to always be 5 digits then a capital letter.

Note that this is not really extra work: you are going to need a validation step anyway, because OCR output is unreliable even in a single language."
Unusually High BLEU score on a NMT model,"According to recent publications, it is not impossible to get BLEU scores as high as yours for English‚ÜíIrish. Nevertheless, without any other knowledge, they certainly seem too high.

From the command line arguments, there does not seem to be any evident problem.

The most probable explanation is, as you already pointed out, a data leakage between validation/test and training. Note that, while you removed exact duplicates, you may be getting partial matches that go unnoticed. You may want to look into different similarity metrics. The most straightforward is the Jaccard Similarity."
Can BRAT be used for text classification annotation?,"According to their documentation , BRAT does a lot of things, but text classification isn't one of them. BRAT is ""too powerful"" for that. I'd recommend you use a tool like Prodigy instead.

This should do what you're looking for."
What is the difference between and Embedding Layer and an Autoencoder?,"Actually they are 3 different things (embedding layer, word2vec, autoencoder), though they can be used to solve similar problems. (i.e. dense representation of data)

Autoencoder is a type of neural network where the inputs and outputs are the same but in the hidden layer the dimensionality is reduced in order to get a more dense representation of the data.

Word2vec contains only 1 hidden layer but the inputs are the neighborhood words and the output is the word itself (or the other way around). So it cannot be an autoencoder cause the inputs and outputs are different.

Embedding layer is only a ""simple"" layer in a neural network. You can imagine it as a dictionary where a category (i.e word) is represented as a vector (list of numbers). The value of the vectors are defined by backpropagating the errors of the network."
Learning token dictionary,"Actually you can often just split on the white space, remove punctuation, and lowercase. This will provide your tokens. For example, if I had a string ""jdf asdsa sdfr"" (no English), then I could derive the tokens jdf, asdsa, and sdfr. The only thing that is known before hand would be the stopwords (and, be it the), which do come from the English dictionary. However, in this case, it does not sound like you would need stopwords. I would suggest looking at the following libraries available in python:

SpaCy

NLTK

Scikit-learn"
What is the difference between CountVectorizer token counts and TfidfTransformer with use_idf set to False?,"Actually, the documentation was pretty clear. I'll keep it posted in case someone else searches before reading:

The TfidfTransformer transforms a count matrix to a normalized tf or tf-idf representation. So although both the CountVectorizer and TfidfTransformer (with use_idf=False) produce term frequencies, TfidfTransformer is normalizing the count."
Word based perplexity from char-rnn model,"Actually, there is a formula which can easily convert character based PPL and word based PPL.

PPL=
2
(BPC‚àóNc/Nw)
ùëÉ
ùëÉ
ùêø
=
2
(
ùêµ
ùëÉ
ùê∂
‚àó
ùëÅ
ùëê
/
ùëÅ
ùë§
)

where 
BPC
ùêµ
ùëÉ
ùê∂
 is character based 
PPL
ùëÉ
ùëÉ
ùêø
, 
Nc
ùëÅ
ùëê
 and 
Nw
ùëÅ
ùë§
 are the number of characters and words in a test set, respectively.

The formula is not completely fair, but it at least offers a way to comparing them. The following are some reference.

[1] Hwang K, Sung W. Character-Level Language Modeling with Hierarchical Recurrent Neural Networks[J]. 2016.

[2] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.

[3] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.Subword language modeling with neural networks. Technical report, Un-published Manuscript, 2012."
How does MITIE perform named entity recognition?,"After having used MITIE for a few weeks, I feel like I at least have enough to answer my basic questions:

(and 3.) All models need to be trained from scratch - there is no online method to add new samples to the model as they come in. This is unfortunate because MITIE takes at least 45 minutes to an hour to train on a ~20k-sized dataset.
The datasets I used were ATIS, CoNLL 2003, and DBpedia

I've found MITIE to be quite good as far as classification accuracy goes, although it takes a bit of work to prepare datasets for it."
Confusion about Keras' skipgram and sampling table utilities,"After looking through the code for the skipgram method, I figured it out: it has to do with the sampling_factor parameter. It defaults to 1e-5, which means that for such a small sampling factor it's almost never going to actually pick a sample unless your dataset & vocabulary size is big enough to actually get some samples. If you have a small vocabulary (like in the toy example), you should set that to something much higher: when I set it to 1.0 (i.e. take a sample as often as possible) I got lots of pairs, even for the toy example."
Train a spaCy model for semantic similarity,"After vectorizing your custom text, you need to do one of the 2 things in spaCy:

load those binary vectors to nlp.vocab with something like nlp.vocab.load_rep_vectors .
Or simply replace the vec.bin file in ""data/vocab/vec.bin"".

Detailed information here: https://stackoverflow.com/questions/43524301/update-spacy-vocabulary ."
Building own embedding from sequence,"Almost any embedding method can learn the relationship between those discrete actions.

For example, Python's gensim package has a word2vec implementation that supports training on your own model.

You'll have write your tokenizer that creates pairs of words (e.g., ""add wall"" or ""edit wall"") as tokens. Conventional tokenizer will split on whitespace."
why there is no preprocessing step for training BERT?,"Although a definitive answer can only be obtained by actually trying it and it would depend on the specific task where we evaluate the resulting model, I would say that, in general, no, it would not improve the results to remove stopwords and punctuation.

We have to take into account that the benefit of BERT over more traditional approaches is that it learns to compute text representations in context. This means that the representations computed for a word in a specific sentence would be different from the representations for the same word in a different sentence. This context also comprises stopwords, which can very much change the meaning of a sentence. The same goes for punctuation: a question mark can certainly change the overall meaning of a sentence. Therefore, removing stopwords and punctuation would just imply removing context which BERT could have used to get better results.

That is why you will not see stopwords being removed for deep learning approaches applied to tasks where language understanding is key for success.

Furthermore, while blindly doing stopword removal has been a ""tradition"" in tasks like topic modeling, its usefulness is beginning to be questioned even for those tasks in recent research.

regarding tokenizer

BERT has a word-piece vocabulary that was learned over the training corpus. I don't think removing tokens manually is an option here, given that they are word pieces and you wouldn't know in which words they may be used. It would be possible, however, to identify low frequency tokens by encoding and large vocabulary and then remove the lowest frequency ones. Nevertheless, BERT's vocabulary has almost 1000 (purposefully) unused tokens, so there is room to remove unused tokens if that's what you want. I don't think it would make a difference, though."
Any research on segmentation of non-text contents out of (mostly) text-documents?,"Although I didn't implement it so far, I am pretty sure natural language text vs code snippets is easy:

For each block, you make compare the distribution of characters to ground-truth natural language text vs. code. See my paper The WiLI benchmark dataset for written language identification, page 4 ""Single-Character Frequency Analysis""."
"How do ""intent recognisers"" work?","Although not directly answering your question, you may be interested in the field of automated question answering. To answer natural language text questions they must first be understood, which overlaps with your problem.

A good resource is the course by Jurafsky and Manning. Particularly the sections on semantics and question answering may help with what you are looking for. There are accompanying lecture videos available on youtube here."
What algorithm to use for extracting information from bank statements,"Am currently working on something in this domain.

The rough process I am currently following is -

Extract data from PDFs (ubiquitous version of Bank Statements nowadays) into more usable formats. Currently,converting them to TXT files first as an intermediate step.

Generically, bank statements (from a specific bank) tend to be structured in the same format. Hence, while converting the TXT to CSV, you can structure the algo such that it knows what to pick up - based on a rough analysis of the TXT file.

Use 2 different data frames to store the statements content - one df keeping track of the transactional data (storing 3 values - Date, Description and Amount) and the other to keep track of the metadata (storing 2 values - Key and Value)
You can determine the rough structure of the TXT file by going through a few TXTs. e.g. the transactions generally begin after a specific element's occurrence in every bank statement. Ignore the page footers / headers that recur in case of multi-page transactions. Also, another observation would be that all data in a specific column of the transactions (the transactions are read in as a table by my program [link given below]) appear together.
Doing the above enables the transaction and meta data to a relatively structured and processable format

Perform analysis on the Description column or the entire transactional df you have thus generated.

You can use the following code to convert a given PDF to a txt file -

from __future__ import unicode_literals
import os
import sys  
reload(sys)  
sys.setdefaultencoding('utf8')
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter
from pdfminer.layout import LAParams
from cStringIO import StringIO

def pdfparser(data):
    dest = data[:-3]+""txt""    
    print ""\n\n\n\n\n"",dest    
    fp = file(data, 'rb')
    rsrcmgr = PDFResourceManager()
    retstr = StringIO()
    codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)
    # Create a PDF interpreter object.
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    # Process each page contained in the document.
    for page in PDFPage.get_pages(fp):
        interpreter.process_page(page)
        data =  retstr.getvalue()
        data = unicode(data,'utf-8', errors='ignore')
    #print data
    # write data to a file
    print data
    with open(dest, ""w"") as f:
        f.write(data)

# set the working directory
path = ""F:/banking/payments/""
os.chdir(path)
fls=os.listdir(path)

for x in fls:
    if x[-3:]==""pdf"":
        pdfparser(path+x)"
Eliminate low quality predictions in a classification task,"An alternative to thresholding the classification probability would be to set a threshold on the ratio between the highest reported probability and the second highest reported probability. For example, a threshold of 2 would be interpretable as: ""Only retain classifications where the likelihood of the class assignment is at least twice as likely as the next most likely class."""
Is there a way to rank the Extracted Named Entities based on their importance/occurence in a document?,"An easy way would be to use TF-IDF (term frequency‚Äìinverse document frequency). It can help you find how much terms stand out in a document (by comparing with your entire corpus) and use it to rank your entities.

TfidfVectorizer from scikit-learn

Just note that the TfidfVectorizer is on a word level. So some processing will be needed if your entities can consist of more than one word.

Alternatively you could use a model that allows you to produce a heatmap of the words. Then you can use that heatmap to look up your NEs in that heatmap. This paper, A Structured Self-Attentive Sentence Embedding, could give you some ideas."
Is n-gram a special instance of bag of word? What are their differences?,"An n-gram is a sequence of tokens/words in order (therefore bigrams to-go and go-to are different). On the other hand, a bag of words is the count of token/word occurrences in a piece of text, which losses the ordering information.

They, however, apply to different scopes. You compute a bag of words over a whole text. An ngram within a text is just a sequence of N words, where N is usually small (e.g. 2, 3). Also, note that you can have a ""bag of ngrams"" where you count the occurrences of ngrams instead of individual words."
Skipgram - multiple formulations?,"Andrew Ng word2vec @1:29. In the video, his choice of window size is more than one. In his example, if ""orange"" is the center word, the target word could be ""juice"", ""glass"", ""my"", which are all within the window size of 5.

In your example, ""cat sat on the mat"", if you choose a window size of one, you can get 7 training samples:

(cat, sat),
(sat, on),(sat, cat),
(on, sat), (on, the), 
(the, on), (the, mat)


If this is the entire corpus, p(cat|sat), probability of sat occurs as the center word, cat being the context, is 0.5, p(sat|cat), cat being the center word, sat being the context is 1. The distributed word representation of ""cat"" and ""sat"" being the center word should be different.

""it seems like for the same center word we can get different outputs:"" That's because they are predicting more than one context words.

@31:35 224 2017 lec2, the slide says ""predict surrounding words of radius m for every word"". That is, the problem is to find a distribution of context words given the center word, so for one center word, the model will have more than one output words (unless you set m to 1 and specify the direction as well).

""I don't really understand how it is possible to get different outputs if you multiple Weights 2 by the Hidden Layer.""

@39:37 224 2017 lec2 The professor did a dot product between the matrix output and the word representation. It's also shown on the top of the note: ""
u
T
o
v
c
ùë¢
ùëú
ùëá
ùë£
ùëê
""

Multiply the hidden layer (d * 1) by Weights 2 (V * d) will get a V*1 vector. That's 
u
o
ùë¢
ùëú
, output vector representation. Then we do 
u
T
o
v
c
ùë¢
ùëú
ùëá
ùë£
ùëê
 to get the output for each context word. After that, we apply softmax and compare that value with the context word's one-hot representation (
y
ùë¶
 vs 
y
^
ùë¶
^
), three comparisons in the prof's example.

That sounds like the idea of the Continuous Bag of Words model."
Understand clearly the figure: Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification,"Answering this in terms of NLP examples is quite hard, remember ""All models are wrong, some models are useful."" First think of this in an image classification problem context, you want to use a large number of filters to collect a large number of features out of the image, one could detect edges, the other could detect densely coloured areas, one might turn a region to b&w. Extend a similar logic to text, by using a lot of filters, in this case 128, you are trying to capture a lot of features. For an example like , "" I like movies very much"", a certain filter might detect that like is a positive word and not a similarity comparison, a certain filter of size 2 might detect very much and detect that it is an expression of degeree. You can go on like that, it will be hard to come up with 128 features but the idea is to get enough features. If you think the number is unreasonable and might lead to overfitting, you can reduce the number and compare your results.

No, 1- maxpool means that you take the maximum value of the output vector after applying a filter to the input. So it has nothing to do with the longest word but rather choose an element from the output that express the extracted feature to the highest amount."
Latent space vs Embedding space | Are they same?,"Any embedding space is a latent space.

I'm not expert in this specific topic, but in general the term ""latent space"" refers to a multi-dimensional space in which elements are represented but their representation is not directly interpretable and/or observable. Typically this is in contrast to a space where dimensions have an interpretable meaning, for example in the traditional bag of words representation a dimension corresponds to a particular word.

The term ""latent"" applies to some variable which is not directly observable, for example the ""latent variable"" in a HMM is the state that the model tries to infer from the observations. It's sometimes called the ""hidden variable"".

Naturally a latent space is relevant only if it is meaningful with respect to the represented objects and/or the target task. This is what these sentences mean."
How do GPT models go from token probabilities to textual outputs?,"Any language model can generate text with different approaches:

Greedy decoding: you get the highest probability token at each time step.
Sampling: the generated token is sampled from the probability distribution at each time step.
Temperature sampling: the generated token is sampled probability distribution after applying a temperature factor 
Œ±
ùõº
, which can either flatten the distribution or sharpen it.
Beam search: you keep the highest k most probable subsequences (i.e. the ""beam""); when you finish decoding them, you output the most probable one.
Top-k sampling: you sample from the probability distribution, but only considering the top k most probable tokens.
Nucleus sampling: you sample from the probability distribution, but only consider the top probability tokens that add up to a specific cumulative probability p.

The OpenAI's API allows selecting the following approaches for both the completions and the chat endpoints:

Temperature sampling, with the temperature parameter.
Nucleus sampling, with the top_p parameter.

You can specify both of them, but OpenAI suggests you only use one of them at the same time.

If you want to know the specific detail of the implementation of these approaches, you can check this post from the Huggingface blog."
How can I predict the acceptance of an article by publisher?,"Applied Predictive Modeling book has a case study of acceptance rate of grant proposals, which was a Kaggle competition. You can get many good ideas from there.

from the dataset, what kinds of features you would look at
from the book, step-by-step explanations of how to build models
from the Kaggle competition, approaches by the contestants

I think it is very hard to predict the quality of the paper from the text of the paper, but you might have much more luck by including features such as academic rank of the author(s), their past publishing history and success, acceptance rate of the journals the paper was submitted to, and things such as even the length of the paper. Such proxy features for paper quality actually might be very successful..."
Do repeated sentences impact Word2Vec?,"Are the sentences exactly the same, word to word? If that is the case I would suggest removing the repeated sentences because that might create a bias for the word2vec model, ie. repeating the same sentence would overweigh those examples single would end with higher frequency of these words in the model. But it might be the case that this works in your favor for find synonyms. Subsample all the unique sentences, not just the most frequent ones to have a balanced model.

I would also suggest looking at the FastText model which is built on top of the word2vec model, builds n grams at a character level. It is easy to train using gensim and has some prebuilt functions like model.most_similar(positive=[word],topn=number of matching words) to find the nearest neighbors based on the word embeddings, you can also use model.similarity(word1, word2) to easily get a similarity score between 0 and 1. These functions might be helpful to find synonyms."
"In the Keras Tokenizer class, what exactly does word_index signify?","Are the words all words in texts, or are they maxed at SOME_NUMBER?

Yes, it will be maxed at the most common SOME_NUMBER-1 words.

And are the dict values for word_index the frequency of each word, or just the order of the word?

It is just the index of the word in the dictionary.

You can read more here in the documentation."
Metrics for Name Entity Recognition,"As a first start I would recommend to use Precision and Recall.

You can also produce stats on the different types of errors:

entity not found (false negative, or Type I error). Also you can get the false negative rate by dividing by the total number of true entities. This would be 1-recall.

entity found where it shouldn't be (false positive or Type II error). Also you can get the false positive rate by dividing by the total number of returned entities. This would be 1-precision.

entity found but system got wrong number of tokens (e.g. York instead of New York). You can also get the ""token boundary error rate"" by dividing by the total number of true entities.

So the first two error rates I mentioned don't give any extra info other than precision and recall but the third allows you to check how accurately you are getting entity boundaries.

If you are doing entity disambiguation too, you can produce additional metrics by only counting a success if the entity was correctly resolved.

You can also use the F-score (thanks Erwan) which combines precision and recall. However you need to be careful because you have a large class imbalance of O-tokens vs entity tokens, and the F-score gives equal importance to precision and recall. You also have the options of weighted F-scores such as F2 score.

Other option: ROC curve and AUC

Finally I would add that if your model outputs a score or probability rather than a binary classification, it means you can tweak its sensitivity by changing the threshold (do you want to count all entities with score>0.5 as entities, or score>0.75, for example). This naturally leads to the possibility of calculating the false positive and false negative rates for every possible threshold choice. You can plot it on a graph and you have a ROC curve. This is a nice graphical way to see the performance of your model and exactly how it misclassifies instances. The area under the ROC curve is itself a nice metric called AUC. A classifier that is as good as random has AUC=0.5, a classifier that classifies everything wrong has AUC=0, and a perfect classifier has AUC=1. I normally plot the ROC curve as well as generating metrics, Sklearn and most stats packages have a function to do it."
Extract canonical string from a list of noisy strings,"As a naive solution I would suggest to first select the strings which contain the most frequent tokens inside the list. In this way you can get rid of irrelevant string.

In the second phrase I would do a majority voting. Assuming the 3 sentences:

Star Wars: Episode IV A New Hope | StarWars.com
Star Wars Episode IV - A New Hope (1977)
Star Wars: Episode IV - A New Hope - Rotten Tomatoes

I would go through the tokens one by one. We start by ""Star"". It wins as all the string start with it. ""Wars"" will also win. The next one is "":"". It will also win.

All the tokens will ein in majority voting till ""Hope"". The next token after ""Hope"" will be either ""|"", or ""("" or ""-"". None of the will win in majority voting so I will stop here!

Another solution would be probably to use Longest common subsequence.

As I said I have not though about it much. So there might be much more better solutions to your problem :-)"
Machine learning roadmap not for beginners [closed],"As a professional researcher at a top 3 universities in the USA, I will give you my perspective. Please note that everyone's path is different so listen to lots of opinions. The best hints and advises will be part of those opinions so you will get an idea of a possible path. Also, note that I won't assume that you are a prodigy student that you might drop out from college and companies such as FAIR, Google etc will take you under their wings and train you as a researcher. My advice will be mainly focused on what on average you need to increase your chances of success.

First of all, becoming a professional researcher and working at FAIR are 2 different things (that of course can be achieved at the same or at different times). Now, to become a professional researcher you will need professional training = a PhD programme. The PhD years are the years that you will be trained in order to carry out original research. So pursuing a PhD should be your number 1 goal mid-term (after graduating).

Bare in mind that as a Research Scientist in FAIR you will be expected to define your own research path which should align with the objectives of the larger group of researchers that you will belong. This means that you need to ask the right questions. Questions that are interesting and potentially lead to interesting and impactful work. And of course this implies that you know very well the literature of the subject of your research. Also, usually Research Scientists at FAIR, Google etc have already some good impactful research highlights from their PhD/Postdoc and the company is be interested in incorporating these research directions to their research directions. In other words they have something of value to ""sell"" and not just good skills.

This leads to my next point: specialization. If your goal is to work as a Research Scientist at FAIR (and for example not Google) you need to have specific reasons on why FAIR. Do you like something specific about their research? If you want to become an RL expert for example this might not necessarily lead you to FAIR as they are not as heavily invested in RL compared to e.g. Google. These are things you need to think about.

Having a bit of experience here and there in ML won't lead you anywhere if you don't start focusing on what you want to do in order to achieve your goals. We are talking about a marathon not a sprint so you need to think about long term goals (a plan for the next 10yrs).

Knowing about architectures and have zero ML foundation won't lead you far in the field. As you gain experience start understanding group/family of methods/techniques that do X. For example you might know about PPO, A2C but you might not know what are the differences between Policy Gradient Methods and value methods. These are things that you need to think about for the more focused path that you will take. Successful researchers know very well their field and can come up with successful next steps that push the boundary of their field. You need to build a more general understanding than just knowing about architectures. This leads to my final point: Math.

Being good in Math and Stats is extremely valuable as a researcher in ML. Yes, select the tough path and become good in the math of your field. Don't listen to people telling you that you don't need math.

Few other things: get involved in research groups early to find out what you like. If needed attend a master of research or a master of science especially if you need more courses. Email people to get advice and ask questions again and again! Try to publish early and then apply for a PhD. Choose a good supervisor that will be also a mentor (VERY important as some people are supervising without mentoring). You might end up at a very competitive lab but supervisor might be so busy that you will get nothing from him/her. Think also what YOU get from a project/collaboration/apprenticeship etc. Always keep your math and coding skills sharp. And lastly focus (again), narrow down the research fields that intrigue you.

I tried to cover as much as I could. Good luck with everything and remember exercise patience and persistence!"
What should the numerical values of the <startofsentence> and <endofsentence> token vectors be?,"As commented by @Erwan, the start/end of sequence tokens are like any other token: they are part of the embedding table and they are identified by their index to that table.

The vector values of the start/end tokens in the embedding table are learned during the training of the network, like the other tokens."
Performing a text classification based on a dictionary,"As defined, there's no ML in this problem: the program would associate each keyword to a category, so it would consist of a loop over the words in the documents, and inside the loop there is an if statement:

for every word in doc
   if associated_category[word] is defined then
       return associated_category[word]


That's it, this is the full rule-based classifier.

But this would not be a good system of course. Two of the main issues are:

Some queries/documents may contain several keywords, for example ""eta"" and ""destination"". In this case the category is ambiguous.
Some queries/documents may not contain any of the keywords. For example ""What happened to my parcel?"" should clearly be a delivery_issue, but doesn't contain any of the keywords.

So the problem as defined is not well specified, this will lead to ambiguities and errors."
What is considered short and long text in NLP (document similarity),"As Erwan said in the comments, it depends. In my experience, it depends specifically on two things:

Tokenization method: The length of a document in number of tokens will vary considerably depending on how you split it up. Splitting your text into individual characters will result in a longer document than splitting it into sub-word units (e.g. WordPiece), which will still be longer than splitting on white space.

Model: Vanishing gradients aside, an RNN doesn't care how long the input text is, it will just keep chugging along. Transformers, however, are limited. BERT can realistically handle sequences of up to 512 WordPiece units, while the LongFormer claims to handle sequences of up to 32k units (given sufficient compute resources). Thus your documents of 10 - 600 tokens would be long for BERT but short for the LongFormer.

Whether you should treat documents of length 10 differently from those of length 600 is not something I can answer without knowing the details of your specific task. Intuitively, I doubt a very short document would ever be very similar to a much longer one, simply because it likely contains less content."
"LSA, LDA or NMF in Topic Modeling?","As far as I know LDA is the state of the art approach for topic modeling, but I'm not following the field very closely. So I would say that it's pretty safe to use LDA, and my guess is that the different approaches are likely to give similar results overall.

In case you want to try different methods, the question of evaluating topic models is quite complex. This article might help.

Side note: there is a non-parametric variant (no need to choose the number of topics) of LDA called Hierarchical Dirichlet Processes."
"Definition of a ""lexicon"" in Named Entity Recognition","As far as I know the word ""lexicon"" is mostly used for a simple list of words (or terms), I would say it's quite rare to use it for describing a list of patterns/rules. ""gazetteer"" and ""dictionary"" would be a bit more general in my opinion, for instance one can have a ""dictionary of rules"" which associates specific patterns with actions. But overall I agree with your impression that these terms are often used interchangeably, I'm not aware of any standard definition."
Threshold determination / prediction for cosine similarity scores,"As far as I know there is no satisfactory answer:

One uses a threshold in order to avoid having to choose a specific K in a top K approach. The threshold is often selected manually to eliminate the sentences which are really not relevant. This makes this method more suitable for favouring recall, if you ask me.
Conversely, one uses a ""top K"" approach in order not to select a threshold. I think K is often selected quite low in order to keep mostly relevant sentences, i.e. it's an approach more suitable for high precision tasks.

The choice depends on the task:

First, the approach could be chosen based on what is done with the selected sentences: if it's something like question answering, one wants high precision usually. If it's information retrieval, one wants high recall. If it's a search engine, just rank the sentences by decreasing similarity.
Then for the value itself (K or threshold), the ideal case is to do some hyper-parameter tuning. i.e. testing multiple values and evaluate the results. If this is convenient or doable for the task, then look at a few examples and manually select a value which looks reasonable."
Automatically categorize parts of a piece of writing,"As far as I know there's no specific task for this, it's general text classification. It's also related to text segmentation. There are certainly existing systems for similar tasks, but probably specific to a certain type of data.

In general this would be a supervised process, you would need a sample containing many documents in which the parts are pre-annotated. I think sequence labeling algorithms would be the standard approach."
How to segment old digitized newspapers into articles,"As far as I know topic segmentation is not a particularly easy task with clean data, so it's likely to be challenging with noisy old French.

It's not exactly the same problem so I'm not sure if this is useful but you might want to look into using stylistic features in order to help the model detect the changes between articles. There has been a fair amount of work on the task of style change detection as part of the PAN series (the task has run for 3 years, results and papers are available from the previous years).

Hope this helps."
How to do Named Entity Recognition in Tables?,"As far as I know you don't have a lot of options, you're probably stuck with heuristics:

Capital letters
Regular expressions (e.g. for dates)
List of predefined entities (e.g. from Wikipedia) stored in a dictionary"
How does BERT deal with catastrophic forgetting?,"As far as I know, no neural network can be immune to catastrophic forgetting during fine-tuning (which is essentially controlled retraining).

The key is to not fine tune the pretrained model for longer epochs, or with higher learning rates. This ensures that the learned knowledge from the lower layers is preserved more or less intact, while also helping the model learn from the new data used for fine-tuning, as mentioned here : https://github.com/huggingface/transformers/issues/1019"
Tweet Classification into topics- What to do with data,"As far as I'm aware there is no correct/standard way to apply topic modelling, most decisions depend on the specifics of the case. So below I just give my opinion about these points:

I have removed, before cleaning the data (removing mentions, stopwords, weird characters, numbers etc), all duplicate instances (having all three columns in common), in order to avoid them influencing the results of topic modelling. Is this right?
Should I, for the same reason mentioned before, remove also all retweets?

In general there is no strict need to deduplicate the data, doing it or not would depend on the goal. Duplicate documents would affect the proportion of the words which appear in these documents, and in turn the probability of the topic these documents are assigned to. If you want the model to integrate the notion of popularity/prominence of tweets/words/topics, it would probably make sense not to deduplicate and keep retweets. However if there is large amount of duplicates/retweets the imbalance might cause less frequent tweets/words to be less visible, possibly causing less diverse topics (the smallest topics might get merged together for instance).

Until now, I thought about classifing using the ""per-document-per-topic"" probability. If I get rid of so many instances, do I have to classify them based on the ""per-word-per-topic"" probability?

I'm not sure what is called the ""per-document-per-topic"" probability in this package. The typical way to use LDA in order to cluster the documents is to use the posterior probability of topic given document (this might be the same thing, I'm not sure): for any document 
d
ùëë
, the model can provide the conditional probability of every topic 
t
ùë°
 given 
d
ùëë
. The sum of this value across topics sums to 1 (it's a distribution over topics for 
d
ùëë
), and for classification purposes one can just select the topic which has the highest probability given 
d
ùëë
.

Do I have to divide the dataset into testing and training? I thought that is a thing only in supervised training, since I cannot really use the testing dataset to measure quality of classification.

You're right, you don't need to split into training and test set since this is unsupervised learning.

Antoher goal would be to classify twitterers based the topic they most are passionate about. Do you have any idea about how to implement this?

The model gives you the posterior probability distribution over topics for every tweet. From these values I think you can obtain a similar distribution over topics for every tweeter, simply by marginalizing over the tweets by this author 
a
ùëé
: if I'm not mistaken, this probability 
p(t|a)
ùëù
(
ùë°
|
ùëé
)
 can be obtained simply by calculating the mean of 
p(t|d)
ùëù
(
ùë°
|
ùëë
)
 across all the documents/tweets 
d
ùëë
 by author 
a
ùëé
."
Keep retweets during topic-modelling [duplicate],"As far as I'm aware there is no correct/standard way to apply topic modelling, most decisions depend on the specifics of the case. So below I just give my opinion about these points:

I have removed, before cleaning the data (removing mentions, stopwords, weird characters, numbers etc), all duplicate instances (having all three columns in common), in order to avoid them influencing the results of topic modelling. Is this right?
Should I, for the same reason mentioned before, remove also all retweets?

In general there is no strict need to deduplicate the data, doing it or not would depend on the goal. Duplicate documents would affect the proportion of the words which appear in these documents, and in turn the probability of the topic these documents are assigned to. If you want the model to integrate the notion of popularity/prominence of tweets/words/topics, it would probably make sense not to deduplicate and keep retweets. However if there is large amount of duplicates/retweets the imbalance might cause less frequent tweets/words to be less visible, possibly causing less diverse topics (the smallest topics might get merged together for instance).

Until now, I thought about classifing using the ""per-document-per-topic"" probability. If I get rid of so many instances, do I have to classify them based on the ""per-word-per-topic"" probability?

I'm not sure what is called the ""per-document-per-topic"" probability in this package. The typical way to use LDA in order to cluster the documents is to use the posterior probability of topic given document (this might be the same thing, I'm not sure): for any document 
d
ùëë
, the model can provide the conditional probability of every topic 
t
ùë°
 given 
d
ùëë
. The sum of this value across topics sums to 1 (it's a distribution over topics for 
d
ùëë
), and for classification purposes one can just select the topic which has the highest probability given 
d
ùëë
.

Do I have to divide the dataset into testing and training? I thought that is a thing only in supervised training, since I cannot really use the testing dataset to measure quality of classification.

You're right, you don't need to split into training and test set since this is unsupervised learning.

Antoher goal would be to classify twitterers based the topic they most are passionate about. Do you have any idea about how to implement this?

The model gives you the posterior probability distribution over topics for every tweet. From these values I think you can obtain a similar distribution over topics for every tweeter, simply by marginalizing over the tweets by this author 
a
ùëé
: if I'm not mistaken, this probability 
p(t|a)
ùëù
(
ùë°
|
ùëé
)
 can be obtained simply by calculating the mean of 
p(t|d)
ùëù
(
ùë°
|
ùëë
)
 across all the documents/tweets 
d
ùëë
 by author 
a
ùëé
."
LSTM Feature engineering: using different Knowledge Graph data types,"As general points:

Multivariate RNN: You can use multiple sequential features as an input to your recurrent layers. Taking pytorch as a reference, you can see that the input of LSTM object is a tensor of shape 
(L,
H
in
)
(
ùêø
,
ùêª
ùëñ
ùëõ
)
 or 
(L,N,
H
in
)
(
ùêø
,
ùëÅ
,
ùêª
ùëñ
ùëõ
)
 for batched, where 
L
ùêø
 is the length of your sequences whereas 
H
in
ùêª
ùëñ
ùëõ
 is the number of input features. In this approach, you can leave mapping tokens to a vocabulary as part of the standard procedure of a standard embedding being learnt.
You may be able to use a multi-label approach (as opposed to multi-class), if I understand your question correctly.
Multimodal learning: If features related to embeddings can be considered static/not evolving over time, you may want to add a second auxiliary port to your network, to specifically model this data type. This second part would consist of a feed-forward network with fully connected layers. The fixed-length vector representations / embeddings at the outputs of your RNN and FFN modules could get concatenated before passed to your classification layer. In this way you allow the model to reason from a joint representation of both data modalities.

Hope it helps."
Spacy Returns Nonidentical Results for Doc. Examples?,"As I commented on your post - I think you have a problem with your environment. If you cannot solve that, you should file a bug with the spacy team on their GitHub.

I tested the same code and could only see a difference in results compared to the website when using language models of different sizes. If you installed the English model using the default command: python -m spacy download en, then you get the smallest model by default. You should be using the medium-sized model to produce their results. I compare the two below

The script
import spacy

# Small setup
nlp_small = spacy.load('en')
tokens_s = nlp_small(u'dog cat banana')

# Medium setup
nlp_medium = spacy.load('en_core_web_md')
tokens_m = nlp_medium(u'dog cat banana')

# large setup would use 'en_core_web_lg' ...


print('Results using small model:\n')
for token1 in tokens_s:
    for token2 in tokens_s:
        print(token1.text, token2.text, token1.similarity(token2))

print('\nResults using medium model:\n')
for token1 in tokens_m:
    for token2 in tokens_m:
        print(token1.text, token2.text, token1.similarity(token2))

The output
Results using small model:

dog dog 1.0
dog cat 0.53906965
dog banana 0.28761008
cat dog 0.53906965
cat cat 1.0
cat banana 0.4875216
banana dog 0.28761008
banana cat 0.4875216
banana banana 1.0

Results using medium model:

dog dog 1.0
dog cat 0.8016855
dog banana 0.24327648
cat dog 0.8016855
cat cat 1.0
cat banana 0.28154367
banana dog 0.24327648
banana cat 0.28154367
banana banana 1.0


The results are clearly better when using the medium sized model - and they match the results on the website.

Here is the spacy version as used within a conda environment to produce these results:

n1k31t4@n1k31t4~$ conda list | grep spacy
spacy                     2.0.12                    <pip>


And the Python version information:

n1k31t4@n1k31t4:~$ ipython
Python 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:53:17) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help.


Although I ran the script on Linux (OP used MacOSX) and my Python version is slightly newer... I wouldn't expect these things to explain the difference in results."
Dataset for Named Entity Recognition on Informal Text,"As I understand it, these are the properties that you're seeking in a sample dataset:

Text data
It should be informal, i.e. have typos, slang, and basically something not professionally edited
Something other than Twitter (I don't blame you, Twitter is a useful yet way overused example datasource in text mining)

Here are some recommendations:

Emails from the SpamAssassin corpus -- note that both ""ham"" (non-spam) and spam datasets are available
microblogPCU data set from UCI, which is data scraped from the microblogs of Sina Weibo users -- note, the raw text data is a mix of Chinese and English (you could perform machine translation of the Chinese, filter to only English, or use it as-is)
Amazon Commerce reviews dataset from UCI
Within the bag-o-words dataset, try using the Enron emails
The Twenty Newsgroups dataset
This nice collection of SMS spam
You can always scrape (extract) your own text data from the Internet; I'm not sure which language or statistical package you're using, but XPath-based packages are available in R (rvest, scrapeR, etc) and Python to accomplish this"
Gradient Check is failing for RNN,"as i understand, you have the wrong backprop gradient implementation. Here you should take into account, that rnn's hidden state h has its previous state in the equation: h[time-1]. This is also must be extracted via chain rule. For more information suggest to refer this post.
It also contains Python rnn implementation."
NLP: To remove verb and find the match in a sentence [closed],"As indicated by @hssay, your way seems to be PoS tagging and then removing verbs. If you don't want to get your hands dirty, you might prefer to use the off-the-shelf Google natural language web API. If you try the UI, click on the Analyse button, and then under the Syntax tab, look for Part of Speech and VERB.

Another similar toolkit that you could use as an API is StanfordNLP."
What techniques should I use to compare the similarity between a bunch of texts?,"As of now, I can think of two ways to formulate this problem:

1. Search problem

Parse your job listings and index them in some sort of search engine like Solr or ElasticSearch. You can build capabilities like Semantic search using Word2Vec models, etc.

Now write a query engine which takes a resume and queries this Search engine. It will be blazing fast since job listing will be all indexed.

2. Similarity problem

I would have created hybrid similarity function. For example:

a) How many top key words matched between resume and job listing

b) Similarity of resume and job listing using Doc2Vec (Pre compute vector for job listings)

c) Using algorithms like Locality Sensitive Hashing to reduce the lookup space

This approach will be slow but might yield a good result."
Training with less data,"As rightly pointed out by @erwan, it is a bad idea to use data augmentation with 'text data'

The problem of 'training with less data' can be approached in many ways, here I enlist two ways which helped me with significant impact:

(a) One approach would be to use semi-supervised approach. There are open sourced language models trained on insanely massive datasets, that can be used to perform a specific task like custom NER or sentence classification. Transfer learning is more useful today then ever.

(b) Anyways if we want to go ahead with Data augmentation, 'Sentence‚ÄêChain Based Seq2seq Model for Corpus Expansion' is one of the proven methods to proceed. Please find the link to paper here.

I will strongly recommend to experiment with BERT before moving to Corpus Expansion. For introduction to transfer learning, BERT etc. please visit here."
How is the Gaussian noise given to this BLSTM based GAN?,"As stated in 3.2 Model setup

The generator G is made up of embedding layer, one bidirectional long short-term memory (BLSTM) [21] layer, one fully connected (FC) layer.

And

Gaussian noise is 10-dim vector concatenated with the output of BLSTM

So the noise is concatenated to the embeddings computed by the BLSTM for each time step. I think they concatenate the same 10-dim vector to each embedding output of the BLSTM but this is not clear.

The obtained vector (embedding concatenated with noise) is fed to the Fully-connected layer with softmax activation to compute the probability that the word should be translated or not.

Concerning page 2 footnote about ""ignoring the noise""

I think they simply ignore it in their explanations of the model, but it is effectively used in the model by simply appending a gaussian noise vector to the embeddings output of the BLSTM.

As you see in the following figure they do not show the noise, but it is actually concatenated to the output embeddings of the BLSTM in the Generator."
Understanding Classifier performance on text data,"As the data is imbalanced and skewed towards few classes, that's why RF and Logistic results are biased causing high FP values, therefore high precision and low recall.
SVC on the other hand might have tried to create hyperplanes to get most out of the other side of curve, thereby producing different results.

To improve the results, try to use non linear kernels and also try to balance the input data (by scaling etc) before feeding to classifiers."
How does Phrases in Gensim work?,"As the gensim tool cites the very famous paper by Mikolov - ""Distributed Representations of Words and Phrases..."" using which it is implemented. In the paper if you look at the section ""4 Learning Phrases"" they give a nice explanation of how n-grams are calculated (Equation 6).

So, if want to count bigrams this formula is straight-forward; score(wi, wj) is the score between any two words occuring together. But when counting trigrams, 'wi' will be a bigram and 'wj' will be a word. And same follows for any number of grams."
How to classify features into two classes without labels?,"As you know, clustering is Unsupervised learning algorithm. Since you don't know the number of clusters, it becomes hard to find the best possible separation (number of clusters). There is a very good paper published on validating clustering techniques. There are 3 criteria defined in this paper for validating your clusters. You can take a look at that.

Also, the link below gives you some code in R for cluster validation. If you want you can try this too-

http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/"
Can embeddings generated by word2vec be similar for words which never share any words in the same sentence?,"As you send a bag of words into the CBOW as an input, and it works based on the n-grams, as in your case words A and B do not share any co-occurrences with each other, it means their not presented in any shared n-grams, and their vectors should not be near each other at all."
Activation method and Loss function for multilabel multiclass classification,"Assuming all the labels have the same importance you can have a sigmoid for every class at the output. For each of the classes it will ask, is this class part of this sentence or not? The loss is just the sum of the individual log losses for the outputs. If some labels are more important you can scale them accordingly in your loss function."
Low-dimensional path representation learning,"Assuming that the goal is to find paths which are similar with each other in the dataset, I would suggest trying to directly compare pairs of paths with an appropriate similarity/distance function. Since the order in the path is clearly relevant, I think a sequence-based measure like the Levenshtein edit distance is a good candidate.

The idea would be to calculate the distance between every pair of paths in the dataset. Once this is done the matrix of distances can be used to cluster similar paths together.

I think that the only potential issue with this approach is computational complexity: in case there are many paths in the dataset, computing all the pairs of distances could be costly."
Naive Bayes as a baseline model in an NLP task,"Assuming that the Preprocessed_Text column contains a regular string, you don't have to do any kind of join since you variable text is a single string.
It's indeed recommended to calculate the bag of words representation only on the training set. It's ""cleaner"" in the sense that it prevents any possible data leakage, and it's more coherent with respect to applying the model to any fresh test set. However in this case there might be out-of-vocabulary words in the test set, this is normal. You can do this by splitting the ""preprocessed text"" data first, then calling cv.fit_transform() only on the training instances. Later the test set instances are encoded using cv.transform(), which just maps the new text using the previous representation."
Heauristics for a NER model prediction,"Assuming that there are almost always clear markers at the beginning of the enumeration, i.e. either ""required skills"" or ""nice to have"" (or any variant of these two), I would suggest trying to add custom features for example:

last marker seen from the current position, a categorical value for either ""Nicetohave"" or ""mandatoryskill"" (actually two boolean features with OHE)
distance in number of words from the current position to the last marker seen

Obtaining values for these features would require a preprocessing step where the markers are extracted and/or labelled, probably with some simple string matching (assuming that there are not too many variants of the markers)."
To extract the skills required for the job given the job description [closed],"Assuming you already have the raw text, you can do the followings.

Create train data:

You need to create set words and bigrams labeled as skills. There might be some available lists to help you out. Otherwise, generate your list using resources such as wordnet and thesaurus. You can also start with a short list based on the data you have and then expand it using word2vec or similar word embedding techniques. For example, we start with a list that contains coding as one of the skills. Then, we query the word2vec pretrained model for closest words/bigrams. There is a fair chance you will end up with programming, software coding, and computer programming.

Another approach would be to cluster the words and bigrams in your dataset using word embedding techniques. Then, look into your clusters to see which ones contain the skill set.

Note that any word/expression not on your list will not be considered as required skills. Therefore, you may need to expand your list after a few trials.

Detecting the Skills

Tokenize your raw text into words and expressions
Remove stop words
Encode your tokens using an embedding (Word2Vec, FastText, etc)
Use the list from the previous step to add labels to your data (anything on the list is True, other as False)
Train a binary classifier (Naive Bayes classifier should be good enough)
Evaluate your model, feature set, and labels. If needed, refine and repeat."
Creating training data,"Assuming you are doing supervized learning to train a model that when deployed will take text as input and output a label (e.g., topic) or class probability, then what you probably want to do is balanced, stratified sampling. Assuming sufficient labelled data, ensure that your final training set has a balanced number of text examples for each class/label. Depending on your situation, you may need to over/under sample or somehow deal with the problem of highly imbalanced classes (see 8 tactics to combat imbalanced classes).

The simplest NLP approach to use a bag of words technique, simply indicating the presence/absence of a word in the sentence. Thus each sentence becomes represented as vector of length n, where n = the number of unique words in your data set.

data_set <- c(""the big dog"", ""the small cat"", ""the big and fat cow"")
words <- strsplit(data_set, split = "" "") #tokenize sentences
words
##[[1]]
##[1] ""the"" ""big"" ""dog""
##
##[[2]]
##[1] ""the""   ""small"" ""cat""  
##
##[[3]]
##[1] ""the"" ""big"" ""and"" ""fat"" ""cow""


vec <- unique(unlist(words)) #vector representation of sentences
##[1] ""the""   ""big""   ""dog""   ""small"" ""cat""   ""and""  
##[7] ""fat""   ""cow"" 

m <- matrix(nrow = length(data_set), ncol = length(vec))

for (i in 1:length(words)) { #iterate the index of tokenized sentences
  vec_rep <- vec %in% words[[i]] #create binary word-feature vector
  m[i,] <- vec_rep #update matrix
}

df <- data.frame(m, row.names = NULL)
names(df) <- vec
df
##   the   big   dog small   cat   and   fat   cow
##1 TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE
##2 TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE
##3 TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE


Sometimes you can increase performance by adding bi-gram and tri-gram features.

##given: ""the big dog""
##unigrams <- {the, big, dog}
##bigrams <- {the big, big dog}
##trigrams <- {the big dog}


Sometimes weighting words by their frequency improves performance or computing the tf-idf.

Another way to increase performance, in my experience, has been custom language feature engineering, especially if the data is from social media sources replete with spelling errors, acronyms, slang, and other word variants. Standard NLP approaches will typically remove stop words (e.g., the, a/an, this/that, etc.) from vector representations (because closed class, high frequency words often don't help discriminate among class/label boundaries). Because vector representations are typically highly dimensional (approximately num of unique words in the corpus/data set), dimensionality reductions techniques can increase performance. For example, one can compute chi-sq, info gain, etc. on a word feature's distribution across classes -- only keep those features/words above some threshold or below some pre-established p value)."
what is the first input to the decoder in a transformer model?,"At each decoding time step, the decoder receives 2 inputs:

the encoder output: this is computed once and is fed to all layers of the decoder at each decoding time step as key (
K
endec
ùêæ
ùëí
ùëõ
ùëë
ùëí
ùëê
) and value (
V
endec
ùëâ
ùëí
ùëõ
ùëë
ùëí
ùëê
) for the encoder-decoder attention blocks.
the target tokens decoded up to the current decoding step: for the first step, the matrix contains in its first position a special token, normally </s>. After each decoding step 
k
ùëò
, the result of the decoder at position 
k
ùëò
 is written to the target tokens matrix at position 
k+1
ùëò
+
1
, and then the next decoding step takes place.

For instance, in the fairseq implementation of the decoding, you can see how they create the target tokens matrix and fill it with padding here and then how they place an EOS token (</s>) at the first position here.

As you have tagged your question with the bert tag, you should know that what I described before only applies to the sequence-to-sequence transduction task way of using the Transformer (i.e. when used for machine translation), and this is not how BERT works. BERT is trained on a masked language model loss which makes its use at inference time much different than the NMT Transformer."
"in NLP academic paper, would it be okay to refer the ""token embeddings"" as ""tokens""?","At least to me, it would sound strange, as I would understand tokens as the discrete textual units they are, not their assigned vectors.

I would suggest that you don't try to force nonstandard simplifications. Just say what you want, in a technically accurate way, preferably using short sentences to avoid making it difficult for the reader to follow your discourse."
Theano vs Tensorflow for building Neural Networks for NLP tasks,"At the end this comes down to one's opinion and experience but I ll attempt to give you a decent answer.

Theano has been around longer and you will be able to find more resources on it. Both to learn and people's published work on it.

On the other hand Tensorflow has been getting better and better since it came out. It has a free online course available and Google even released an open model for natural language understanding.

You may also have to check what hardware you have available. Tensorflow does not run on as many hardware as Theano currently. (Although they are actively working on Windows and OpenCl support.)

A good comparison on various Deep Learning Frameworks can be found there."
What would be the target input for Transformer Decoder during test phase?,"At training time, the input to the decoder is the target sentence tokens, which are indeed unknown at the test time. What you call the second input are the desired outputs, which are not usually referred to as an input to the decoder, 1. for clarity, 2. they are technically input to the loss function.

At test time, we do not need the loss function, but we still need to pass some input to the decoder. The decoding proceeds autoregressively, i.e., at each decoding step, we execute the decoder layers and get a probability distribution over the target tokens. We select one token (typically the best-scoring one, but it gets trickier with beam search) and append it to the input of the decoder. It means that the input to the decoder is generated one token at a time, gradually as the sentence is decoded."
How word2vec understands the relationship between numbers?,"Attempting an answer from the understanding I have of word2vec and deep learning through my personal experience and some live presentations of Yoshua Bengio and Yann Le Cun that I had the chance to attend.

As mentioned in the question, I think too that numbers are grouped together as they are have similar functions in the sentences where they are found.

For the reason of how they get ordered, as said in my comment: Intuitively I would say that it will be true for small numbers and less true for big numbers (the way 7-9-8 are ordered seems to go that way). When we speak we often say things like: ""I ate 1 or 2 apples"" or ""I will buy 2 or 3 things"" but rarely ""I will buy 3 or 2 things"". I think this is how it ends up ordering the numbers. Bigger numbers will appear less in a corpus so they will most probably not be ordered as precisely.

When looking at tutorials about word2vec, an example that is often given is that, if we see the relationship between 2 words as a vector, the final representation of the words by the model will give that the vector linking ""man"" to ""woman"" is approximately the same as the one linking ""king"" to ""queen"". Or that the vector linking ""Paris"" to ""France"" is approximately equal to the one linking ""Rome"" to ""Italy"". This even allows to do operations such as: France + Italy - Paris = Rome! The clear mechanics of how these relationships are so accurate (compared to other approaches) are, as of today, still obscur to the scientific community.

If these kind of operations works for countries and main cities, it seems logical that the vectors ""3-2"" and ""2-1"" point in the same direction, i.e. that 1-2-3 are ordered (given that, as said previously, consecutive numbers are more likely to be used together in a sentence). Of course, to achieve this, the numbers have to frequently appear in the training corpus.

I am very happy that someone showed this result with numbers because the way word2vec has been trained in most of the models that are available online did not includ digits. They were replaced by some symbols i.e., 12.34 becoming ##.##, and such observations cannot be made.

Any suggestion to improve this answer is welcome!"
How does attention mechanism learn?,"Attention weights are learned through backpropagation, just like canonical layer weights.

The hard part about attention models is to learn how the math underlying alignment works. Different formulations of attention compute alignment scores in different ways. The main is Bahdanau attention, formulated here. The other is Luong's, provided in several variants in the original paper. Transformers have several self-attention layers instead (I just found a great exaplanation here).

However, backprop lies at the basis of all them. I know it's amazing how attention alignment scores can improve the performance of our models while using the canonical learning technique intact."
What is auxiliary loss in Character-level Transformer model?,"Auxiliary losses are additional terms added to your global loss to induce learning further upstream in your model and can be seen as a way to combat vanishing gradients.

An early reference would be how Google's Inception model uses auxiliary classifiers. The general idea is that you take an intermediate output of your model and use it as the prediction in a separate loss function. So during training, your model is acting as both a global model (all parameters subject to that final global loss) and a model composed of sub-models with specific terms applying exclusively to your auxiliary tasks.

In practical terms, you can follow this example. If the link ever dies, here's a generic view:

# let's say we have two auxiliary losses, and thus two auxiliary outputs
final_task_prediction, predictions = model.forward(x)

global_loss = loss(final_task_prediction, y)
intermediate_loss_1 = aux_loss_1(predictions[0], aux_y_1)
intermediate_loss_2 = aux_loss_2(predictions[1], aux_y_2)

# can weight auxiliary losses however we choose
total_loss = global_loss + .4*intermediate_loss_1 + .8*intermediate_loss_2
total_loss.backward()


As an example, say we have an input sequence ABCDEF. To quickly run through the auxiliary losses in the Al-Rfou et al. paper

Multiple positions works as an auxiliary task of ""given A, predict B, then given B (and our internal memory of A), predict C"" (etc)

Intermediate hidden representations works the same as multiple positions, except intermediate layers also get their own loss terms

Multiple predictions breaks the target prediction sequence into multiple subsequences, so our auxiliary task is ""given A predict B,C ; given B, predict C,D"" (etc)

Hope this helps!"
Backpropagation of a transformer,"Backpropagation extends to the full model, through all decoder and encoder layers up to the embedding tables."
Training a model purely on weak labels,"Based on a quick read of the paper linked in the comment:

I just don't understand if your model (BERT or not) is only trained on these weak labels then you are treating them as ""ground-truth"",

Correct, but only for the training: the model is trained to recognize the labels obtained with a ""quick and dirty"" method.

and more importantly, don't you already know how to create ""ground-truth"" (by the rules-based system) ??? What's the point of the 2nd step?

No, because the real ground truth they are interested in is not those from the ""quick and dirty"" method. If they were, it would indeed be sufficient to run their rule-based system. The goal is to predict the labels obtained in what the authors call the ""Gold Standard Corpus"", which was manually annotated and never seen by the model.

Typically the quick and dirty method will result in some classification errors. The point of tuning the model with these labels is to see if the model can extrapolate from these low-quality labels to high-quality labels. This ability to generalize beyond the training data is based on the underlying semantic information contained in the original BERT-like model. For example this model might be able to associate a specific sport like swimming with ""physical activity"", even though the weak supervision doesn't contain this association."
How do we pass data to a RNN?,"Based on how much i got to know i propose the correct scheme would be third one i.e.

Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss) words from each article (for the sake of simplicity let us assume batch size = m)
Set the initial hidden state H0 = [0,0,..,0]
Calculate loss and gradient on this batch and update the parameters
We move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration
Do this to the end

Why we wont take any but Scheme 1 and Scheme 3 is because: H0 should only be initialized with a vector of zero when there is no context available (if you have some additional information, why not use it?!). The objective is to maximize the probability of each article and the sentences are not independent. Also, there is no reason to not treat period ""."" as a step in itself. We do calculate its word embedding too

Why not Scheme 1 - Because of catastrophic interference ( As the network in Scheme 1 fed in same input again and again ) combined with slow learning it would result in"
A multi label text classification problem,"Based on some discussions and on the commentaries, the conclusion is that this problem could be rather considered as one of the following NLP tasks (some of which are pretty similar..) :

Q&A (as suggested by @Akavall too)
Intent Classification (or NER)
One shot Learning
Semantic Role Labeling
Sequence Labeling (as suggested by @Erwan)

Thanks!"
Spacy custom POS tagging for medical concepts,"Based on the example, it looks like you need more than simple POS tagging. Thankfully there is a full subdomain of NLP devoted to biomedical data, and there are many tools available which can help with this kind of task:

In case the data is made of biomedical research papers, you will find a lot of resources related to the Medline and PubMedCentral databases:
UMLS and the tool MetaMap
PubTator, a recent annotated version of the biomedical literature.
SemRep for relations.
cTakes is another annotator system which is more specialized with clinical texts.
SciSpacy is a Spacy variant specialized for biomedical text. It can also annotate medical terms with UMLS labels.

The last one in particular seems particularly appropriate in your case. biomedical text presents a lot of specific difficulties which cannot be handled with general domain models.

Note that there are probably more tools and resources, this a very active domain.

(disclaimer: I recycled a large part of an older answer)"
Document similarity matching between Doc2Vec documents,"Based on what you have shared, it seems like Doc2Vec should be suited to your objective.

That said, I think the Doc in the package name can lead people astray. It gives the impression that you can feed in any documents and it will find the similarities among them. And while you certainly can feed whole documents, the reality is that the similarity / dissimilarity may not be aligned in a useful way to your task. I have found that training the model on smaller chunks of the document, like sentences or paragraphs, allows for the model to identify subtle differences that can then be aggregated to the whole document.

In your case, are the 17 documents you mentioned, exemplars of 17 different types of documents whose label you want to assign to the entire corpus? If so, Doc2Vec may be overkill. Have you looked into using Tf-Idf? Sometimes this approach works well if you don't necessarily need the word embeddings. Just some food for thought."
How to interpret Hashingvectorizer representation?,"Basic Background

Imagine the process of count vectorizer: you first create a vocabulary which maps each word (or n-gram) to an integer index (index in the document term matrix). Then, for each document, you count number of times a word appears and set that value at appropriate index to build vector representation for the document.
This can potentially create a very large number of features since each n-gram/token is one feature.
Even if you want to limit the total number of features by using some trick like top-N words by occurrence, you still need to calculate and hold in memory the map of all word-counts. This can be potentially prohibitive in some applications.
Similar problem happens for TfIDf, where you additionally store the mapping of word to document occurance for calculating the IDf part.
Either way, you are doing multiple passes over the data and/or potentially large amount of memory consumption.
The problem is also with bounds or predictability: you do not know the potential memory usage upfront in first phase.
Hashing vectorizer can build document representation for all documents in one single pass over the data and still keep memory bounded (not necessarily small, the size depends on size of hash-table).
In a single pass, you calculate hash of a token. Based on the hash value, you increment the count of particular index in the hash-table (the array underlying the hash table implementation). You get representation of current document without looking at every other document in the corpus.
This gives rise to problem with representation accuracy. Two different tokens may have hash collision.

So you are in effect trading [representation accuracy and explanatory power] Vs. [space (bounded predictable memory usage) and time (no multiple passes on the data)].

Answers to your specific questions

There is a (sort) correlation between input words and features: through the hash function. But this correlation is potentially defective (hash-collisions) and there's no inverse transformation (you can't say what word is represented by feature number 207).
There's no fit and transform. For a fixed hash-function, no dataset specific learning is happening (ala word2vec).
There's no semantic interpretation of the distance. Two words semantically similar words may not be close to each other in the representation. As long as two almost (syntactically, based on tokens) similar documents are close enough, it will work on text classification.

Why Would It Work?

Given these information, you are right in being skeptical: why on earth this should work? The answer is empirical: a randomized representation like hashing works reasonably well in practice (the benefits from exact count based representation are not that great). There might be some theoretical explanation too but I don't know it enough. If curious, you can probably read up this paper."
How to generalize comments using NLP,"Basically, we want to cluster similar comments and assign a name/entity to it. I suggest you to use Doc2Vec to convert the comments to fixed-sized vectors. Each of your comments will then be a n-dimensional vector. Comments with similar words/phrases will lie in the close proximity of one another.

Now, using K-Means Clustering, we can form clusters of vectors that represent comments having a similar meaning. Once the clusters are formed, assign a name to each of them.

For a given sample ( comment ), the model will first transform the given comment into a vector. The model will then check for a cluster that is nearest to the given sample's vector. The output will be the name of the nearest cluster."
Categorization of Natural Language Processing Tasks,"Basically, you need to know about the unsupervised learning tasks in NLP. For this, we mostly vectorize the input sentences using an embedding matrix.

Text Summarization:

Text Summarization not based on Neural Networks is used in a number of systems. These systems vectorize the input and then rank the sentences using a ranking algorithm ( like TextRank ) and cosine similarity based on their importance. The most important sentences are then given as output. You can read more here.

Similarity between two documents:

If you have a powerful document vectorizer like Doc2Vec, then the vectors of similar documents are similar to each other with some score. This score can be derived using cosine similarity.

A technology blog will have a higher score of similarity with a computer science based blog rather than a holiday/tourism blog.

Such systems are useful in many use cases. Clustering algorithms have also been applied here.

Sentiment Analysis is not an unsupervised learning task. It is a classification task:

For training a sentiment analysis model, you need a dataset which consists of text and its corresponding sentiment ( categorical or binary ). Such a model could not be achieved through clustering or ranking methods. But, yes, these models use word embedding like the other models.

Some links which describe the basic tasks categorized under NLP:

https://natural-language-understanding.fandom.com/wiki/List_of_natural_language_processing_tasks

https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/

‚ÄúDeep Learning for NLP: An Overview of Recent Trends‚Äù by Elvis https://link.medium.com/iItIayc0NW

https://blog.algorithmia.com/introduction-natural-language-processing-nlp/"
Questions of understanding - Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation,"Beam search is just an optimization technique which selects the top 
k
ùëò
 most probable ""solutions"" out of a large set of candidate ""solutions"". You can see it in algorithm 1 in the paper: the function kbest just pick the top 
k
ùëò
 items.

by grouping together hypotheses that have met the same number of constraints into banks

Apparently they use the word ""bank"" to mean a set of hypothesises grouped together, there's nothing much to it.

How would you describe these terms and are there any tutorial sources you would recommend for understanding the dynamic beam allocation?

In general don't expect tutorials for concepts in research papers, it's very rare that state of the art research becomes mainstream enough for that :)

Usually you have to work your way though the references when you don't understand something."
what's the motivation behind BERT masking 2 words in a sentence?,"Because BERT accepts the artificial assumption of independence between masked tokens, presumably because it makes the problem simpler and yet gave excellent results. This is not discussed by authors in the article or anywhere else to my knowledge.

Later works like XLNet have worked towards eliminating such an independence assumption, as well as other potential problems identified in BERT. However, despite improving on BERT's results on downstream tasks, XLNet has not gained the same level of attention and amount of derived works. In my opinion, this is because the improvement did not justify the complexity introduced by the permutation language modeling objective.

The same assumption is made by other pre-training approaches, like Electra's adversarial training. The authors argue that this assumption isn‚Äôt too bad because few tokens are actually masked, and it simplifies the approach."
Why do we need to add START <s> + END </s> symbols when using Recurrent Neural Nets for Sequence-to-Sequence Models?,"Because of the encoder-decoder structure. The encoder reads the input sequence to construct an embedding representation of the sequence. Terminating the input in an end-of-sequence (EOS) token signals to the encoder that when it receives that input, the output needs to be the finalized embedding. We (normally) don't care about intermediate states of the embedding, and we don't want the encoder to have to guess as to whether or not the input sentence is complete or not.

The EOS token is important for the decoder as well: the explicit ""end"" token allows the decoder to emit arbitrary-length sequences. The decoder will tell us when it's done emitting tokens: without an ""end"" token, we would have no idea when the decoder is done talking to us and continuing to emit tokens will produce gibberish.

The start-of-sequence (SOS) token is more important for the decoder: the decoder will progress by taking the tokens it emits as inputs (along with the embedding and hidden state, or using the embedding to initialize the hidden state), so before it has emitted anything it needs a token of some kind to start with. Hence, the SOS token.

Additionally, if we're using a bidirectional RNN for the encoder, we're definitely going to want to use both SOS and EOS tokens since the SOS token will signal to the reversed-input layer when the input is complete (otherwise, how would it know?)."
"BERT - The purpose of summing token embedding, positional embedding and segment embedding","Because these are the pieces of information needed to accomplish the loss' tasks, that is, both masked language modeling (i.e. predicting the masked tokens) and next sentence prediction (i.e. predict whether the second segment followed the first segment in the original text). These are the specific reasons:

Token embeddings are needed to identify the word/subword being processed as input, as well as the token being masked.
Positional embeddings are needed because without them, the Transformer cannot distinguish the same token in different positions (unlike recurrent networks like LSTMs). For more details, you can refer to this answer.
Sentence embeddings are needed for the secondary task of the loss: next sentence prediction. They are needed to easily tell apart the different parts of the input. For more details, you can refer to this answer.

Also, note that a normal Transformer architecture already adds token embeddings and positional embeddings.

The reason why these embeddings are added up instead of e.g. concatenated can be found in this answer. Adding them up you basically are learning the optimal way of combining them, instead of fixing it a priori."
Extract names from email address,"Before talking about the solution, why don't you focus on the content instead? I think it would be more helpful to solve your problem, considering that most of the email addresses end with the sender's sign, Name Surname. Also, the probability of failing to obtain this information from an email address is much higher than the probability of failing to get it from the content. Especially, this is the case with company email addresses which might not contain the whole name in the email address (first letter of name and surname e.g. John Travolta - jtravolta@company.com), but it must contain the author's full name (at least the name) at the end. Furthermore, consider that plenty of email addresses will contain only name or surname or neither of them, but substitutive words like superboy122133@+++.com :D. But most of the email apps contain a default sign that includes name and surname. In addition, you can combine these two techniques. That is, combine the email address data with email content data so that, if it is infeasible or impracticable to obtain data from one of these, then you can use another one.

However, if let's say you have to do it with nothing but an email address I think using Machine Learning techniques would be overrating or overestimating the problem. Also, using non-machine learning techniques does not mean you are simplifying the solution, all these techniques give the best outcome when they are applied in the correct context. Let's imagine a simple situation: if you know or can easily infer that [tax] = 0.2 * [salary] + 20 $, why would you find (or fit) this equation using Machine Learning?

Unless you have data in the format of |email address, fullname|, you shouldn't start with using Machine Learning. (If you would have |email address, fullname| data, as an option, you would train a model to learn the general relationship between the email address and full name, thus you would identify similar email addresses).

However, in this current situation, one approach would be finding all possible patterns in the email addresses. Which can be

first letter of the name and surname jtravolta@+++.com
name, special characters and surname john_travolta@+++.com
name and numbers john1954@+++.com
...

Then these features that are extracted from email addresses using identified patterns can be compared with other emails either hashing or using string distance algorithms.

One alternative approach would be having a hashed dictionary of all available names and surnames, then you can cut pieces(substrings) from the email address then hash them to find the names and surnames from the address (Of course, vice versa would be highly inefficient). The email addresses that have the most similar, properties would be matched.

Another solution would be, using the above-mentioned patterns, you can generate a bunch of artificial email addresses. Considering that it is highly probable that there is not a dataset that includes the name and surname of people and their one or more email addresses, data augmentation is the first order of business. (I am not sure whether the data augmentation term fits this situation. If it does not then let's say data generation). So your input would be Name Surname (you can include middle name, number, etc.), and output would be randomly generated email addresses based on the pre-defined patterns. The number of emails that are generated for a single input should be randomly selected also, but be careful about the generation of the same email address more than once. E.g. input -> John Travolta -> output -> j_travolta12@+++.com, john.t.99@+++.com, john.travolta@+++.com (Lets suppose for this example we randomly choose 3 emails to be generated).

Then after you created, email addresses with all possible (almost) patterns you can get help from Machine Learning techniques. So the model might give you a probability with the relevant name and surname. (Also, you can configure the output so that it would give you top n name and surnames which have higher probability)

Another thing that is needed to be considered is the possibility of two different persons having the same name and surname. Lastly, independently from using which approach your solution cannot be perfect because, for example, it is not possible to understand whether the character 'j' stands for John or Jake in the email address. Thus, if you can integrate the email content into your solution, that will increase the performance drastically.

Update Accordingly: Check this answer which does not exactly answer your problem, but the context is the same."
Does BERT use GLoVE?,"BERT cannot use GloVe embeddings, simply because it uses a different input segmentation. GloVe works with the traditional word-like tokens, whereas BERT segments its input into subword units called word-pieces. On one hand, it ensures there are no out-of-vocabulary tokens, on the other hand, totally unknown words get split into characters and BERT probably cannot make much sense of them either.

Anyway, BERT learns its custom word-piece embeddings jointly with the entire model. They cannot carry the same type of semantic information as word2vec or GloVe because they are often only word fragments and BERT needs to make sense of them in the later layers.

You might say that inputs are one-hot vectors if you want, but as almost always, it is just a useful didactic abstraction. All modern deep learning frameworks implement embedding lookup just by direct indexing, multiplying the embedding matrix with a one-hot-vector would be just wasteful."
How can word2vec or BERT be used for previously unseen words,"BERT does not provide word-level representations, but subword representations. This implies that when an unseen word is presented to BERT, it will slice it into multiple subwords, even reaching character subwords if needed. That is how it deals with unseen words. Therefore, BERT can handle out-of-vocabulary words. Some other questions and answers in this site can help you with the implementation details of BERT's subword tokenization, e.g. this, this or this.

On the other hand, word2vec is a static table of words and vectors, so it is just meant to represent words that are already in its vocabulary."
How pre-trained BERT model generates word embeddings for out of vocabulary words?,"BERT does not provide word-level representations, but subword representations. You may want to combine the vectors of all subwords of the same word (e.g. by averaging them), but that is up to you, BERT only gives you the subword vectors.

Subwords are used for representing both the input text and the output tokens. When an unseen word is presented to BERT, it will be sliced into multiple subwords, even reaching character subwords if needed. That is how it deals with unseen words.

ELMo is very different: it ingests characters and generate word-level representations. The fact that it ingests the characters of each word instead of a single token for representing the whole word is what grants ELMo the ability to handle unseen words."
BERT word embedings for finding word definition,"BERT generates contextualized word embeddings, which means that BERTprovides the most accurate embeddings when a word is in a sentence(context). For each of the words within the sentence, BERT will generate a vector of numbers. In your case, you will have a good representation of the word ""bank"". So if you have a sentence for all the other words that you are trying to match with ""bank"", all you need to do is to extract the embeddings for them, then compare them with a similarity matric (i.e. cosine similarity). this way you are ranking them to see which ones are most correlated to ""bank""."
Hierarchical Classification - machine learning model with NLP,"Bert is a model that is able to extract meaning from text and make good classifications, even if the text hasn't been learned before.

That's why you can train a Bert model with many sentences that cover all classes, and then you should be able to classify any text.

https://www.analyticsvidhya.com/blog/2021/12/text-classification-using-bert-and-tensorflow/

https://www.kaggle.com/code/merishnasuwal/document-classification-using-bert

This is the most universal solution adapted to any text.

Here are all text classification models:

https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads

They are mostly built to classify sentiment, but they could classify anything, like this one:

https://huggingface.co/cardiffnlp/roberta-large-tweet-topic-multi-all?text=I%27m+sure+the+%7B%40Tampa+Bay+Lightning%40%7D+would%E2%80%99ve+rather+faced+the+Flyers+but+man+does+their+experience+versus+the+Blue+Jackets+this+year+and+last+help+them+a+lot+versus+this+Islanders+team.+Another+meat+grinder+upcoming+for+the+good+guys"
Can we use BERT for only word embedding and then use SVM/RNN to do intent classification?,"BERT is a transformer based model. The pre-trained BERT can be used for two purposes:

Fine-tuning: This involves, fine-tuning with the new data to a specific task such as classification or question-answering, etc. Here, the BERT itself acts like a classifier.
Extracting embeddings: Here, you can extract the pretrained embeddings. The difference between Word2Vec (or other word embeddings) and BERT is that BERT provides contextual embeddings, meaning, the embeddings of each word depends on its neighbouring words. However, since it's contextual embeddings, we can make an assumption that the fist token which is '[CLS]' captures the context can be treated as sentence embeddings as be used as input to 'SVM' or other classifer. But, in the case of RNN, you may want to take the pretrained embeddings of each token to form a sequence.

So, how you want to use BERT still remains a choice. But if you can fine-tune the BERT model, it would generally yield higher performance. But you'll have to validate it based on the experiments."
What is a 'hidden state' in BERT output?,"BERT is a transformer.

A transformer is made of several similar layers, stacked on top of each others.
Each layer have an input and an output. So the output of the layer n-1 is the input of the layer n.

The hidden state you mention is simply the output of each layer.

You might want to quickly look into this explanation of the Transformer architecture : https://jalammar.github.io/illustrated-transformer/

Note that BERT use only Encoders, no Decoders."
"How does BERT and GPT-2 encoding deal with token such as <|startoftext|>, <s>","BERT is not trained with this kind of special tokens, so the tokenizer is not expecting them and therefore it splits them as any other piece of normal text, and they will probably harm the obtained representations if you keep them. You should remove these special tokens from the input text.

In the case of GPT-2, OpenAI trained it only with <|endoftext|>, but it has to be added after the tokenization. Some people mistakenly add it before tokenization, leading to problems. <|startoftext|> is specific to the library gpt-2-simple."
Semantic text similarity using BERT,"BERT is trained on a combination of the losses for masked language modeling and next sentence prediction. For this, BERT receives as input the concatenation of the special token [CLS], the first sentence tokens, the special token [SEP], the second sentence tokens and a final [SEP].

[CLS] | First sentence tokens | [SEP] | Second sentence tokens | [SEP]

Some of the tokens in the sentences are ""masked out"" (i.e. replaced with the special token [MASK]).

BERT generates as output a sequence of the same length as the input. The masked language loss ensures that the masked tokens are guessed correctly. The next sentence prediction loss takes the output at the first position (the one associated with the [CLS] input and uses it as input to a small classification model to predict if the second sentence was the one actually following the first one in the original text where they come from.

Your task is neither masked language modeling nor next sentence prediction, so you need to train in your own training data. Given that your task consists of classification, you should use BERT's first token output ([CLS] output) and train a classifier to tell if your first and second sentences are semantically equivalent or not. For this, you can either:

train the small classification model that takes as input BERT's first token output (reuse BERT-generated features).

train not only the small classification model, but also the whole BERT, but using a smaller learning rate for it (fine-tuning).

In order to decide what's best in your case, you can have a look at this article.

In order to actually implement it, you could use the popular transformers python package, which is already prepared for fine-tuning BERT on custom tasks (e.g. see this tutorial)."
How BERT model differentiate words with different meanings? [closed],"BERT models work with sentences, not words: the self-attention in the transformer architecture is considering each token in respect to all other tokens in the sentence.

""I'm going to implement Transformers in Python"" vs. ""I'm going to watch Transformers on TV later tonight"" - there should be enough surrounding context to distinguish them. I.e. the John Firth quote: ""You shall know a word by the company it keeps.""

Note that the output of the the embedding layer (which is usually fed in to the first transformer layer) is the same for identical tokens (*): at this point the model has no way to tell the difference. It is only the interaction with the other tokens, as it passes through each layer, that enable a distinction to emerge.

Being able to distinguish between them also relies on the model having seen enough training sentences where each sense of a word was used.

*: pedantically, position codes are added in, before feeding into layer 1."
Trained BERT models perform unpredictably on test set,"BERT-style finetuning is known for its instability. Some aspects to take into account when having this kind of issues are:

The number of epochs typically used to finetune BERT models is normally around 3.
The main source of instability is that the authors of the original BERT article suggested using the Adam optimizer but disabling the bias compensation (such a variant became known as ""BertAdam"").
Currently, practitioners have shifted from Adam to AdamW as optimizer.
It is typical to do multiple ""restarts"", that is, train the model multiple times and choose the best performing one on the validation data.
Model checkpoints are normally saved after each epoch. The model we chose is the checkpoint with best validation loss among all epoch of every restart we tried.

There are two main articles that study BERT-like finetuning instabilities that may be of use to you. They describe in detail most of the aspects I mentioned before:

On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines
Revisiting Few-sample BERT Fine-tuning"
Sentiment analysis of tweets (Train model on a labelled dataset and use on some other unlabelled data),"Better approach would definitely be supervised learning model. There are two alternatives for you to go:

(1) What you could try is to use a transformer model that was trained on another sentiment case, like movie or restaurant reviews. First, you could try how this model works for your use-case and then use it to label your unlabeled data.

(2) Or you could label some tweets yourself (like 100-200) and then finetune another sentiment transformer model on this data. Then you need to label a lot less data then if you start from scratch."
Is there a good German Stemmer?,"Big problem and very good question!

I used spacy in the past, which has a German module. I guess stemming is not supported, but lemmatization.

Looking at the output below, I don't think that spacy will solve your problem to be honest. However, I just wanted to let you know about this option.

Spacy Lemmatization:

#pip install spacy
#python -m spacy download de

import spacy
nlp = spacy.load('de_core_news_sm')

mywords = ""Das ist schon sehr sch√∂n mit den Expertinnen und Experten""

for t in nlp.tokenizer(mywords):
    print(""Tokenized: %s | Lemma: %s"" %(t, t.lemma_))


Result:

Tokenized: Das | Lemma: der
Tokenized: ist | Lemma: sein
Tokenized: schon | Lemma: schon
Tokenized: sehr | Lemma: sehr
Tokenized: sch√∂n | Lemma: sch√∂n
Tokenized: mit | Lemma: mit
Tokenized: den | Lemma: der
Tokenized: Expertinnen | Lemma: Expertinnen
Tokenized: und | Lemma: und
Tokenized: Experten | Lemma: Experte"
Why are Chunking and IOB tags necessary?,"BIO(L) tagging is important (but as you correctly noted, not necessary) part of a NER pipeline. Main idea behind such split is to facilitate learning in following manner.

Take English as an example, some words will (most likely) never end a Named Entity, like adjectives, so the model will never tag them as the L(ast) part of a named entity. The same applies to the L-tag.

What is crucial, is that many models, like Conditional Random Fields, learn not only tags themselves, but also the transition probability, so, if you will get some chunk of text that is tagged as B_ O_ L_, that sequence is incorrect, but when you learn the transitions as well, model will figure out, that if you get a strong Beginning and End, the inside part should also be a entity part."
Using BM25 to rank words,"BM25 is usually used in information retrieval. In this task, you have a query and a lot of documents(maybe millions), and then you want to find a subset of these documents that are most relevant to your query. A ranking of a set of documents will be provided from the most relevant to the least.

If by efficient you mean fast in a computational way. I would say BM25 is pretty fast with respect to other algorithms that are using deep neural networks.

But if your asking if BM25 results are promising or not. This is debatable as BM25 is being used for a long time. People usually use it for the first step of ranking, then they do re-ranking with other powerful tools. It doesn't mean that BM25 is giving the best answers. But when you are dealing with thousands or millions of documents, This is a good choice in order to select only a subset of documents that have high scores with BM25 then later use a more accurate algorithm to rerank the results of BM25."
The differences between BNF and JSGF in NLP?,"BNF (Backus normal form) is a general framework that can be adopted to a variety of contexts.

JSGF stands for Java Speech Grammar Format, thus is designed to work the Java programming language.

If I wanted to leverage a larger community of knowledge and have my work compatible with as many systems as possible, I would use BNF. If I was coding in Java, I would use JSGF."
How to ensemble classifier incorporating all features in python?,"Boosting - is the ensemble which tries to add new models that do well where previous models lack. Bagging in scikit lets you send the base classifier as the parameter. Go through 1.11 of the link to understand more!

But since, you already have in mind that SVM performs better Voting Classifier which is present in sklearn.ensemble lets you give weights to the classifiers which you seem to be performing well. For instance in your question, SVM can be given more weight. It also has another parameter 'voting'. If hard, uses predicted class labels for majority rule voting else if soft, predicts the class label based on the argmax of the sums of the predicted probabilities."
Does finetuning BERT involving updating all of the parameters or just the final classification layer?,"Both approaches are reasonable. Updating the BERT weights will train for longer period of time, but should give more accurate results."
Difference between NCE-Loss and InfoNCE-Loss,"Both NCE loss and InfoNCE loss use positive examples and sampled negative examples to contrast a true data distribution with a noise distribution. As a byproduct, you can learn something useful like 
p(y|x)
ùëù
(
ùë¶
|
ùë•
)
 in the case of NCE, or high quality representations of your objects (e.g. documents, images, etc.) in the case of InfoNCE.

NCE casts the contrastive learning task as a binary classification problem, where we want to predict if a data point (a class and a context) came from the noise distribution or the true data distribution.

InfoNCE generalizes this idea and treats this task as a multiclass classification problem, where the goal is to predict which of the 
N
ùëÅ
 examples (1 positive and 
N‚àí1
ùëÅ
‚àí
1
 negatives) came from the true data distribution.

NCE uses the logistic function to generate probabilities, while InfoNCE generates probabilities from a calculation that looks more like softmax (i.e. 
f(y,x)
‚àë
N
i=1
f(
y
i
,x)
ùëì
(
ùë¶
,
ùë•
)
‚àë
ùëñ
=
1
ùëÅ
ùëì
(
ùë¶
ùëñ
,
ùë•
)
). Once you have probabilities, you can apply cross-entropy loss to both.

Examining the loss for NCE and InfoNCE for a single example with 1 negative sample for both NCE and InfoNCE, we see that one loss function cannot easily be derived from the other. See this blog post for the loss functions."
Why does all of NLP literature use Noise contrastive estimation loss for negative sampling instead of sampled softmax loss?,"Both negative sampling (derived from NCE) and sampled SoftMax use a few samples to bypass the calculation of full SoftMax.

The main problem comes from this comment in the linked pdf:

Sampled Softmax

(A faster way to train a softmax classifier)

which is only used for sampled SoftMax, although, negative sampling is as fast for the same reason that is working with few samples. If their performances are at the same level, this could be the reason why researchers are not convinced to switch over to sampled SoftMax. In academia, it is almost always the case that older methods are preferred over new, but equally-competent methods for the sake of credibility.

Negative sampling is NCE minus the logistic classifier. Roughly speaking, it only borrows the term ""F(target) + sum of F(negative sample)s"". Negative sampling is most prominently introduced in the Word2Vec paper in 2013 (as of now with 11K citations), and is backed by the mathematically rigorous NCE paper (2012). On the other hand, sampled SoftMax is introduced in this paper (2015) for a task-specific (Machine Translation) and biased approximation:

In this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an NMT model with a much larger target vocabulary

Noting that negative sampling also allows us to train ""with a much larger target vocabulary""."
spacy multi label classification help,"Bro. It's waste to do classification using spaCy, you can refer Deep learning techniques. But your question is different, spaCy needs dictionary format with labels Positive and negative, Here I will give sample snippet, like this frame your input data

# change input data to spaCy readable format 


train_examples = [] # it needs dict inside tuples insides dictionary (you will understand down)

for index, row in reviews_df.iterrows(): text = row['Text'] rating = row['Score']

label = {""POS"": True, ""NEG"": False} if rating == 1 else {'NEG':True, 'POS': False}

train_examples.append( Example.from_dict(nlp.make_doc(text), {'cats': label}) )


You can refer this https://www.machinelearningplus.com/nlp/custom-text-classification-spacy/ Clearly explained, Use Bert dude, BERT is SOTA model, that gives more accurate results, See here https://analyticsindiamag.com/a-beginners-guide-to-text-classification-using-bert-features/#:~:text=what%20BERT%20is.-,What%20is%20BERT%3F,both%20left%20and%20right%20context."
Need help with entity tagging,"But what about new entities (movie or production company name) that trained system hasn't seen how can we tag them. Re-training the model every time with new released movies won't be feasible.

A NER model should not have to be retrained to tag a new text it has not seen before. If trained successfully it will use information it learned from the labeled data and be able to apply it on new data. For your use case this could be information such as:

Capitalization - common for movie titles and production company names
Words used - frequently used words for either movie titles or company names
Where in text - movie titles perhaps appear early in texts

spaCy is a good library to get started with NER. Here is an example on how to train one using your own data."
True Negatives for Named Entity Recognition,"By definition, true negative is non-entity tokens in the text, agreed by gold standard. So;

TN: 5 = {announced, the, release, of}

But, in practice the number of true negative tokens are quite high and this heavily dominates the metrics, and any measure using TN (like accuracy) becomes highly controversial. Instead, preferred better metrics:

Recall: TP/(TP+FN) - coverage in all correct entities, and
Precision: TP/(TP+FP) - how reliable for a positive result
F1 Score (harmonic mean of precision and recall) can be used as a balanced metrics."
What does 'Linear regularities among words' mean?,"By linear regularities among words, he meant that ""Vectorized form of words should follow linear additive properties!""

V(""King"") - V(""Man"") + V(""Woman"") ~ V(""Queen)"
What features are generally used from Parse trees in classification process in NLP?,"By using a parse tree, you divide your sentence into parts. Suppose, in the example of sentiment analysis, you can use those parts to assign a positive/negative sentiment to each part and then take the cumulative effect of those parts.

This image will help you understand more. The first half has a negative sentiment(mainly because of the word ""dry"") but because of the word ""but"" and the usage of the word ""enjoyed"", the negative sentiment is turned into a positive sentiment.

As for using them, you can simply generate a word vector representation of the individual words in the sentence and use neurons in place of the parent nodes. Each neuron should be connected to another neuron through weights. All the leaf nodes will be the word vector representations of words of the sentence. The top parent neuron(in this case the top blue + symbol) should generate a positive/negative sentiment according to the sentence. This tree structure can be trained in a supervised manner.

Read this paper for a more through understanding.

Image credits: cs224.stanford.edu"
"What is the more natural parsing, the one that leads to the preferred reading of the sentence","Can anyone explain to me, what is more natural in English and why ?

This is a classic example of PP-attachment ambiguity (PP = prepositional phrase). For a full overview of the problem and some traditional approaches, check out this paper. Here I'll cover the basics.

The quick explanation is that the first analysis corresponds to the interpretation

Twain ( bought (a book (for Howells)) )


whereas the second analysis means

Twain ( bought (a book) (for Howells) )


The second one is probably the most natural interpretation: somebody (Twain) buys something (a book) for somebody else (Howells). So, in this case, the subject may be just buying a gift for somebody else.

On the other hand, in the first sentence, somebody (Twain again) buys something (a book again as well) but now that something was intended for somebody else (a book for Howells) yet the buyer is keeping it for himself.

The explanation is that, when parsing this sentence, the for-prepositional phrase is consumed when it is attached to the closest noun (book). Therefore, it's no longer available later to become the for-phrase beneficiary argument of the verb buy, which therefore can no longer have its gift-making meaning and switches instead to its purely transactional transfer-of-ownership meaning.

The branching of the trees represents these two interpretations, that is, the different syntactic ways in which the same linear order of constituents can be arranged. This phenomenon is one of the traditional examples that linguistic structures do not map exactly to the observable temporal sequences we perceive.

The interesting thing in your example is that the ""less typical"" interpretation (the first one, corresponding to the ""not-a-gift"" meaning) is being assigned a higher probability, while I would expect it to be much less likely, since the probability of the ""gift"" meaning should be much higher when the for-phrase has a person name as the head (Howells) ‚Äìpeople just don't go around buying books that belong to other people, but rather for them.

Hope this helps!"
How can i extract words from a single concatenated word?,"Can use a package that relies on a spellchecker to find the best way to split, like this one: https://pypi.org/project/compound-word-splitter/"
How does a CBoW model convert a word to a vector?,"CBoW (Continuous bag-of-words) is a theoretical architecture, not exactly a saved model or a library like gensim. Gensim might be an implementation of CBoW to which you feed a one-hot vector and get your word vector output.

CBoW as a theoretical model gives 'representational meaning' to a word based on the 'representational meaning' of the word surrounding it. 'Representational meaning' may just be a fancy term for word vectors. So basically the model builds word vectors out of surrounding word vectors possibly using negative sampling and the NCE loss, but the main point is that it fine-tunes these vectors until their 'representational meaning' becomes well-honed.

These well honed vectors are then probably callable from gensim through one-hot inputs."
How to incorporate keyboard positions on character level embeddings?,"Character keyboard position information is an example of noisy channel model information, an error that depends on how a word is transmitted. It is very common to add noisy channel model information to spell checkers, including spell checkers that use character-level embeddings.

Most character-level embedding models would automatically learn to model common transmission mistakes. Characters that are frequently confused in the dataset would be embedded closer to each other because they frequently co-occur. There would be minimal gain by explicitly adding channel information to a character-level embedding model during training."
Question answering (QA) vs Chatbots,"Chatbots and Q&A systems differ in their complexity as well as use cases. Let's discuss each of them separately.

Chatbots:

They can answer various questions asked during an interactive conversation. Interactive conversion means the system keeps a track of questions asked earlier and can engage in longer conversations. They have a sought of memory which helps answer in a more friendlier manner. Also, they retrieve information such as weather, stock prices from various sources. Hence, their ability is far beyond Q&A systems in this sense.

Examples could be Google Assistant, Alexa, Siri.

Q&A Systems:

They are programmed to answer questions only from a particular source of information of sometimes questions belonging to a common topic. The ""Ask a question"" dialog found on most websites is an example of such a system. They could be thought of a search engine which only works for a specific topic.

For instance, a Q&A system present on"
"In ChatGPT, what is the difference between Reinforcement-Learning-from-Human-Feedback and Data-Re-Label? [closed]","ChatGPT, being a generative model, generates sequences of tokens. There are no labels in the sense of a classification problem. Therefore, re-labeling using the reward signal does not make sense in the context of ChatGPT.

On the other hand, RLHF is used to lean the model toward generating desirable sequences."
What algorithms should I use to perform job classification based on resume data?,"Check out this link.

Here, they will take you through loading unstructured text to creating a wordcloud. You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used. The idea is to take the unstructured text and structure it somehow. You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices. You also have the option of stemming the words. If you stem words you will be able to detect different forms of words as the same word. For example, 'programmed' and 'programming' could be stemmed to 'program'. You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.

You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.

Example:

1) Load libraries and build the example data

library(tm)
library(SnowballC)

doc1 = ""I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C.""
job1 = ""Software Engineer""
doc2 = ""Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting.""
job2 = ""Quality Assurance""
doc3 = ""Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases.""
job3 = ""Software Engineer""
jobInfo = data.frame(""text"" = c(doc1,doc2,doc3),
                     ""job"" = c(job1,job2,job3))


2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.

# Convert to lowercase
jobInfo$text = sapply(jobInfo$text,tolower)

# Remove Punctuation
jobInfo$text = sapply(jobInfo$text,function(x) gsub(""[[:punct:]]"","" "",x))

# Remove extra white space
jobInfo$text = sapply(jobInfo$text,function(x) gsub(""[ ]+"","" "",x))

# Remove stop words
jobInfo$text = sapply(jobInfo$text, function(x){
  paste(setdiff(strsplit(x,"" "")[[1]],stopwords()),collapse="" "")
})

# Stem words (Also try without stemming?)
jobInfo$text = sapply(jobInfo$text, function(x)  {
  paste(setdiff(wordStem(strsplit(x,"" "")[[1]]),""""),collapse="" "")
})


3) Make a corpus source and document term matrix.

# Create Corpus Source
jobCorpus = Corpus(VectorSource(jobInfo$text))

# Create Document Term Matrix
jobDTM = DocumentTermMatrix(jobCorpus)

# Create Term Frequency Matrix
jobFreq = as.matrix(jobDTM)


Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.

Where you go from here is up to you. You can keep only specific (more common) words and use them as features in your model. Another way is to keep it simple and have a percentage of words used in each job description, say ""java"" would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'.

Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's."
HuggingFace hate detection model,"Check this article: https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa

Model training with explanation is given"
What is purpose of the [CLS] token and why is its encoding output important?,"CLS stands for classification and its there to represent sentence-level classification.

In short in order to make pooling scheme of BERT work this tag was introduced. I suggest reading up on this blog where this is also covered in detail."
Why CNN doesn't give higher accuracy over simple MLP network? [From Keras examples],"CNN (and RNN) models are not general improvements to the MLP design. They are specific choices that match certain types of problem. The CNN design works best when there is some local pattern in the data (which may repeat in other locations), and this is often the case when the inputs are images, audio or other similar signals.

The reuters example looks like a ""bag of words"" input. There is no local pattern or repeating relationships in that data that a CNN can take advantage of.

Your results with a CNN on this data set look reasonable to me. You have not made a mistake, but learned how a CNN really works on this data."
Twitter Data-Analyse: What can I do with the data?,"Community analysis implies graph analysis.

here is a short list of things you can work on:

People often reshares tweets among a certain social group. Minimum-cut method, Girvan‚ÄìNewman and Modularity maximization are someof the starting algorithms to extract these type of substructures.
You can try and find different hierarchies among the groups sharing a particular topics
You can try and analyse the lifetime of tweets for particular topics (survival analysis)

Analysing tweets is closer to graph analytics rather than NLP. Here is a great overview on community analysis. For coding and algorithms, please check graphX Spark library. If your data is not too large, networkX is easier. For survival analysis, lifeline is one of the easier options."
"Best tool for text pre-processing, involving tokenization, lemmatization, stop-word removal, feature vector extraction?","Comparing two libraries or tools in terms of these things is somewhat that is opinion dependent. Some people prefer NLTK for doing almost all the tasks. spacy has also gained quite reputation. But what is better for you depends on what you want do. In my personal experience, I have found that NLTK along with gensim libraries is all that I need to do all the Natural Language Processing tasks."
Training NLP with multiple text input features,"Concatenating the whole question and its answers in a RNN could be an option to try, but then always use a reserved special token (or various) to mark where the questions start. E.g. you could concatenate like:

Question text <1> answer 1 <2> answer 2 <3> answer 3 <4> answer 4

where <1>, <2>... are the special tokens so that the model, with enough examples, may be able to understand its meaning.

The ""enough examples"" is worth stressing, specially as this type of model may require a large complexity (in terms of number of parameters) to make it work, and hence you will need a large enough data set, possibly of the order of the 10k or larger. You can also test some data augmentation by mixing the order of the answers in the input, and of course changing the correct label."
Constructing a Maximum Entropy Classifier for Sentence Extraction,"Conjugate gradient descent is a variation on gradient descent. Gradient descent is a method of finding the minimum of a function by taking steps that reduce the error between the data and the model parameters. Conjugate gradient descent extends gradient descent by searching a plane, instead of a line. The plane is defined as a linear combination of the gradient vector and the previous descent step vector. Conjugate gradient descent is very good at finding the solutions to a set of sparse linear equations.

Gradient descent and variations are general methods for finding the best parameters. The best practice is to define your specific model and then call on a separate gradient descent package to search for optimal values (i.e., hyperparameters) for the model."
Why do Transformers need positional encodings?,"Consider the input sentence - ""I am good"".

In RNNs, we feed the sentence to the network word by word. That is, first the word ""I"" is passed as input, next the word ""am"" is passed, and so on. We feed the sentence word by word so that our network understands the sentence completely.

But with the transformer network, we don't follow the recurrence mechanism. So, instead of feeding the sentence word by word, we feed all the words in the sentence parallel to the network. Feeding the words in parallel helps in decreasing the training time and also helps in learning the long-term dependency.

We feed the words parallel to the transformer, the word order (position of the words in the sentence) is important. So, we should give some information about the word order to the transformer so that it can understand the sentence.

If we pass the input matrix directly to the transformer, it cannot understand the word order. So, instead of feeding the input matrix directly to the transformer, we need to add some information indicating the word order (position of the word) so that our network can understand the meaning of the sentence. To do this, we introduce a technique called positional encoding. Positional encoding, as the name suggests, is an encoding indicating the position of the word in a sentence (word order)."
Context Based Embeddings vs character based embeddings vs word based embeddings,"Context-based or contextual means that the vector contains information about the use of the word in a context of a sentence (or rarely a document). It thus does not make sense to talk about the word embeddings outside of the context of the sentence.

Models such as BERT use input segmentation into so-called subwords which is basically a statistical heuristic. Frequent words are kept intact, whereas infrequent words get segmented into smaller units (which often resemble stemming or morphological analysis, and often seem pretty random), ultimately keeping some parts of the input segmented into characters (this is typically the case of rare proper names). As a result, you get contextual vectors of subwords rather than words.

Character-based embeddings usually mean word-level embeddings inferred by from character input. For instance, ELMo used character-level inputs to get word embeddings that were further contextualized using a bi-directional LSTM. ELMo embeddings are thus both character-based and contextual.

Both when using sub-words and when using embeddings derived from characters, there are technically no OOV words. With subwords, the input breaks to characters (and all characters are always in the vocabulary). With character-level methods, you always get a vector from the characters. There is of course no guarantee that the characters are processed reasonably, but in most cases they are.

Models that use static word embeddings (such as Universal Sentence Encoder) typically reserve a special token for all unknown words (typically <unk>), so the model is not surprised by a random vector at the inference time. If you limit the vocabulary size in advance, the OOV tokens will naturally occur in the training data."
Converting to lowercase while creating dataset for NER using spacy,"Converting to lower case is a historical method to combat data sparsity. The idea is that if you don't have a lot of data, case usually does't matter, so remove the meaningless variable.

But for NER case is an important clue - capital words are more likely to be proper nouns, for example. So you definitely don't want to lower case things.

In general, aggressive preprocessing was necessary in the past, when data was scarce and it was hard to fit everything in memory if you had a lot of data. But that's no longer the case, so it's better to use unmodified text to give the models as many clues as possible."
Best practical algorithm for sentence similarity,"Cosine Similarity for Vector Space could be you answer.

Or you could calculate the eigenvector of each sentences. But the Problem is, what is similarity?

""This is a tree"", ""This is not a tree""

If you want to check the semantic meaning of the sentence you will need a wordvector dataset. With the wordvector dataset you will able to check the relationship between words. Example: (King - Man + woman = Queen)

Siraj Raval has a good python notebook for creating wordvector datasets."
Cosine Similarity but with weighting for vector indexes,"Cosine similarity won't work very well because it's only based on whether the rank at position 
i
ùëñ
 is the same in vector 1 and vector 2.

For instance the vectors [3,2,4,1,5] vs [2,3,5,1,4] will have very low similarity because 4 positions out of 5 are different, even though there are only two swaps between (2,3) and (4,5).

A much better way to measure the similarity between two rankings is Spearman Rank Correlation.

Note that (if I'm not mistaken) in this case you could also directly use Pearson correlation, since the numeric values are already ranks. Spearman just ranks the values before applying Pearson correlation. So normally the two will give you the same result.

Note also that this method won't give more weight to the top of the ranking than the bottom. I'm not aware of a measure which does this, other than defining a custom weighted measure."
How Sklearn-crfsuit interpret text features,"CRF models are not DL models, they use text features in the traditional way as categorical data (equivalent to one-hot-encoding). However they are different from other supervised ML methods because they exploit the sequential information of the text.

Typically the text sequence is represented word by word, and each word can have several additional ""features"" usually represented as columns. For example:

The     DET   the
dog     NOUN  dog
chases  VERB  chase
a       DET   a
cat     NOUN  cat


The real features used by the CRF are computed based on the rules defined before training the model. They can involve the position of the current word or any other position relative to the current one. The features are usually simple binary values, for examples ""doc[n-1][2] == 'DET'"" would represent the condition ""the previous word is a determiner"": ""previous word"" is indicated as n-1 in the sequence, column 2 indicates the POS column. The exact syntax to specify the rules may differ with different CRF implementations but the principle is the same."
Data amount for a very simple chatbot,"Crowdsource your results.

Your problem is very similar to writing Amazon Alexa skills. Creating intents, utterances and slots for Alexa skills. One of the recommended methods for developing this section of Alexa skills is to simply ask your friends and peers how they would go about asking for a given outcome. Crowdsourcing your inputs will give you some robust results because other people may think quite differently than you. I would recommend hopping into whatever Slack/Discord/chat channels you belong to and asking there. There may be more formal sources for such information out there but I am not directly aware of them."
Papers on anger detection in dialogues,"DeepMoji is a fun project that came out of MIT, which predicts emojis that are most related to an input sentence. Gimmicks aside, it seems perfectly adequate for your task of anger detection.

Paper
Blog post
Pretrained models: PyTorch or Keras."
Alternatives to doc2vec?,"Depending on your target task. If you are to classify documents, then e.g. fastText has it's own approach and there are other classification techniques, not strictly generating embeddings, like LSA / LDA (using topic modelling) or word mover distance."
making conclusions after sentiment analysis,"Depends what your Goal is.

But generally you see that positive and negative Sentiment probabilities are disjunct. Meaning your model focuses for one class only, negative one. ANd thats it, you conclude your data sample belongs to class ""Negative"". What that means, depends on the Definition of this class."
find bigrams in pandas,df_2['bigram_scored'] = df_2['bigram_finder'].apply(lambda x: x.score_ngrams(bigram_measures.raw_freq))
How to vectorize newline \n in tensorflow textVectorization layer?,Did you look at the tokens that are used during the vectorization? Maybe the '\n' character is not considered as a token. Thus it is not vectorized. That is why while trying to get back to the initial text from the vectorized form it doesn't appear.
Difference between text-based image retrieval and natural language object retrieval,"Disclaimer: I can only answer for the NLP part since I'm no expert for image processing.

I assume that text-based image retrieval is the task of finding the image (or the part of an image) which corresponds to a short text which exclusively describes the object. Practically it means that any content word (i.e. excluding grammatical words like determiners) in the text refers directly to the object: ""a bike"", ""a black cat"", ""the red car"", etc. For a ML process it means that there's nothing to analyze in the text, every word can directly be associated with a characteristic of the image.

By contrast Natural Language object retrieval involves analyzing the text. For instance ""the cat on the left of the picture"" is different than ""the picture on the left of the cat"", even though the words are the same. Additionally there can be different ways to refer to the same object: ""the book at the left of the shelf"" may be the same as ""the leftmost book"" or ""the book next to the green book"". There are usually many ways to express the same meaning with language, and that makes the task much more complex. Additionally I would assume that mapping positional descriptions to the image characteristics can be tricky: ""the man behind the tree"" or ""the second bridge"" in a 2D image requires the model to ""understand"" depth. In a picture with two dogs, ""the small dog"" requires the model to ""understand"" size relation between objects. Humans intuitively know how to interpret these sentences, but for a machine Natural Language Understanding hasn't been solved yet (it might never be)."
ChatGPT and my PhD research [closed],"Disclaimer: I don't know any detail about the ChatGPT model itself, this answer is based on my readings.

The question is quite vague, but ChatGPT has obvious flaws that are not difficult to find: even if it produces very high quality which looks exactly as if produced by a human, the main issue is always the same: it only looks like, it impressively mimics the data it was trained with, but of course the model doesn't have any understanding of what it produces. This implies that it cannot do any reasoning, and actually it doesn't have any way to even know if its information is correct or not, and it can happily invent things and make logical links which don't exist.

I found this article quite good at exhibiting the issue. There is also a whole discussion on StackOverflow and a well argued policy to ban ChatGPT answers, due to their complete lack of reliability."
Cosine similarity between sentence embeddings is always positive,"Disclaimer: This is actually a tentative explanation, it provides a possible answer, but it does not contain proof.

First of all, contrary to added comments, cosine similarity is not always in the range 
[0,1]
[
0
,
1
]
. This range is valid if the vectors contain positive values, but if negative values are allowed, negative cosine similarity is possible. Take for example two vectors like 
(‚àí1,1)
(
‚àí
1
,
1
)
 and 
(1,‚àí1)
(
1
,
‚àí
1
)
 which should give a cosine similarity of 
‚àí1
‚àí
1
 since the two vectors are on the same line but in opposite directions.

Going back to the question, we should ask if it's possible to have positive and negative values in vectors and still have only positive cosine similarity values. The answer is true, it is possible if the embedding vectors are contained into a nappe of a conical surface fixed in origin. (see Wikipedia: Conical surface). Basically, if you rotate the positive space section you still get positive cosine similarities.

Why would that happen with paraphrase-xlm-r-multilingual-v1? If you read the paper which describes the model Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - Reimers,Gurevych in the training detail section they explained that they used pooling layers on top of the BERT-like pre-trained models to obtain a fixed encoding. The pooling layer default model is averaging. Supposing in the convolutional layers' output vectors you get uniform values in range 
[‚àía,a]
[
‚àí
ùëé
,
ùëé
]
. Pooling those output vector values by averaging basically moves the output vectors to be closer together, producing smaller angles so towards positive cosine similarity. Even pooling by mode max tokens has a similar effect. This increases a lot the probability to have resulted embeddings to have only positive similarity, even if they allow positive and negative values in resulted embeddings. As I said I do not have proof, but considering how pooling works and if many convolutions are pooled this is a logical consequence. This does not mean that it is not possible to have negative similarities.

A way to verify this experiment, if you have a good quantity of random sentences would be to plot a histogram of their cosine similarity and to visually inspect that you have few values near zero and a monotonic increase of frequencies as we grow towards 
1
1
. This again would be just a hint.

[Later edit]

I have run some experiments myself to address the insights provided by @albus_c (thank you).

First of all I don't have sentences and anyway I don't use Python, so I generated artificial data (vectors with random values from a standard normal) in a matrix with rows being the instance vectors to be compared via cosine similarity. I noticed an interesting phenomena: on average the cosine similarity between random vectors have a shorter range of absolute values as the lengths of the vectors increases.

In the graph above we can see that for small vector sizes the empirical distribution covers the whole range 
[‚àí1,1]
[
‚àí
1
,
1
]
, and this range shrinks as vector sizes grows. This is important because the role of the pooling layers is to reduce the size of input vectors while retaining important information. As a consequence, if the pooling is aggressive the range of cosine similarity will increase on average. This is what @albus_c noticed, I think.

I also implemented a 2D pooling layer function over the random sample with average and max pooling. What I noticed, contrary to my intuition, is that averaging does not decrease the range of cosine, but keeps it in the same range. Due to the previous effect, however (pooling shrinks vector sizes and increase the range of cosine as a consequence) the final effect is that the cosine range is increased. In the case of max pooling, however, the cosine range is shrinked and moved drastically to positive values, as can be seen in the below graph.

In the graph above we can see in the upper left the histogram of cosine similarities on random vectors of size 
768
768
. On upper right histogram we have cosine similarities only for vectors of size 
384
384
 for comparison. I applied a 2d pooling layer with size 
2
2
 and slide 
2
2
. In the lower left graph we have similarities after max pooling. We clearly observe the values moving towards 
1
1
 in positive range. In the lower right we have similarities after mean pooling. We notice the range has increased compared to original (upper left) but is similar with the range onvectors of same size (upper right).

I did not worked out an analytic explanation for that, those are only simulations. The normal shape that appears is due to how I generated data, in real life it can look different, but I expect tendencies to remain the same. I have also experimented with different sizes of pooling. If the size of the pooling patch increases the effects increases dramatically on max pooling while remaining the same for averaging. If the slide of the pooling is lower than the size of the patch (the patches overlap) a correlation appears between resulted vectors and the cosine range shrinks more due to that correlation for both max and average pooling.

I think a proper analitycal explanation can be also given, and if I will have results and time I will update the answer again, but I do not expect to change what we already see in the simulations."
Pretrained handwritten OCR model,"Discover open-source deep learning code and pretrained models at Model Zoo

These are pre-trained sources available in the Github.

Handwritten Text Recognition with TensorFlow

More

Handwriting OCR

Handwritten Text Recognition (OCR) with MXNet Gluon

Some Helpful Resources:

Handwriting recognition and language modeling with MXNet Gluon
Handwriting OCR: Line segmentation with Gluon"
How to cluster n-grams?,"Distributional semantics models are based on representing an instance with the words that represent its meaning. Typically if one is interested in representing the meaning of a word 
w
i
ùë§
ùëñ
 then the words that represent its meaning are the ones which appear close to it, e.g. withing a window of N words preceding/following it.

Example where 
w
i
ùë§
ùëñ
 is the target word with a window +/- 2:

w
1
,
w
2
,...,
w
i‚àí3
,
w
i‚àí2
,
w
i‚àí1
,
w
i
,
w
i+1
,
w
i+2
,
w
i+3
,...,
w
N
ùë§
1
,
ùë§
2
,
.
.
.
,
ùë§
ùëñ
‚àí
3
,
ùë§
ùëñ
‚àí
2
,
ùë§
ùëñ
‚àí
1
,
ùë§
ùëñ
,
ùë§
ùëñ
+
1
,
ùë§
ùëñ
+
2
,
ùë§
ùëñ
+
3
,
.
.
.
,
ùë§
ùëÅ

So the ""meaning"" of 
w
i
ùë§
ùëñ
 in this instance would be represented by the four words in the context window: 
w
i‚àí2
,
w
i‚àí1
,
w
i+1
,
w
i+2
ùë§
ùëñ
‚àí
2
,
ùë§
ùëñ
‚àí
1
,
ùë§
ùëñ
+
1
,
ùë§
ùëñ
+
2
. By collecting occurrences of 
w
i
ùë§
ùëñ
 everywhere in the document one obtains a set of contexts that can be used in different ways. For example one could build a context vector 
a
1
,...
a
|V|
ùëé
1
,
.
.
.
ùëé
|
ùëâ
|
 over the vocabulary 
V
ùëâ
 where each cell 
a
k
ùëé
ùëò
 contains the frequency of 
w
k
‚ààV
ùë§
ùëò
‚àà
ùëâ
 in the context of the target word. By applying the same process with different words, each word can be represented with its context vector and then various operations can be made on these vectors: similarity measures, clustering, etc.

Generally clustering based on words semantics is done with specific methods such as Latent Semantics Analysis.

In theory the method can be applied to any unit (e.g. n-grams), but more complex units require more data, more memory and more computing power."
Identifying parts of speech for individual words (not texts),"Do not worry about single words. State-of-the-art PoST models are typically based on Recurrent Neural Networks (RNNs). They take sequences of words as inputs, and classify them based on their context words. If your model is good, a single word that is part of a broader expression should be classifier correctly.

The main dataset that I reccomend is the free part of the Penn Treebank dataset, available from python library nltk. From the same module you can also download the Brown corpus. there are compatibility problems between the two (the PoS tags are different), however you could assemble them together using universal tagset (less precise, but applicable to both).

Additionally, check this great Kaggle dataset. It's a dataset for both PoST and NER tasks, and you can use it to train your classifier. (It seems the tagset is the same of Penn Treebank, which means you might assemble the two datasets into an even bigger one.)"
Creating new classifications,"Do you mean a class label that the algorithm has never seen before? Then no, it is not possible. If you dont have labelled data for all your samples you can run a LDA, to get some topics and then assign labels based on the topics obtained. Even this approach wouldnt be really great. Since your problem is classification, I would recommend restricting your domain and using only samples for which the label is known. For better classification accuracy you can look into CNN."
Sentence similarity using Doc2vec,"Doc2Vec (and words vectors) need significant amount of data to learn useful vector representation. 50k sentences is not sufficient for this. To overcome this, you can feed word vectors as initial weights in Embedding Layer of network.

For example, code from following question :

How to implement LSTM using Doc2Vec vectors?

model_doc2vec = Sequential()
model_doc2vec.add(Embedding(voacabulary_dim, 100, input_length=longest_document, weights=[training_weights], trainable=False))
model_doc2vec.add(LSTM(units=10, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))
model_doc2vec.add(Flatten())
model_doc2vec.add(Dense(3, activation='softmax'))
model_doc2vec.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


Output of ""Flatten"" layer will be vector representation of a sentence / document.

Article with example code."
Word embeddings for Information Retrieval - Document search?,"Doc2Vec is on possible approach. With this, model learns to ""cluster"" similar sentences together.

Most simplistic approach is to aggregate word vectors but that ignores order of words. Details on few of the approaches :

https://towardsdatascience.com/sentence-embedding-3053db22ea77 https://medium.com/explorations-in-language-and-learning/how-to-obtain-sentence-vectors-2a6d88bd3c8b"
Text similarity using RNN,"Doc2Vec, Mikolov's paper will solve your problem. Here is the paper. You can find a gensim implementationhere. While using RNN, using GLOVE or Googl Word2Vec will be always useful even if your sequences are of the same length. To solve the variable sequence problem you can pad the sentences and use bucketing or just pad to one uniform length, truncating longer sentences and padding shorter sentences."
Which approach to select category based on keywords,"Does anyone have a better suggestion or are there just complete algorithms for this already available?

Apparently you want to do information retrieval (IR) but without the information part: normally an important part of the IR process is the set of documents that the user is searching for (for example text, images video). This is important because knowing the documents which correspond to a query gives semantic information about the keywords.

Now, if you only have the keywords available then there's no semantic to help with the clustering. So you are right that the only thing you can use is the spelling. i assume that by "" simple fuzzywuzzy"" you mean ""fuzzy matching"", i.e. using string similarity measures. I can think of two options:

You can compare each keyword with each other with some string similarity measure such as Levenshtein edit distance, Jaro, or characters n-gram based Jaccard, cosine etc.
You can represent each keyword as a vector based on the char n-grams in the word, then you can apply clustering with k-means for example.

Topic modeling techniques wouldn't work because you don't have large text documents."
Is Annoy a machine learning algorithm to find nearest neighbor ? and is it similar to K nearest neighbor algorithm?,"Don't have enough reputation to comment to a resource, so answering this myself.

About Annoy

Annoy is a library being used here for finding approximate nearest neighbours, approximate being the key word here.

Understanding K-NN and Approximate NN

Now, let's see what is the difference with example of a problem.

Say you have 10 entities (words / sentences / objects / anything) for which you have vectors.

1 vector for each entity.

Let's consider that the metric we're interested in finding for finding neighbours is cosine similarity / cosine distance

How many pairs would this form? Roughly (10 * 9) / 2 = 45.

So, you can find cosine similarity between the 45 pairs and then find K-nearest neighbours you want. Pretty simple. This is your K-Nearest Neighbours algorithm.

Also given you are storing these 45 cosine similarities somewhere, you basically have to deal with storing 45 float numbers (cosine similarities can be between -1 and 1).

Now, let's scale up the problem. What if instead of having 10 entities, you have 10,000,000, i.e. 10M entities. Pretty easy to have that number of entities exist in real world.

Now, you'll need to find cosine similarity between (10M * (10M-1) / 2) = 4,99,99,99,50,00,000 pairs. Which is an enormous number! Basically, the complexity grows with O(n^2), where n is number of entities.

Now, think about finding K-nearest neighbours for any given entity. You'll also be running sorting algorithm for 10M similarities for it. Also, let's just think about storage again. Say 1 float number takes 64 bits (or 8bytes). How much storage do you need to store those many pairs?

4,99,99,99,50,00,000 * 8 bytes ~ 364 TeraBytes!

Getting a memory (RAM) of that size is near to impossible.

So, you understand the problem we're having with regular K-NN approach. We don't wish to calculate all pairs, and it's very difficult to store distances for those many pairs and calculate true K-nearest neighbours at runtime for any entity.

This is where Approximate Nearest Neighbours comes into picture. These say, that given your need of the problem is almost always going to be to get only the K Nearest neighbours, why bother calculating similarities for any given entity to those entities that have zero chances to appear in neighbourhood?

Think about a space where you have clusters of points. Say you need to only find the 3 most nearest neighbours for every point for your problem, then you would never need to find similarities between points of different clusters. Right?

This is the idea that Approximate Nearest Neighbour algorithm runs on.

For any given entity, instead of calculating similarities/distances to each of the other point, it chooses X other points and only calculates distances to those.

This reduces the complexity from O(n2) to just O(n*k). Storage becomes easier, Calculation becomes faster.

A hint of Annoy's implementation

While I'm myself not familiar with all implementation details of Annoy's ANN approach. Usually, most ANN approaches require building a Cluster Index first -- which basically decides the X leaders with which you'll find similarities. Then, the cluster index is used to find Approximate K-nearest neighbours for all given points.

The reason why this is called 'approximate' is that the cluster building process is not perfect. If it were, it'll take days or years of computation and defeat the purpose of reducing complexity. It is approximate cluster indices. Which means, sometimes you can miss out on one of the most similar entity appearing in neighbourhood of given entity. Usually, however this doesn't happen so much that it affects practically. Spotify, Facebook, Google and many large companies are using approximate nearest neighbours approaches.

Annoy is one of the most widely used library for doing approximate nearest neighbours."
How to determine if character sequence is English word or noise,"During NLP and text analytics, several varieties of features can be extracted from a document of words to use for predictive modeling. These include the following.

ngrams

Take a random sample of words from words.txt. For each word in sample, extract every possible bi-gram of letters. For example, the word strength consists of these bi-grams: {st, tr, re, en, ng, gt, th}. Group by bi-gram and compute the frequency of each bi-gram in your corpus. Now do the same thing for tri-grams, ... all the way up to n-grams. At this point you have a rough idea of the frequency distribution of how Roman letters combine to create English words.

ngram + word boundaries

To do a proper analysis you should probably create tags to indicate n-grams at the start and end of a word, (dog -> {^d, do, og, g^}) - this would allow you to capture phonological/orthographic constraints that might otherwise be missed (e.g., the sequence ng can never occur at the beginning of a native English word, thus the sequence ^ng is not permissible - one of the reasons why Vietnamese names like Nguy·ªÖn are hard to pronounce for English speakers).

Call this collection of grams the word_set. If you reverse sort by frequency, your most frequent grams will be at the top of the list -- these will reflect the most common sequences across English words. Below I show some (ugly) code using package {ngram} to extract the letter ngrams from words then compute the gram frequencies:

#' Return orthographic n-grams for word
#' @param w character vector of length 1
#' @param n integer type of n-gram
#' @return character vector
#' 
getGrams <- function(w, n = 2) {
  require(ngram)
  (w <- gsub(""(^[A-Za-z])"", ""^\\1"", w))
  (w <- gsub(""([A-Za-z]$)"", ""\\1^"", w))


  # for ngram processing must add spaces between letters
  (ww <- gsub(""([A-Za-z^'])"", ""\\1 \\2"", w))
  w <- gsub(""[ ]$"", """", ww)

  ng <- ngram(w, n = n)
  grams <- get.ngrams(ng)
  out_grams <- sapply(grams, function(gram){return(gsub("" "", """", gram))}) #remove spaces
  return(out_grams)
}

words <- list(""dog"", ""log"", ""bog"", ""frog"")
res <- sapply(words, FUN = getGrams)
grams <- unlist(as.vector(res))
table(grams)

## ^b ^d ^f ^l bo do fr g^ lo og ro 
##  1  1  1  1  1  1  1  4  1  4  1 


Your program will just take an incoming sequence of characters as input, break it into grams as previously discussed and compare to list of top grams. Obviously you will have to reduce your top n picks to fit the program size requirement.

consonants & vowels

Another possible feature or approach would be to look at consonant vowel sequences. Basically convert all words in consonant vowel strings (e.g., pancake -> CVCCVCV) and follow the same strategy previously discussed. This program could probably be much smaller but it would suffer from accuracy because it abstracts phones into high-order units.

nchar

Another useful feature will be string length, as the possibility for legitimate English words decreases as the number of characters increases.

library(dplyr)
library(ggplot2)

file_name <- ""words.txt""
df <- read.csv(file_name, header = FALSE, stringsAsFactors = FALSE)
names(df) <- c(""word"")

df$nchar <- sapply(df$word, nchar)
grouped <- dplyr::group_by(df, nchar)
res <- dplyr::summarize(grouped, count = n())
qplot(res$nchar, res$count, geom=""path"", 
      xlab = ""Number of characters"", 
      ylab = ""Frequency"", 
      main = ""Distribution of English word lengths in words.txt"",
      col=I(""red""))


Error Analysis

The type of errors produced by this type of machine should be nonsense words - words that look like they should be English words but which aren't (e.g., ghjrtg would be correctly rejected (true negative) but barkle would incorrectly classified as an English word (false positive)).

Interestingly, zyzzyvas would be incorrectly rejected (false negative), because zyzzyvas is a real English word (at least according to words.txt), but its gram sequences are extremely rare and thus not likely to contribute much discriminatory power."
Why BERT tokenizers function differently?,"Each BERT variant is trained with text that has been prepared differently, e.g. as the name implies, BERT uncased is trained with text where all letters are lowercase. This means that the vocabulary extraction process has also use lowercase text as input, and therefore gives as result a different vocabulary than the same vocabulary extraction process used with text in its original casing.

Note that, as the vocabularies are different, each model should be used with the tokenizer it was used to train it. Using a model with a different tokenizer may lead to bad results.

The choice of tokenizer, therefore, is tied to the choice of BERT model. The criteria to use one or the other (e.g. uncased vs. cased), depends on the case. For instance, for doing named entity recognition (NER) in English, it may be important to keep the original casing so that the model can more easily distinguish proper nouns."
How to precompute one sequence in a sequence-pair task when using BERT?,"Each token position at each of the attention layers of BERT is computed taking into account all tokens of both sequences. This way, there is not a single element that depends on just the first sequence and therefore it is not possible to precompute anything to be reused for different second sequences.

As you can see, the very nature of BERT's network architecture prevents you from factoring out the computations involving the first sequence.

In other similar architectures like ALBERT, there are some parts that could be reused, as the embedding computation (because ALBERT's embeddings are factorized, making the embedding matrix smaller but adding a multiplication at runtime), but I am not sure that reusing this computation would save a lot.

I don't know of any architecture made for sequence pairs that would let you do what you described, as most sequence pair approaches derive from BERT, which itself relies on computing attention between every token pair.

One option would be to use a network that gives you a fixed-size representation (i.e. a vector) of a sentence: you would use it on each of the sentences in a pair, and then you would feed both vectors to a second network (e.g. a multilayer perceptron receiving the concatenation of both vectors) to compute the final output. Depending on the task, this may give you good results and allow you to do the mentioned precomputing. To obtain the sentence representation vector, you may use BERT itself (the output at the [CLS] position) or some other architecture like LASER."
What are some options for an offline chatbot on Android?,"EasyNLU project might interest you: https://github.com/kolloldas/EasyNLU

EasyNLU is a Natural Language Understanding (NLU) library written in Java for mobile apps. Being grammar based, it is a good fit for domains that are narrow but require tight control."
Are Word2Vec and Doc2Vec both distributional representation or distributed representation?,"Effectively, Word2Vec/Doc2Vec is based on distributional hypothesis where the context for each word is its nearby words. Similarly, LSA takes the entire document as the context. Both techniques solve the word embedding problem - embed words into a continuous vector space while keeping semantically related words close together.

On the other hand, LDA isn't made to solve the same problem. They deal with a different problem called topic modeling, which is finding latent topics in a set of documents."
How can I get probabilities of next word with ELMO?,"ELMO cannot be used to predict the next word.

The reason is that ELMO is like two language models combined: one normal language model that predicts the next word based on the previous ones and another language model in the reverse direction. The hidden representations obtained from both directions are combined to generate the word predictions.

This way, ELMo cannot be used as is for next word prediction because it is designed to receive the whole sentence and is not trained in a causal language model loss.

In order to select a model for next word prediction, you should focus in normal (causal) language models, not bidirectional LMs (e.g. ELMo), masked LMs (e.g. BERT, XLM) or permutation LMs (e.g. XLNet).

Some examples of popular causal language models are OpenAI's GPT and GPT-2, or Transformer-XL. I would recommend taking a look at Hugging Face's PyTorch transformers github repo, which makes it super easy to load pretrained weights for renowned LMs"
How to measure Entity Ambiguity?,"Entity Linking is a type of supervised machine learning, thus many of the common performance metrics could be used. In particular, creating a confusion matrix would identify where one label was predicted but the ground-truth was different. Confusion matrices can be calculated with counts or normalized, a normalized data would be an estimate of ""ambiguity degree"" relative to the other labels in the current dataset.

Other classification measures such has F-score, precision, and recall could also be used. In particular, low precision for a label would suggest the model has trouble disambiguating entities from nearby text. ""Cheap and easy entity evaluation"" goes into more technical details.

Inter-rater reliability could also be used, the raters could be different humans or different models. If the joint-probability of agreement between different raters is low then entities could be regarded as difficult to disambiguate.

The performance also depends on the relative value of an exact match vs partial match."
Can I use euclidean distance for Latent Dirichlet Allocation document similarity?,"Euclidean distance -by which in this application, I assume you mean the euclidean distance in an 
n
ùëõ
-dimensional space defined by the distribution of document contents among 
n
ùëõ
 topics considered, is a valid measure to use in comparing the topics represented within two documents.

What you're doing by applying this method is quantifying a topic frequency difference within this newly defined space, and so interpretation of these quanta will require analysis of the space. For example, what euclidean distance indicates that documents are relatively similar?

In distiction, the normalized result of something like the hellinger distance provides an easily interperable framework by which to evaluate the results- a score of 0 indicates no overlap in the distribution over the topics in question of the two documents, and a 1, perfect overlap.

For the efficiency concerns, it's not clear to me why you couldn't truncate your topics considered to the crucial topics and then calculate any of the metrics on the distributions over ony those topics, rather than the entire universe of considered topics."
Quality check for preprocessing of Text data,"Evaluating any task consists in defining the task formally so that there is a way to define what is a correct output as objectively as possible. For example a good Machine Translation system produces a good translation if it has the same meaning as the input sentence and is grammatically correct in the target language.

Assuming that this task of preprocessing is formally defined, then the evaluation should measure how ""correctly preprocessed"" is the output:

are the stem and lemma always correct?
are the stop words and only the stop words removed?
Etc.

Usually one would build a test set, manually add the correct output and then compare the system output against this gold standard.

However ""preprocessing"" is generally not considered a task by itself, because by definition it's a step for another task. Importantly, the steps of preprocessing depend on the other task, they are not always the same. For example stop words removal makes sense only for tasks based on distributional semantics, i.e. related to the topic. Preprocessing may also include steps which depend on the volume of data."
Metrics for unsupervised doc2vec model,"Evaluation is based on the task, not the type of model used for it. In the tutorial that you link the task would be simple document similarity. Afaik a more common variant is the information retrieval setting, where the goal is to rank documents according to their similarity against a query document.

The tutorial mentions that they use a test set for which a link to a paper is provided. This paper explains that human annotators rated the similarity level between pairs of documents. That would be the standard way to obtain annotated data, but of course it takes a lot of manpower. It doesn't seem that the ground truth information is provided with the test set used in the tutorial though, I assume that this is why the author only proposes a manual inspection of the results.

In general there are probably many benchmark datasets (with ground truth annotations) publicly available for similar tasks, but they are not always easy to find and I'm not very knowledgeable about these. I think a good place to start would be the SemEval series of competitions, every year they release various datasets related to this kind of task."
How to measure the accuracy of an NLP paraphrasing model?,"Evaluation should always be specific to the target task and preferably rely on some unseen test set.

The target task is paraphrasing, so the evaluation should be designed to check externally how good the generated sentences are as paraphrases. Usually this kind of task (the output is similar to Machine Translation) is evaluated by using a gold standard set of paraphrases and measuring how close the generated paraphrase is to the gold standard paraphrase(s). The comparison commonly uses some variant of BLEU score.

Practically the way to know how to evaluate a common task is to search the state of the art: for example this paper uses the method described above with various similarity measures (BLEU, ROUGE and variants apparently)."
How word2vec can be used to identify unseen words and relate them to already trained data,"Every algorithm that deals with text data has a vocabulary. In the case of word2vec, the vocabulary is comprised of all words in the input corpus, or at least those above the minimum-frequency threshold.

Algorithms tend to ignore words that are outside their vocabulary. However there are ways to reframe your problem such that there are essentially no Out-Of-Vocabulary words.

Remember that words are simply ""tokens"" in word2vec. They could be ngrams or they could be letters. One way to define your vocabulary is to say that every word that occurs at least X times is in your vocabulary. Then the most common ""syllables"" (ngrams of letters) are added to your vocabulary. Then you add individual letters to your vocabulary.

In this way you can define any word as either

A word in your vocabulary
A set of syllables in your vocabulary
A combined set of letters and syllables in your vocabulary"
NLP algorithms for categorizing a list of words with specific topics,"Extending the basic idea stated in comments by @krayyem, you are looking for an Ontology and/or Taxonomy. The short story is to (1) build them based on your data or (2) use existing ones.

Building from your data

You need lots of Is-A pairs which indicate what kind of thing the concept is e.g. Maradona is a football-player and football is a sport. For this purpose, you extract info from your text based on some patterns and update info then update pattern based on info and the loop goes on till it does not change anymore. see this answer though.

Solution you are probably looking for

Use existing knowledge bases that some of which are in Python e.g. wordnet. You can find more in the links I provided for the answer I linked above.

How to proceed kind of idea

Find the dominant keyword in the found keywords. You may simply just count and go through the concept graph (your ontology, the knowledge base) and find the parent concept. If the graph is not accessible in this form, you may create it yourself in Python with existing APIs and/or graph libraries in Python like Networkx.

If more input is needed please comment here. Good Luck!"
Advantage of a treebank in XML format,Extensible Markup Language (XML) is a way to encoding documents in a format that is both human-readable and machine-readable. It is also relatively simple and commonly used. It allows data and metadata to be linked. Those are the reasons that is often used for treebank data.
"can I use public pretrained word2vec, and continue train it for domain specific text?","fastText pretrained models should give you a boost to classification task.

gensim on the other hand has possibility to load model and train it with new texts but if you need to account for new words, you need to use

build_vocab(update=True...)

So you can take with fastText pretrained embeddings to gensim and update with your texts."
Convert Word to Semantic Prime,"Finding a robust model for what you are looking or trying to build is quite difficult at this point of time and as per I know there are no any such existing algorithms to do this.


One approach that you can follow is make a Knowledge base for your primes containing info/word it can be related with.
To handle most of the relation you will need a word dictionary which contains all the possible related words. Making such an exhaustive dictionary is not completely possible/feasible. But if you have some domain specific work you can make a dictionary which can handle 80% of you cases. You can use wordnet/word2vec to find the most similar words of your base primes and can extrapolate those words.


Apart from these you would need a set of possible relations that the word can have. For this you can use models like Open IE to extract the Subject-Object-Relation and you can associate these relations with your primes. But for this you will require a dataset of your domain from which you can get the possible relations."
How to generate abbrevations from shortend words in medical records,"First a warning: is your data anonymized? Even if it is, be extremely careful because medical history is super-sensitive personal information. There are legal requirements about how to handle this kind of data and what you can or can't do with it.

As far as I know there's no python API for cTakes or Metamap. Anyway I guess that such an API would boil down to a system call, so it wouldn't be very good.

As an alternative you could implement your own system using the UMLS Metathesaurus, which is a massive list of medical terms grouped by concept (MetaMap extracts UMLS terms/concepts)."
Masked Language Modeling on Domain-specific Data,"First I suggest reading the transformers paper. Couple of quick notes is that this model consists of an encoder and a decoder, and the original task the paper is trained on is machine translation. Datasets (benchmarks) they used to train and evaluate this model from scratch were WMT 2014 Engligh-to-German, WMT 2014 English-to-French (section 5.1 of the paper). The conclusion is that you cannot train transformers from scratch unless you have your sentence pairs in 2 languages.

MLM on the other hand is something that BERT used for training. So if you want to go in this direction, you can use the pre-trained BERT and fine-tune it with Masked language model head on top of your BERT model using your own dataset. You need to stick to the tokenizer that BERT was originally trained on but at least you are taking advantage of some general context that was learned during training BERT from scratch. If you want to train BERT either from scratch or fine-tune with MLM head you can follow this tutorial from hugging face"
what can be done using NLP for a small sentence samples?,"First I think it's worth mentioning that in the context of an exploratory study with a small dataset, manual analysis is certainly as useful as applying NLP methods (if not more) since:

Small size is an advantage for manual study and a disadvantage for automatic methods.
There's no particular goal other than uncovering general patterns or insights, so it's unlikely that the results of an automatic unsupervised method would exhibit anything not directly observable.

That being said one can always apply automatic methods indeed, if only for the sake of observing what they can capture or not.

Observing frequency (point 1) can always be useful. You may consider variants with/without stop words and using document frequency (number of documents containing a term) instead of term frequency.
points 3 and 5 are closely related: LDA essentially clusters the sentences by their similarity using conditional words probabilities as hidden variable. But the small size makes things difficult for any probabilistic method, and there could be many sentences which have little in common with any other.
Syntactic analysis with dependency parsing can perfectly be applied to any sentence, but the question is what for? As far as I know this kind of advanced analysis is not used for exploratory study, it's used for specific applications where one needs to obtain a detailed representation of the full sentence. Traditionally this was used for higher-level tasks involving semantics, often together with semantic role labeling and/or relation extraction. I'm not even sure that this kind of symbolic representation is still in use now that end-to-end neural methods have become state of the art in most applications.
I agree that summarizing a short sentence is pointless. You could try to summarize the whole set of sentences though, if that makes sense.

In the logic of playing with any possible NLP method, you could add a few things to your list:

Lemmatizing the words, this can actually be useful as preprocessing.
Using embeddings or not: on the one hand this can help finding semantic similarities through the embedding space, on the other hand the small size makes it questionable to project the data in a high dimension space.
Finding colocations (words which tend to appear together in the same sentence) with association measures such as Pointwise Mutual Information.
Spelling correction and/or matching similar words with string similarity measures.
It's unlikely that there's any interest in it but there are also stylometry methods, i.e. studying the style of the text instead of the content. These range from general style like detecting the level of formality or readability to trying to predict whether two texts were authored by the same person."
What are x variable and y variable in word2vec model if it is supervised learning,"First I would call it self-supervised in the sense that you dont need human labeling, but still need labels for NN to learn.

Both CBOW and skip-gram are ""combined"" into word2vec but they are shallow-neural-network architectures (albeit different ones).

Since they are different architectures, they do different things:

CBOW learn to predict the word by the context. Or maximize the probability of the target word by looking at the context.

skip-gram model is designed to predict the context so given the word xyz it must understand it and tell us that there is a huge probability that the context abc__dfg is what ""surrounds"" xyz.

Given that, you could try to deduce yourself what should input and output data look like to model this desired input output of CBOW, skip-gram (matrix representation of words and than probabilities and a vector representing a word and multiclass output).

Check also this answer"
Identifying documents similar to specific clusters,"First Idea: Go as usual. Calculate the distance according to of each new document to the centroids of 100 clusters and see to which cluster the belong (minimum distance wins).
Second Idea: You already know which clusters are your targets. Assign a label (e.g. 1) to these 10 and and another label (0) to the rest 90. Then train a classifier and try to predict the label for new documents. (The process of labeling in supervised learning is usually based on expert annotation i.e. an acceptable accuracy in labeling. But with this method you lose that accuracy so expect that not all the labels are perfectly fine as a clustering algorithm is blindly do that)"
What is the use of [SEP] in paper BERT?,"First let's understand why the format is like this.

BERT was pretrained using the format [CLS] sen A [SEP] sen B [SEP]. It is necessary for the Next Sentence Prediction task : determining if sen B is a random sentence with no links with A or not.
The [SEP] in the middle is here to help the model understand which token belong to which sentence.

At finetuning time, if you use a different format than pretraining format, you might confuse the model : he never saw 2 sentences formatted as [CLS] Sen A Sen B [SEP]. The model doesn't know there is 2 sentences, and will consider it as a single sentence.

If you finetune on enough data, BERT can learn the new format. This can be helpful if you need to change the input format.

But in your case, you don't need to do this. Changing the format for the sake of changing the format is just going to confuse your model, he will have to learn more thing, and there will be inconsistencies between pretraining and finetuning.

Will the s1 in second one integrate more information from s2 than the s1 in first one does?

No. Inserting a SEP token or not will not change the amount of information exchange between the tokens of the 2 sentences. In both case the model will compute attention based on the 2 sentences. Each sentence can see the other sentence's tokens, no matter of the SEP.
The only thing you will do by removing the SEP token is confuse your model.

Will the token embeddings change a lot between the 2 methods?

We don't know. It will definitely change, but how much ? We cannot answer. My guess is that token representations will not change a lot (because tokens are the same), but CLS representation will change a lot (instead of representing the links between 2 sentences, it will represent something else)."
How to impute missing text data?,"First most of the time there's no ""missing text"", there's an empty string (0 sentences, 0 words) and this is a valid text value. The distinction is important, because the former usually means that the information was not captured whereas the latter means that the information was intentionally left blank. For example a user not entering a review is not missing information: the user chose not to enter any text and it cannot be assumed that this choice is equivalent to whatever text is the most common.

To the best of my knowledge there's no imputing in NLP. Imputing can make sense in some cases with a numerical value (even then it should be used cautiously), but in general text is too diverse (unstructured data) for the concept of ""most frequent text"" to make any sense. In general substituting real text (or absence of text) with artificially generated data is frowned upon from the point of view of evaluation.

Thus in my opinion the main design options are the following:

Leave the text empty. Most of the time an empty text can be represented like any other text value, e.g. as a TFIDF vector made of zeros.
Discard instances which have no text. For example in text classification no text means no input data at all, so there's no point performing the task for such cases.
Treating instances with no text as special cases based on the specifics of the task. For example such instances could be systematically assigned the majority class, if that makes sense for the task."
Improving accuracy of Text Classification,"First of all good job done in processing the data and coming up with your base model. I would suggest few things that you can try:

Improve your model my adding bigrams and tri-grams as features.
Try doing some topic modelling like latent Dirichlet allocation or Probabilistic latent Semantic Analysis for the corpus using a specified number of topics - say 20. You would get a vector of 20 probabilities corresponding to the 20 topics for each document. You could use that vector as input for your classification or use it as additional features on top of what you already have from your base model enhanced with bigrams and trigrams.
Another thing I would say is try using a tree based classifier ensemble to capture non-linearity and interactions between the features. Either of random forest or gradient boosting would be fine. In gradient boosting you can use xgboost as its a pretty good package that gives good classification.
If you are familiar with Deep Learning you can give a try with a Recurrent Neural network architecture(mostly the LSTM versions).

Hope this helps and let me know if this improves your classification accuracy."
NLP Basic input doubt,"First of all in BOW model, the order is not represented. Decision Tree does not care if ""heavy"" is the first feature or last feature and it works the same for both as BOW just models the ""existence"" of words in documents. So your final doubt is actually nothing to worry about. In both sentences you have the word ""heavy"" and in that column you get a 
1
1
 for both sentences (or TF, or TF-IDF or any other count you use).

LSTM sees the order as it has a memorising behaviour. That means word ""heavy"" has an index and during long training, your model learns the probability of usage of word heavy. That means it models your text. so your understanding is right, there is a representation of words for last too."
How to use different classes of words in CountVectorizer(),"First of all your question is about stemming words as mentioned in the other answer which can be found in any Python NLP library such as Spacy or NLTK.

The other point to mention here is that despite the other answer, what libraries has as Stop Words list is not actually stop word! Do no remove them! In NLP stop words should be extracted based on working corpus not based on a predefined list. In practice removing this kind of stop words usually reduces the performance on specific domain corpuses.

The Third point is that depending on the classifier and loss function you use, TF-IDF might be better than Count Vectorizer. I suppose it works better specially if Log Loss is the cost function but I am not sure. Just give it a try."
Similarity of words using BERTMODEL,"First of all, I think you are confused with pretrained and finetuned.

BERT is pretrained on a lot of text data. By using this pretrained BERT, you have a model that already have knowledge about text.

BERT can then be finetuned on specific dataset, where BERT learn specific knowledge related to the dataset. That's why a finetuned BERT is bad on other datasets : the knowledge does not apply.

You have a custom dataset. So you don't want to use a finetuned model, but you want to still use a pretrained model (to have access to the general knowledge of BERT).

If you use a BERT model from scratch (not pretrained), you're basically creating an empty model, randomly initialized : it's useless.

Now that we have clarified this, let's see your problem.

What you are doing is calling the constructor of BertModel with the arguments of the forward pass. You first need to create the model, and then use the forward pass :

from transformers import BertModel, BertConfig
config = BertConfig(...)    # Here your parameters to initialize your BERT as you want, such as number of heads, etc...
model = BertModel(config)
hidden_reps, cls_head = model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)


BUT, as I explained in the first part, you should not do this : it's completely useless and your results are going to be random. You want to use a pretrained BERT, to have some meaningful results :

from transformers import BertModel
model = BertModel.from_pretrained('bert-base-uncased')
hidden_reps, cls_head = model(token_ids, attention_mask=attn_mask, token_type_ids=seg_ids)


Note : Your results can be improved further if you finetune your BERT model on your custom dataset.

Final point : Don't reinvent the wheel

People already tried to use BERT for word similarity. Instead of implementing this from scratch, using only a pretrained model, potentially adding bug to your own implementation, just use some already existing code !

I'm not sure about NER, but for word similarity, you can take a look at BERT Score. Their model is already finetuned for word similarity, so it's likely that you will get higher score using this approach, and also much easier path to take."
Shall I use the Euclidean Distance or the Cosine Similarity to compute the semantic similarity of two words?,"First of all, if GloVe gives you normalized unit vectors, then the two calculations are equivalent. In general, I would use the cosine similarity since it removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure ""term frequency"" space using the Euclidean distance. They will be right on top of each other in cosine similarity."
How to find impactful words affecting classification?,"First of all, let's just clarify that proving causation is quite difficult. And therefore, you will mostly need to show correlation. Also, for text classification, the impact of each word is not linear. Each word impacts the meaning of the sentence relative to the other words and vice versa, therefore, you need to keep that in mind.

With that said, there are a few ways you can look at the impact of words on a classification:

Frequencies

If you have multiple classes, you could look at how frequently certain words appear in each class. If the word ""good"" only appears in positive sentences, it is safe to assume that it has a high impact.

This method has the benefit of being model-independent.

Feature importance

If you are using a bag-of-word representation as your input, you could look at how much the presence/absence of each feature affects the results. For instance, let's say that you can classify positive sentences with 80% accuracy with all words as features. Now, try to classify the sentences again by removing certain words. If you remove the word ""good"" from sentences, you can look at how the performance varies.

Or, you could do the opposite and try to simply classify each word. If you classify the word ""good"" as positive, it means that it is probably a word that impacts sentences into being positive. If your model provides confidence, you could use that to know how much information the word brings to the sentence.

Attention mechanism

The attention mechanism will be able to tell you which part of the input influences the results the most. This works better if you use a sequential model"
Pre-trained models for finding similar word n-grams,"First off, there aren't, to my knowledge, models trained specifically to generate ngram embeddings. Although, it would be very easy to modify the word2vec algorithm to accommodate ngrams.

Now, what can you do?

You could compute the ngram embedding by summing up the individual word embeddings. Potentially, you could apply weights based on tfidf for instance, but not required. Once you have 1 embedding, simply find a nearest neighbor using cosine distance.

Another approach, though more computionally expensive would be to compute the Earth Mover's Distance (also called Wasserstein) between ngrams and find nearest neighbors this way."
does ValueError: 'rat' is not in list means not exist in tokenizer,"First, a tokenizer doesn't have a dictionary of predefined words, so anyway it doesn't make sense to ""add a new token"" to a tokenizer.

Instead it uses indications in the text in order to separate the tokens. The most common indication is of course a whitespace character "" "", but there are lots of cases where it's more complex than that. This is why there would be many cases where the second method with sent.split("" "").index(word) would not return the same tokens (punctuation marks, for example).

Also the tokenizer doesn't change the text, so if the sentence contains the word rattan it cannot transform it into the word rat. Why are you testing this? Btw rattan is a real word, in case this is the issue."
How to properly compare these two confusion matrix?,"First, about interpreting these confusion matrices: the sum of every row is 1, which implies that every value is a conditional probability p( predicted label | true label ), i.e. the probability of a given true label to be a particular predicted label. Example: the top left cell in both matrices is 0.01, which means that when the true label is 5 the probability that the system predicts label 1 is 1%.

The two confusion matrices show the results of the predictions by two different systems. This means that in general there is not one which is correct and the other which makes errors, just two different ways to predict labels which might lead to different types of errors.

The diagonal shows the True Positive cases, i.e. cases where the predicted label is the same as the true label (this is important since these are the ""correct"" cases). The probabilities show very little difference between the two systems on the diagonal, the top one being slightly better for labels 3 and 4 and the bottom one slightly better for label 5.
In general it's important not to look only at the diagonal especially for ordinal values, because if the predicted value is far from the true value it's a more serious error than if it's close to it. Example: if the true value is 4 it's better for a system to predict 3 than 1 (both are errors but the latter is worse). However here again there is very little difference between the two systems, they appear to have a very similar behaviour.

In order to quantitatively summarize and compare the performance of the two systems, confusion matrices are too complex. Typically one would use an appropriate evaluation measure, for instance micro or macro f-score (classification evaluation) or Mean Absolute Error (regression evaluation)."
Bug in sentiment analysis and classification for unlabeled text [closed],"First, congratulations for thinking to do a qualitative analysis of the results :) I know it should be obvious, but so many people just assume that the system works and don't bother checking their output.

Now, strictly speaking what you're seeing is not a bug. These are errors made by a statistical system. A statistical system is not meant to get everything right, it's only meant to label the input ""to the best of its knowledge"", and its knowledge is limited primarily by (1) the data it was trained with and (2) the assumptions made in the design of the model itself.

I don't know the exact characteristics of the systems that you used, but I can make an educated guess about the errors that you mention:

""fewer people are dying every day"" is likely to be predicted as negative because it contains the word ""dying"". Probably there were no (or very few) examples in the training data which contain the word ""die"" and are labelled positive. As a consequence the system assumes that any sentence containing ""die"" is likely negative. One may notice that the positive semantics of ""fewer people dying"" is completely lost on the system, because it focuses on simple clues (individual words), it's not able to parse more complex phrases.
""The audience here in the hall has promised to remain silent."" would be a similar case: the word ""silent"" or perhaps the two words ""remain silent"" likely were found only in negative examples during training, so the system just generalizes wrongly that a sentence containing these words is negative.
The sarcastic ""Oh really?!"" is an even more complex concept for the system to properly identify. The task of sarcasm detection is studied on its own because it's such a difficult task for a machine. I don't follow this field closely so I could be wrong, but I don't think the task has reached any satisfying level of maturity yet, let alone been integrated with standard sentiment analysis systems.

Nonetheless these errors don't mean that the results are useless. If you annotate manually a random sample and evaluate the performance of the system on this sample, hopefully you'll see that overall the system performs decently. That's what is expected of a statistical system: it's not reliable on an individual basis, but normally it's doing a good job in average.

More generally, all these errors show that the problem of Natural Language Understanding is far from being solved yet... and it might never be. The good news is that there's still a lot of interesting problems to solve for NLP scientists ;)"
What are the advantages/disadvantages of using tfidf on n-grams generated through countvectorizer?,"First, CountVectorizer produces a matrix of token counts, not TF-IDF weights. In order to obtain TF-IDF weights you would have to use TfidfVectorizer.

If the goal is to study term frequency, there is no point using TF-IDF since TF-IDF weights are different from frequency. TF-IDF is used to reduce the weight of tokens which appear frequently compared to tokens which appear rarely. Moreover, TF-IDF weights are at the level of a document, so they cannot be used as a measure of global comparison across all documents. Across documents you could use the IDF (Inverse Document Frequency) part only, but then why not simply use Document Frequency.

Note that the same applies to a token count matrix: the values are at the level of a document. In order to find the global frequency one has to sum across the documents for every token.

Finally if you are trying to find the most frequent terms/n-grams of any length, it's difficult to compare frequencies between n-grams of different length. Additionally you're going to find real ""terms"" mixed with frequent grammatical constructs, for example ""it is"" is not a term but it's a frequent n-gram."
Methods for learning with noisy labels,"First, I wouldn't use the word ""noisy"" here because if you know which instances are ""wrong"" then these are not noise, they are negative examples. In my opinion ""noisy"" is when positive and negative cases are mixed together in a way that makes it difficult (or impossible) to distinguish between them. I think this matters because you're more likely to find similar use cases and relevant methods using this terminology.

I don't have a precise method to suggest but I would check the state of the art in machine translation: it's also a sequence-to-sequence task in which there are potential positive/negative cases. In particular there has been some work done in MT quality estimation, where the goal is to predict the quality of a translation for a sentence. This might be related because it's about labeling or quantifying how good a translation is, and I would assume that there are works which re-use labelled/scored translations (including potentially wrong ones) in order to obtain a better model. Unfortunately I don't have any pointers since I haven't followed the field recently."
Do Sampling before or after TFIDF step?,"First, let me note that resampling rarely works well with text data, because the diversity of text cannot be simulated by some extremely simplistic process like SMOTE. Of course you can try resampling, but don't forget to compare to the basic case without resampling.

Now, the difference between your two options would be the IDF weights in TFIDF, because the IDF for a token depends on the number of instances which contain this token.

Option 1 with TFIDF before SMOTE: using the true IDF weights corresponding to the original text. Disadvantage: not consistent with the modified frequencies caused by SMOTE.
Option 2 with SMOTE before TFIDF: using IDF weights after changing frequencies, so consistent with these. Disadvantage: IDF weights are artificial and more likely to introduce a bias. For example a rare word like ""overfitting"" might be duplicated many time, hence its frequency is higher and its IDF is lower.

I think that option 1 is preferable, the representation of the text is more faithful to the real data."
Effectiveness of tf-idf on documents with repeated keywords,"First, please note that TFIDF is a very simplistic weighting method.

The principle of the IDF part is indeed to lower the score of words which appear in many documents and give rare words more weight. The rationale is that rare words are more discriminative.

In classification a ""keyword"" which appears in all the documents (or even most of them) cannot be relevant because knowing that the keyword appears doesn't contribute to finding the class of the document. The corpus on which the IDF is calculated is supposed to be a representative sample for whatever task one intends to do. If 'healthcare' belongs to every document in this corpus, then it is assumed that healthcare always belong to any document and therefore doesn't bring any semantic information about the document, similarly to a stop word. It would be pointless to use this word as a feature."
Quantitatively evaluate similarity between two corpus of texts,"First, there are different of text similarity: lexical (using mostly same words), semantic (talking mostly about the same topics), stylistic (written mostly in the same style, possibly by the same author), etc.

I'm assuming the most standard case, i.e. that you're looking for some kind of semantic similarity. There are different methods:

Probably the most simple would be to represent the two texts as TFIDF vectors and use cosine TFIDF to compare them. Mind that preprocessing options can have a huge impact, for example filtering out low frequency, lemmatizing, etc.
More advanced methods would represent the two texts as embeddings and compare these vectors. This requires pretrained word embeddings and would require more computation.

Anyway there is no binary answer, text similarity is typically considered continuous. This makes more sense, because there is often no clear answer: different people may not agree whether two texts are similar or not."
Using nlp to analyze accident report,"Firstly, I hope that the label is either a short summary or words of varying length, not just one word direction. Because moving cars involved in an accident may have multiple directions, or one car could be just parked like the example.

Secondly, given that you are planning to predict varying length label, and given the example text, I am pretty sure that bag of words will not work well. You need the context. And you dont have million sized training set to train a transformer deep neural net. So try to utilize pre-trained embeddings like USE, GloVe etc. If you are using the embedding, it provides you awesome feature engineering done. Just train some low complexity model like random forest/ xgboost on (label, embeddings). You can also explore pretrained summary generators like bertseq2seq, however the code for this didn't work for me yet."
NLP approaches to infer Processes from Text,"Following with the idea of building a classifier, one option is to use nltk library together with Keras-Tensorflow once you have a labeled dataset with the desired process categories. You can go on two main approaches:

bag-of-words
sequence-modeling

As a quick resume of the steps to implement in a text classifier with the first approach, you could follow the ones below (you can find a worked pout example here):

Read and check that your raw input sentences (to be used for training, validating...) have the right format and correct label, something like: 
Preprocess your sentences as needed, which could be these steps:
- lowercase all your words
- remove punctuation characters
- tokenize your words (here, your can define if you want 1-gram tokens, 2-grams tokens...)
- stem your words (so as to eliminate singular/plurals, verbs tenses... (this point is not always straigtforward, because some stemmers like PorterStemmer, SnowballStemmer might offer different performance depending on the selected language), more info here
- add more steps custom for your use case, the ones above are standard ones, but you can filter sentences wich you know do not offer value for you use case

Once you have preprocessed your input data, you should be able top access your vocabulary, to have something like:

and you are ready to vectorize your sentences, to end up with something like: 

build your classifier, where you can try out different models, like a convolutional neural network, a bi-directional LSTM, a transformer model..."
Guidelines to debug REINFORCE-type algorithms?,"For a deep dive into all sorts of possible issues that can come up in practical application of policy gradient, have a look at this paper:

A Closer Look at Deep Policy Gradients

Some common ways to improve policy gradient methods is:

Confine the updates to a ""trust region"" near the current policy (see TRPO and PPO).
Introduce a value network (called Critic in Actor-Critic) which can be used to subtract a baseline that reduces variance in policy-gradient learning."
Changing default values of ANNIE resources in GATE from Java code,"For any of these situations the best approach is to construct the application as you need it using GATE Developer, ""save application state"" to create your own custom gapp file, then pass your new gapp to the PersistenceManager instead of loading the standard ANNIE_with_defaults."
Picking the right NLP model to tag words from a dataset,"For generating lists of words related to the same topic, my first thought would be to take a large corpus of (monolingual) text, apply topic modelling and then collect random top words by topic."
What are useful evaluation metrics used in machine learning,"For model evaluation there are different metrics based on your model:

Confusion matrix

Classification accuracy:
(TP + TN) / (TP + TN + FP + FN)
Error rate:
(FP + FN) / (TP + TN + FP + FN)


Paired criteria

Precision: (or Positive predictive value)
proportion of predicted positives which are actual positive
TP / (TP + FP)
Recall: proportion of actual positives which are predicted positive
TP / (TP + FN)


Sensitivity: proportion of actual positives which are predicted positive
TP / (TP + FN)
Specificity: proportion of actual negative which are predicted negative
TN / (TN + FP)
True positive rate: proportion of actual positives which are predicted positive
TP / (TP + FN)
True negative rate: proportion of actual negative which are predicted negative
TN / (TN + FP)
Positive likelihood: likelihood that a predicted positive is an actual positive
sensitivity / (1 - specificity)
Negative likelihood: likelihood that a predicted negative is an actual negative
(1 - sensitivity) / specificity


Combined criteria

BCR: Balanced Classification Rate
¬Ω (TP / (TP + FN) + TN / (TN + FP))
BER: Balanced Error Rate, or HTER
Half Total Error Rate: 1 - BCR
F-measure harmonic mean between precision and recall
2 (precision . recall) / (precision + recall)
FŒ≤-measure weighted harmonic mean between precision and recall
(1+Œ≤)2 TP / ((1+Œ≤)2 TP + Œ≤2 FN + FP)


The harmonic mean between specificity and sensitivity is also often used and sometimes referred to as F-measure.

Youden's index: arithmetic mean

between sensitivity and specificity
sensitivity - (1 - specificity)
Matthews correlation: correlation between the actual and predicted
(TP . TN ‚Äì FP . FN) / ((TP+FP) (TP+FN) (TP + FP) (TN+FN)) ^ (1/2)
comprised between -1 and 1 Discriminant power normalized likelihood index sqrt(3) / œÄ . (log (sensitivity / (1 ‚Äì specificity)) + log (specificity / (1 - sensitivity))) <1 = poor, >3 = good, fair otherwise

You can find much more here. Also there are some explanations here and you can find useful code snippet from here which are implemented."
Is there any way to define custom entities in Spacy,"For pretrained models, spaCy has a few in different languages. You can find them in their official documentation https://spacy.io/models

The available models are:

English
German
French
Spanish
Portuguese
Italian
Dutch
Greek
Multi-language

If you want support for extra labels in NER, you could train a model in your own dataset. Again, this is possible in spaCy and from their official documentation https://spacy.io/usage/training#ner, here is an example

LABEL = ""ANIMAL""

TRAIN_DATA = [
    (
        ""Horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""Do they bite?"", {""entities"": []}),
    (
        ""horses are too tall and they pretend to care about your feelings"",
        {""entities"": [(0, 6, LABEL)]},
    ),
    (""horses pretend to care about your feelings"", {""entities"": [(0, 6, LABEL)]}),
    (
        ""they pretend to care about your feelings, those horses"",
        {""entities"": [(48, 54, LABEL)]},
    ),
    (""horses?"", {""entities"": [(0, 6, LABEL)]}),
]


nlp = spacy.blank(""en"")  # create blank Language class
ner = nlp.create_pipe(""ner"")
nlp.add_pipe(ner)

ner.add_label(LABEL)  # add new entity label to entity recognizer

optimizer = nlp.begin_training()

move_names = list(ner.move_names)
# get names of other pipes to disable them during training
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]

with nlp.disable_pipes(*other_pipes):  # only train NER
    sizes = compounding(1.0, 4.0, 1.001)
    # batch up the examples using spaCy's minibatch
    for itn in range(n_iter):
        random.shuffle(TRAIN_DATA)
        batches = minibatch(TRAIN_DATA, size=sizes)
        losses = {}
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
        print(""Losses"", losses)


If you want to use an existing model and also add a new custom Label, you can read the linked article in their documentation where they describe the process in details. Actually, it is quite similar to the code above."
NLP to calculate similarity ratio between sentences of max 5-6 words,"For spelling mistakes, Levenshtein distance is a good way to start. This metric essentially calculates the distance between two words in terms of substitutions, deletions, and insertions between them.

For instance, ""bear"" can be written as ""fear"" with one substitution (changing the ""b"" to an ""f""). In your case, FRANC is just the word FRANCE with a deletion (of the letter ""E""). You can set an appropriate threshold based on how long the sentence is. For instance, for shorter sentences you may want a smaller threshold (a Levenshtein distance of 1), whereas for your 5 word sentences, you may be willing to accommodate a threshold of 3 or 4.

This library allows you to calculate Levenshtein distance fairly easily, as a percentage of the total length of the word. For instance, in the ""bear""/""fear"" example I mentioned above, the words are 75% similar (3 characters stay the same out of 4 total characters). You can experiment with the percentage threshold that works best for you, though again I would recommend adjusting the threshold based on the length of the sentence."
What are machine learning/deep learning models for generating contextually related words and synonyms?,"For synonyms I would directly use WordNet.

[added] For contextually similar words the traditional approach is to extract a context vector for every target word:

for every occurrence of a target word extract the words within a -/+ N window (e.g. N=5).
for every target word aggregate all its context words in a single context vector over the whole vocabulary.

Finally once a context vector has been calculated for every target word a similarity measure can be used, for example cosine. That means for every target word, compare its vector against any other candidate.

The same approach can be used with word embeddings instead of context vectors."
"Clustering mixed data types - numeric, categorical, arrays, and text","For the tags: Do you know how they are generated? How many unique tags do you have? If they are self-generated (ie lots of tags that can be subsets of other tags). You might need to do tag consolidation which will also help with dimensionality reduction of the word vectors. If you can provide a bit more information about what your data looks like and where it comes from, I can perhaps provide a more in depth answer.

For the text: You might want to try using word embeddings. You can use a pre-trained word2vec model.

I am not sure if it makes sense to use two different distance metrics. Your categorical data looks like its integers, is it ordinal, or are those indexes?"
terminology and advice for NLP on multiclass classification with ordered levels,"For the target variable I would probably choose the numerical representation from 0 to 3 and consider this problem a regression task:

Main advantage: preserve the order over the levels. I think it's really important to represent the fact that the error between levels 0 and 1 is less important than an error between levels 0 and 3.
As a result, the predicted value would be a real number between 0 and 3 (maybe even outside this range sometimes). Typically one can re-normalize this value (replace with closest integer).
Note that it's still reasonable enough to consider this as multiclass classification problem.

But imho the most difficult part will be the design of the NLP process, because to my knowledge this is not a standard task. Suggestions:

Approach 1
A custom Named Entity Recognition would be trained only to detect the different target diseases. Note that if these are quite standard there can pretrained models.
Then a different regression model would be trained for each specific disease. Maybe the input could be a short window around the target term, in order to make sure the model using the info related to this disease in case there are several.
Approach 2
Two custom NER models, one for detecting the target diseases and another for detecting the clues related the level (e.g. 'no', 'a lot').
Then probably a simple ad-hoc program to match the right clue to the right disease, typically based on the distance between the two."
How to create clusters based on sentence similarity?,"For this problem, I would start with a simple bag of words model and use that as a baseline.

This is an example : https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183 , http://brandonrose.org/clustering

For logs, this might be sufficient since it will easily filter out common words like date etc.

If this does not work well for a given usecase, next step is to try Doc2Vec and similar methods that project sentences to a vector space."
How to implement hierarchical labeling classification?,"For this problem, you need to develop three models:

Model 1- for two main categories
Model 2- for sub-category A
Model 3- For sub-category B


So when you want to predict the result for an unseen data, first you use the Model 1, to find the main category. Based on the prediction and by using an if-else statement, you decide to perform another prediction using Model 2 or Model 3.

Consequently, the Model 1 is a binary classification task, but Model 2&3 are mutli-class classification task. Your networks may look like this:

Model 1:

model1 = Sequential()
model1.add(Dense(60, input_dim=X.shape[1], activation='relu'))
...
model1.add(Dense(1, activation='sigmoid'))
model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


Model 2 & 3:

model2 = Sequential()
model2.add(Dense(8, input_dim=X.shape[1], activation='relu'))
...
model2.add(Dense(n_subcats, activation='softmax'))
model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])


Just don't forget to use correct y (label) for main and sub-categories for each model."
How to compute conditional probability in code?,"For this specific paper, the conditional distribution is calculated as follows:

P(
f
i
=v|
f
1:i‚àí1
)=
exp(
o
i,v
)
‚àë
V
v=1
exp(
o
i,v
)
ùëÉ
(
ùëì
ùëñ
=
ùë£
|
ùëì
1
:
ùëñ
‚àí
1
)
=
exp
‚Å°
(
ùëú
ùëñ
,
ùë£
)
‚àë
ùë£
=
1
ùëâ
exp
‚Å°
(
ùëú
ùëñ
,
ùë£
)

on page 1702"
What is the best question generation state of art with nlp?,"For your first part of the question as to which question generation approaches are good - Neural question generation is being pretty popular (as of 2018/2019) among NLP enthusiasts but not all systems are great enough to be used directly in production. However, here are a few recent ones which reported the state-of-art performances in 2019 and have shared their codes too:

https://github.com/ZhangShiyue/QGforQA
https://github.com/PrekshaNema25/RefNet-QG

This one is a 2020 one (now that NLP performances have been improved with Transformers) 3. https://github.com/patil-suraj/question_generation

Besides, if you want more control as to understand and fix for wrongly generated questions, I would suggest the more traditional rule-based approach like the below which is more reliable than the above neural ones ones and generates a larger amount of question-answer pairs than the above 2:

http://www.cs.cmu.edu/~ark/mheilman/questions/
https://bitbucket.org/kaustubhdhole/syn-qg/src/master/

To answer your second question, if your QG model is generating an answer, then it makes sense to use cosine similarity. Assuming your question generation is at the sentence level, you will mostly have short answer spans and hence averaging Glove or Paragram word vectors might serve you better results than the Universal Sentence Encoder."
dealing with HuggingFace's model's tokens,"For your first question, you can check if the tokenizer covers a certain string with the following:

text = 'today is a good day üòÉ'
ids2string = lambda ids: tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(ids))
ids2string(tokenizer(text)['input_ids'])
> <s>today is a good day üòÉ</s>


If emoji is not included in the tokenizer creation, the tokenizer will replace it with the unknown special token. You can access that with tokenizer.special_tokens_map['unk_token']. You can drop or keep them, shouldn't make much of a difference.

Alternatively, if you're going to fine-tune, you can add your own tokens to the existing tokenizer with tokenizer.add_special_tokens. However, in this case the embeddings for that token will be random. And you need to train them."
TF-IDF for Topic Modeling,"Formally the problem of topic modelling is a clustering problem: given a collection of text documents, group together the documents which are topically similar.

So technically it can indeed be done with a TF-IDF representation of documents as follows:

Collect the global vocabulary across all the documents and calculate the IDF for every word.
Represent every document as a TF-IDF vector the usual way: for every word, obtain the term frequency in the document (TF) then multiply by the global IDF for this word (IDF). Note that every vector must represent the document over the global vocabulary.
Use any clustering method over the vector representations of the documents: K-means, hierarchical clustering, etc.

Note that this method is unlikely to be as good as state of the art methods for topic modelling."
"How is the connection between Text Mining, NLP and Tasks like Tokenization, Lemmatization, Stop-word Removal etc.?","From a research perspective, the domain is called Natural Language Processing (NLP). This is the term that people use to describe their specialty, to name their teams, big conferences, etc. For the sake of completeness I have to mention that the term Computational Linguistics is also used quite a lot (sorry for adding another term to your confusion!).

To my knowledge the term ""text mining"" is never used as a scientific field, and actually not used that much. Wikipedia defines text mining as the general process of deriving information from text, so from this point of view it's a general term which includes most of NLP. So technically your option 1 is probably the most correct, however I don't think anybody would ever say that ""NLP is an application field of text mining"", it doesn't sound right because text mining is not the name of the domain.

In the usage ""text mining"" usually refers to the exploratory (often unsupervised) side of applications, in a way which is somewhat similar to what data mining is to machine learning. But honestly I don't think it's worth the effort of trying to define formally the relationship or exact limits of these concepts which overlap a lot and evolve quite fast anyway. In other words: don't overthink this ;)

For the record NLP is in at the intersection of many fields or subfields: it overlaps with speech processing, information retrieval, knowledge representation, data mining, etc.

Anyway, welcome to the field :)"
OpenNLP Coreference Resolution (German),"From an older version of the OpenNLP README:

Training the Tools

There are training tools for all components expect the coref component. Please consult the help message of the tool and the javadoc to figure out how to train the tools.

The tutorials in our wiki might also be helpful.

The following modules currently support training via the WordFreak opennlp.plugin v1.4 (http://wordfreak.sourceforge.net/plugins.html).

coreference: org.annotation.opennlp.OpenNlpCoreferenceAnnotator (use opennlp 1.4.3 for training, models are compatible)

Note: In order to train a model you need all the training data. There is not currently a mechanism to update the models distributed with the project with additional data.

As you can see, OpenNLP does not provide training tools for the coreference component. However, it seems at one point it was possible to train new models for OpenNLP's coref component using the third-party WordFreak plugin... however, it hasn't been updated in over a decade, so your mileage may vary."
Is there any library available for balancing imbalanced text dataset?,"from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler
from imblearn.under_sampling import NearMiss, RandomUnderSampler

ros = RandomOverSampler(random_state=777)
X_ROS, y_ROS = ros.fit_sample(testing_tfidf, testing_target)

smt = SMOTE(random_state=777, k_neighbors=1)
rus = RandomUnderSampler(random_state=777)


Good article for reference"
What are some standard ways of computing the distance between individual search queries?,"From my experience only some classes of queries can be classified on lexical features (due to ambiguity of natural language). Instead you can try to use boolean search results (sites or segments of sites, not documents, without ranking) as features for classification (instead on words). This approach works well in classes where there is a big lexical ambiguity in a query but exists a lot of good sites relevant to the query (e.g. movies, music, commercial queries and so on).

Also, for offline classification you can do LSI on query-site matrix. See ""Introduction to Information Retrieval"" book for details."
Theoretical and practical comparison of CTC and seq2seq loss in Tensorflow,"From my own work, I give a few, sparse empirical reports:

bLSTMs are suitable for seq2seq loss in Tensorflow in case they are used within the encoder of an encoding-decoding scheme. I have not tried to use bLSTMs within the decoder.
--
seq2seq loss could be trained almost equally well with both RMSProp and ADAM on both G2P and P2G tasks.
--"
Entity Recognition in Stanford NLP using Python,"From the following links, I understood that we can use a specific classifier by doing.

Load the specific classifier:

java -mx600m -cp ""*;lib\*"" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz


Set ner model to the classifier in the code:

{ ""ner.model"", ""english.all.3class.distsim.crf.ser.gz"" }


Stanford, Stack Overflow"
Specifics about ChatGPT's Architecture,"From the OpenAI website, we know that ChatGPT is a fine-tuned version of GPT-3.5 (text-davinci-002).

On the GPT-3.5 presentation page, they mention that the number of parameters is 175B (in the footnote of the table there, we can read that there may be slight differences with the actual model used in the API, though), which matches the size of GPT-3. Therefore I understand that ChatGPT is the same size as GPT-3.

The details of the architecture of GPT-3 have been made public in the paper:

Also, in the paper they mention that:

All models use a context window of nctx = 2048 tokens

So:

Number of layers: 96
Number of attention heads: 96
Dimensions of its hidden layers: 12288
Sequence length: 2048
Number of parameters: 175B

Note that, as ChatGPT comes from GPT-3 and GPT-3 is an evolution of GPT-2, we also know from their papers that the architecture differs somewhat from a standard Transformer decoder. See this answer for details on the specific architectural differences."
"Trying to implement a ""smart compose"" feature","From the same blog,

In this hybrid approach, we encode the subject and previous email by averaging the word embeddings in each field. We then join those averaged embeddings, and feed them to the target sequence RNN-LM at every decoding step.

The BoW part of their hybrid approach is to get the general context of the email conversation by averaging the word embeddings in the subject and the previous email. Passing this vectorized representation of the general context at each step of the RNN-LM, which takes into account word order, helps get predictions that are more tailored to the subject of the conversation.

Note that at each step, the RNN-LM is getting 3 types of input -

Embedding for the previous word - For immediate context.
State vector - For localized context
Subject and previous email averaged embeddings - For a wider, conversation-level context

Hopefully, it is clearer now, that the model incorporates order with the use of RNN-LM and the BoW part is to condition the output to the subject of the conversation."
Are stopwords helpful when using tf-idf features for document classification?,"From the way the TfIdf score is set up, there shouldn't be any significant difference in removing the stopwords. The whole point of the Idf is exactly to remove words with no semantic value from the corpus. If you do add the stopwords, the Idf should get rid of it.

However, working without the stopwords in your documents will make the number of features smaller, which might have a slight computational advantage."
Significance of log in DCG metric?,"From the wikipedia page:

Previously there has not been any theoretically sound justification for using a logarithmic reduction factor2 other than the fact that it produces a smooth reduction. But Wang et al. (2013)[3] give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.

The work showed that the log function is able to converge to consistent results for different ranking functions. It distinguishes the methods nicely even at high numbers of documents (at thelimit).

For example, here is a graph of the ranking measures using NDCG@k, where only the top k entries in the ranked list are considered:"
Having trouble figuring out how loss was calculated for SQuAD task in BERT paper,"From your description it sounds like for every position 
i
ùëñ
 in the input text the model predicts
p
S
(i)=P(correct start position is¬†i)
ùëù
ùëÜ
(
ùëñ
)
=
ùëÉ
(
correct start position is¬†
ùëñ
)
and
p
E
(i)=P(correct end position is¬†i).
ùëù
ùê∏
(
ùëñ
)
=
ùëÉ
(
correct end position is¬†
ùëñ
)
.
Now let 
s
^
=arg
max
i
p
S
(i)
ùë†
^
=
arg
‚Å°
max
ùëñ
ùëù
ùëÜ
(
ùëñ
)
 and 
e
^
=arg
max
i
p
E
(i)
ùëí
^
=
arg
‚Å°
max
ùëñ
ùëù
ùê∏
(
ùëñ
)
 be the most probable start and end positions (according to the model).

Then by ""maximum scoring span is used as the prediction"" they just mean that they output 
(
e
^
,
s
^
)
(
ùëí
^
,
ùë†
^
)
 when predicting.

Then ‚ÄúThe training objective is the loglikelihood of the correct start and end positions‚Äù means that if the correct start and end positions are 
s
‚àó
ùë†
‚àó
 and 
e
‚àó
ùëí
‚àó
, they try to maximize the predicted probability of 
s
‚àó
ùë†
‚àó
 and 
e
‚àó
ùëí
‚àó
. If the start and end positions are independent then this is equal to 
p
S
(
s
‚àó
)
p
E
(
e
‚àó
)
ùëù
ùëÜ
(
ùë†
‚àó
)
ùëù
ùê∏
(
ùëí
‚àó
)
 and taking the negative log the loss becomes
L(
e
‚àó
,
s
‚àó
)=‚àílog
p
S
(
s
‚àó
)‚àílog
p
E
(
e
‚àó
).
ùêø
(
ùëí
‚àó
,
ùë†
‚àó
)
=
‚àí
log
‚Å°
ùëù
ùëÜ
(
ùë†
‚àó
)
‚àí
log
‚Å°
ùëù
ùê∏
(
ùëí
‚àó
)
."
Extracting specific data from unstructured text - NER,"From your question, I too feel it's a NER problem. And about the dataset, unless there is a data set which tags the reservation numbers and is similar to your application, you WILL have to create your own data set.

I worked on a similar problem before and my dataset looked something like this:

<TEAM>Northern</TEAM>   NNP
<TEAM>Ireland</TEAM>    NNP
man NN
<PLAYER>James</PLAYER>  NNP
<PLAYER>McIlroy</PLAYER>    NNP
is  VBZ
confident   JJ
he  PRP
can MD
win VB
his PRP$
first   JJ
major   JJ
title   NN
at  IN
this    DT
weekend NN
's  POS
<COMPETITION>Spar</COMPETITION> JJ
<COMPETITION>European</COMPETITION> JJ
<COMPETITION>Indoor</COMPETITION>   NNP
<COMPETITION>Championships</COMPETITION>    NNP
in  IN
<LOCATION>Madrid</LOCATION> NNP


You can see that that I have the entity tag and the part of speech tag in the word. When I parse this dataset for training, I also add the IOB tags (Inside, Outside, and Beginning)

[(('Claxton', 'NNP\n'), 'B-PLAYER'),
 (('hunting', 'VBG\n'), 'O'),
 (('first', 'RB\n'), 'O'),
 (('major', 'JJ\n'), 'O'),
 (('medal', 'NNS\n'), 'O'),
 (('.', '.\n'), 'O'),
 (('British', 'JJ\n'), 'O'),
 (('hurdler', 'NN\n'), 'O'),
 (('Sarah', 'NNP\n'), 'B-PLAYER'),
 (('Claxton', 'NNP\n'), 'I-PLAYER')......]


Then I just used the ClassifierBasedTagger(There are other taggers too). I can't find the source but I used this code:

class NamedEntityChunker(ChunkParserI):
def __init__(self, train_sents, **kwargs):
    assert isinstance(train_sents, Iterable), 'The training set should be an Iterable'

    self.feature_detector = features
    self.tagger = ClassifierBasedTagger(
    train = train_sents, 
    feature_detector = features,
    **kwargs)

def parse(self, tagged_sents):
    chunks = self.tagger.tag(tagged_sents)

    iob_triplets = [(w, t, c) for ((w, t), c) in chunks]

    return conlltags2tree(iob_triplets)


Here features is a function which returns a dictionary of the features to be used such as the previous word, previous word's pos tag etc. Just features to train the model on.

{
    'word' : word,
    'lemma' : stemmer.stem(word),
    'pos' : pos,
    'allascii' : allascii,

    'next-word' : nextword,
    'next-lemma' : stemmer.stem(nextword),
    'next-pos' : nextpos,

    'prev-word' : prevword,
    'prev-lemma': stemmer.stem(prevword),
    'prev-pos' : prevpos
}


You can find useful theory here

I hope this helps."
The Right Approach / Method for Address Completion,"Fuzzy address matching and/or record linkage could be a more efficient solution than deep (DL) learning. The training data for DL would be a collection of addresses. Probabilistically searching through that collection of addresses is probably easier (and faster) than training a DL system.

There are a variety of algorithms and paid services that provide this kind of functionality. One commonly used package is Python Record Linkage Toolkit."
Text mining for text matching,"FuzzyWuzzy would be a good python library for this purpose. A code example from the GitHub readme:

>>> choices = [""Atlanta Falcons"", ""New York Jets"", ""New York Giants"", ""Dallas Cowboys""]
>>> process.extract(""new york jets"", choices, limit=2)
    [('New York Jets', 100), ('New York Giants', 78)]
>>> process.extractOne(""cowboys"", choices)
    (""Dallas Cowboys"", 90)


I imagine you would like to use the extract function with limit 5 with choices being set equal to all the 'official names' from your table2 csv file.

Details on installation and use are at the link above."
NLP - Is Gazetteer a cheat?,"Gazetteer or any other option of intentionally fixed size feature seems a very popular approach in academic papers, when you have a problem of finite size, for example NER in a fixed corpora, or POS tagging or anything else. I would not consider it cheating unless the only feature you will be using is Gazetteer matching.

However, when you train any kind of NLP model, which does rely on dictionary while training, you may get real world performance way lower than your initial testing would report, unless you can include all objects of interest into the gazetteer (and why then you need that model?) because your trained model will rely on the feature at some point and, in a case when other features will be too weak or not descriptive, new objects of interest would not be recognized.

If you do use a Gazetteer in your models, you should make sure, that that feature has a counter feature to let model balance itself, so that simple dictionary match won't be the only feature of positive class (and more importantly, gazetteer should match not only positive examples, but also negative ones).

For example, assume you do have a full set of infinite variations of all person names, which makes general person NER irrelevant, but now you try to decide whether the object mentioned in text is capable of singing. You will rely on features of inclusion into your Person gazetteer, which will give you a lot of false positives; then, you will add a verb-centric feature of ""Is Subject of verb sing"", and that would probably give you false positives from all kind of objects like birds, your tummy when you're hungry and a drunk fellow who thinks he can sing (but let's be honest, he can not) -- but that verb-centric feature will balance with your person gazetteer to assign positive class of 'Singer' to persons and not animals or any other objects. Though, it doesn't solve the case of drunk performer."
Any useful tips on transfer learning for a text classification task,"Generally the task of recognizing one type of text against ""anything else"" is a quite difficult problem, since there is so much diversity in text that there cannot be any good representative sample of ""anything else"".

Typically this problem is treated as a one-class classification problem: the idea is for the learning algorithm to capture what represents the positive class only, considering anything else as negative. To my knowledge this is used mostly for author identification and related stylometry tasks. The PAN workshop series offer a good deal of state of the arts methods and datasets around these tasks.

It is also possible to frame the problem as binary classification, but then one must be very creative with the negative instances in the training set. Probably the main problem with your current approach is this: your negative instances are only ""randomly selected among all other topics of the site"". This means that the classifier knows only texts from the site on which it is trained, so it has no idea what to do with any new text which doesn't look like anything seen in the training data. A method which has been used to increase the diversity of the negative instances is to automatically generate google queries with a few random words which appear in one of the positive instances, then download whatever text Google retrieves as negative instance.

Another issue with binary classification is the distribution of positive/negative instances: if you train a model with 50/50 positive/negative, the model expects that by default there is 50% chance for each. This can cause a huge bias when applied to a test set which contains mostly negative instances, especially if these don't look like the negative instances seen during training.

Finally be careful about the distinction semantic topic vs. writing style, because the features for these two are usually very different: in the former case the stop words are usually removed, the content words (nouns, verbs, adjectives) are important (hence one uses things like TFIDF). In the latter it's the opposite: stop words and punctuation should be kept (because they are good indicators of writing style) whereas content words are removed because they tend to bias the model the topic instead of the style. In stylometry features based on characters n-grams have been shown to perform well... even though it's not very clear why it works!"
how to programmatically introduce grammatical errors in sentences,"Generating artificial errors is generally risky in NLP, because it's difficult to make sure that the type and distribution of errors correspond exactly to real human errors. If the artificial errors diverge from real errors and a model is trained based on this data, the model will appear to have very good performance since it will rely on the patterns used to generate the errors. However it might not perform well with real data, and it would be difficult to detect it.

That being said, it's been a problem which has been studied for quite a while so the state of the art should help: Google Scholar gives a lot of references, probably some of these papers provide existing implementations as well. One may notice that the concerns I mentioned above are a recurrent question, with some recent papers analyzing how much artificial errors actually help."
Word embedding of a new word which was not in training,"Generating word embeddings for ""OOV""(out of vocabulary) words is one of the major limitations of many standard embeddings like Glove and word2vec. However, fastText circumvents this problem to some extent.

Instead of the traditional approaches which have distinct vectors for each word, they take a character n-grams level representation. For instance, a word with n= 3, will be represented by the character n-grams:

<wh, whe, her, ere, re>

and the special sequence:

< where >

Here,<>are part of the n-grams.

s(w,c)=
‚àë
gŒµ
G
w
z
T
g
v
c
ùë†
(
ùë§
,
ùëê
)
=
‚àë
ùëî
ùúÄ
ùê∫
ùë§
ùëß
ùëî
ùëá
ùë£
ùëê
Here, 
G
ùê∫
 represents the size of a dictionary of n-grams, and given a word 
w
ùë§
, then 
G
w
‚äÇ{1,...,G}
ùê∫
ùë§
‚äÇ
{
1
,
.
.
.
,
ùê∫
}
 represents the set of n-grams appearing in 
w
ùë§
. They associate a vector representation 
z
g
ùëß
ùëî
 to each n-gram 
g
ùëî
 and represent a word by the sum of the vector representations of its n-grams.

This helps them tackle OOV words via knowing some representations of a subword. For an instance, an OOV word: sechero

The 3-gram:

<se, sec, ech, che, her, ero, ro>

since, these 3-grams may have been encountered during learning, through other known words, like:

<se - section che - cheer ro> - hero

Hence, it can form at least some sensible embedding, instead of returning a useless <UNK>

Fastext in fact is an extension to word2vec, with majorly the feature explained above."
Generating synonyms or similar words from multiples word embeddings,"Gensim has a built in functionality to find similar words, using Word2vec. You can train a Word2Vec model using gensim:

model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)


You can make use of the most_similar function to find the top n similar words. It allows you to input a list of positive and negative words to tackle the 'good' and 'bad' problem. You can play around with it.

model.most_similar(positive=[], negative=[], topn=10, restrict_vocab=None)


An example, provided in the documentation:

model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10, restrict_vocab=None)
[('queen', 0.50882536), ...]


topn = the number of nearest neighbors you want to the input combination list of positive and negative words.

restrict_vocab = an optional integer which limits the range of vectors which are searched for most-similar values. For example, restrict_vocab=10000 would only check the first 10000 word vectors in the vocabulary order. (This may be meaningful if you‚Äôve sorted the vocabulary by descending frequency.)

Here is the link to the documentation of what I am talking: http://man.hubwiz.com/docset/gensim.docset/Contents/Resources/Documents/radimrehurek.com/gensim/models/word2vec.html

Here is a link to how to train a word2vec model from scratch: https://radimrehurek.com/gensim/models/word2vec.html

You can also look at some other functions that come with it which allow you to find similar words just by a single vector, you can find these in the second link:

self.wv.similar_by_vector()
self.wv.similar_by_word()"
Doc2vec '-' symbol occurrence,"gensim's Phrases module may also be helpful:

from gensim.models import Phrases
documents = [
    ""the mayor of new york was there"",
    ""machine learning can be useful sometimes"",
    ""new york mayor was present""
]

sentence_stream = [doc.split("" "") for doc in documents]

bigram = Phrases(sentence_stream, min_count=1, threshold=2)

sent = [u'the', u'mayor', u'of', u'new', u'york', u'was', u'there']
print(bigram[sent])
# Expected output:
# [u'the', u'mayor', u'of', u'new_york', u'was', u'there']


That code is from this other answer (I've copy-pasted it above for convenience).

For more on the Phrases module, check this page out."
What does dimension represent in GloVe pre-trained word vectors?,"Glove creates word vectors that capture meaning in vector space by taking global count statistics. The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words probability of co-occurrence. while optimizing this, you can use any number of hidden representations for word vector.

In the original paper, they trained with 25, 50, 100, 200, 300. These dimensions are not interpretable. after training, we are getting a vector with 'd' dim that captures many properties of that word. If the dimension is increasing, the vector can capture much more information but computational complexity will also increase."
Detect data (web textual content) age,"Go digging through the html - you will find consistent tags/styles/formats across multiple sites.

For example: From the bbc we can see a datetime/timestamp 

As always, someone has already done this for you in the htmldate package, built on dateparser which you can use directly as well."
What are common ways to identify subject and object from a question?,"Google SyntaxNe has pre-trained models for French. It is open source and can be customized for a specific use-case.

https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/universal.md

With this, you can create a dependency tree for questions and evaluate fitness for this application."
How to create a language translator from scratch?,"Google translate itself uses Deep learning to translate sentences which can be seen here.

You can translate sentences across languages for which you need two things :

A large dataset which has pairs of translations ( like English-French ). You can find such a dataset from here.
A sequence-to-sequence RNN model. They have Encoder-Decoder architecture which encodes the source sentence into a thought vector and then decode it to form the translation. This image may be helpful.  The picture shows the Neural Machine Translation adopted by Google.

Also, TensorFlow and Keras help in creating such deep learning models. You can refer to this blog from the author of Keras. These models ( seq2seq ) have gained popularity in text summarization, NMT, and other sequence-to-sequence tasks.

If you wish to create a full-scale production ready language translator then you will need a GPU as these models are heavy to train as well as a very large corpus of text."
How is GPT able to handle large vocabularies?,"GPT-2 does not use a word-level vocabulary but a subword-level vocabulary, specifically byte-pair encoding (BPE). This means that it does not predict the next word but the next subword token.

BPE tries to find the pieces of words that are most reusable. BPE also keeps character subwords (e.g. ""a"", ""W"").

The subword vocabulary used by GPT-2 is 50k tokens big. You can take a look at the vocabulary here. There, you can see that token #12175 is ""retro"" and token #11147 is ""fit"", so when tokenizing word ""retrofit"", you would probably get the 2 subword tokens #68 and #12541. When tokenizing a low-frequency word (i.e. that did not appear in the BPE training data), you probably end up with small subwords tokenization, e.g. ""Kilimanjaro"" --> ""Kil"", ""iman"", ""jar"", ""o"".

Using a word-level vocabulary is very troublesome, because:

the amount of existing word surface forms exceeds the size that it is manageable for a neural net, even for a morphologically simple language like English. For morphologically-rich fusional languages and, especially, agglutinative languages, using word-level vocabularies is much more inconvenient.
language is ""open"", in the sense that new words can be created, either by stitching together existing words (e.g. manspreading), or by making up new words altogether (e.g. bazinga).
the less frequently a token appears in the training data, the less our net will learn to use it. If we have many many different tokens, the frequency of appearance of each token in the training data will be very low (i.e. data scarcity), so our net will not learn properly.

Therefore, subword vocabularies are the norm nowadays."
News Classification,"Have a look at 20ng dataset and it's classification techniques. It is a collection of news articles which are divided into 20 classes. It is not exactly same as yours, but similar. The second line you mentioned about Tim Cook might be a difficult sentence for classification, so I'd suggest you to have a good training dataset before you start.

For simple beginning, you can try text cleaning and tokenizations. Refer to this, one previous answer already provided which has relevant links and further information.

For classification techniques further on themes, I found that this one gives a really good performance on 20ng"
How to extract the positions of employee from raw text [closed],"Have a look at this to see how a Knowledge Base can be built. I would say the best way is to build your own Knowledge Base based on your corpus if you have enough data.

The idea (simplified) for starting is to have patterns such as ""NAME is POSITION"" by seeing some of data. Through this you find many names and positions. Then you extract new patterns from them and scan whole data again. Do it until no new pattern shows up. Then you end up with lots of is-a relations which is what you need.

For extracting positions you may need sentence segmentation. Have a look at slide 56 onward to get an overview.

PS: If you do not have enough data, you may crawl web and get texts and try to make your lexical databases. Target websites with more probability of having such a context.

Hope it helps. Good Luck!"
Latent Dirichlet Allocation vs Hierarchical Dirichlet Process,"HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of ""topics"" in document-modeling terms) is not known a priori. So that's the reason why there's a difference.

Using LDA for document modeling, one treats each ""topic"" as a distribution of words in some known vocabulary. For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).

For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics. So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.

As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance. I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable."
Building a search tool and classifying text using NLP and ML,"Here are a few general thoughts about your project:

As far as I understand, you're trying to extract very specific information from a semi-structured database using free-form natural language queries, correct? If yes it's important that you realize that this is a quite ambitious project, reaching a decent stage of quality is probably going to take a lot of work, and the performance is unlikely to be perfect.

Apparently numerical values and units are important pieces of information for matching the query. In this case you should probably implement a special process for those, because standard text processing is not going to work very well.

Detection: if there is only a small number of possible ways in which these values are written, it's probably more efficient to use ad-hoc regular expressions. If not, you could try to train a custom NER model.

Representation: that's the tricky part imho. For general text vectors are fine, but with vectors it's difficult/impossible to represent special values like those. Given that you have a semi-structured database, you might want to try a more semantic representation adapted to your data: that could involve techniques from semantic role labeling, relationship extraction, etc.

Matching: the advantage of a semantic representation is that you could convert the query to a semantic representation and then apply a detailed matching procedure suited to your data (in particular you can compare corresponding numerical values and use a threshold, or return the difference to represent how far they are)."
How should I use BERT embeddings for clustering (as opposed to fine-tuning BERT model for a supervised task),"Here are the answers:

In sequence modeling, we expect a sentence to be ordered sequence, thus we cannot take random words (unlike bag of words, where we are just bothered about the words and not really the order). For example: In bag of words: ""I ate ice-cream"" and ""ice-cream ate I"" are same, while this is not true for the models that treat entire sentence as ordered sequence. Thus, you cannot pick N random words in a random order.
Choosing tokens is model dependent. You can always preprocess to remove stop words and other contents such as symbols, numbers, etc if it acts as noise than the information.
I would like to clarify that lemmatizing and word-piece tokenization is not the same. For example, in lemmatization ""playing"" and ""played"" are lemmatized to ""play"". But in case of word-piece tokenization it's likely split into ""play""+""##ing"" or ""play""+""ed"", depending on the vocabulary. Thus, there is more information preserved.
max_length should be optimally chosen such that most of you sentences are fully considered. (i.e, most of the sentences should be shorter than max_length after tokenization). There are some models which considers complete sequence length. Example: Universal Sentence Encoder(USE), Transformer-XL, etc. However, note that you can also use higher batch size with smaller max_length, which makes the training/fine-tuning faster and sometime produces better results.
The pretrained model is trained with MAX_LEN of 512. It's a model's limitation.
In specific to BERT,as claimed by the paper, for classification embeddings of [CLS] token is sufficient. Since, its attention based model, the [CLS] token would capture the composition of the entire sentence, thus sufficient. However, you can also average the embeddings of all the tokens. I have tried both, in most of my works, the of average of all word-piece tokens has yielded higher performance. Also, some work's even suggests you to take average of embeddings from the last 4 layers. It is merely a design choice.
Using sentence embeddings are generally okay. But, you need to verify with the literature. There can always be a better technique. Also, there are models specific to sentence embeddings (USE is one such model), you can check them out."
What are the main types of NLP annotators?,"Here are the basic Natural Language Processing capabilities (or annotators) that are usually necessary to extract language units from textual data for sake of search and other applications:

Sentence breaker - to split text (usually, text paragraphs) to sentences. Even in English it can be hard for some cases like ""Mr. and Mrs. Brown stay in room no. 20.""

Tokenizer - to split text or sentences to words or word-level units, including punctuation. This task is not trivial for languages with no spaces and no stable understanding of word boundaries (e.g. Chinese, Japanese)

Part-of-speech Tagger - to guess part of speech of each word in the context of sentence; usually each word is assigned a so-called POS-tag from a tagset developed in advance to serve your final task (for example, parsing).

Lemmatizer - to convert a given word into its canonical form (lemma). Usually you need to know the word's POS-tag. For example, word ""heating"" as gerund must be converted to ""heat"", but as noun it must be left unchanged.

Parser - to perform syntactic analysis of the sentence and build a syntactic tree or graph. There're two main ways to represent syntactic structure of sentence: via constituency or dependency.

Summarizer - to generate a short summary of the text by selecting a set of top informative sentences of the document, representing its main idea. However can be done in more intelligent manner than just selecting the sentences from existing ones.

Named Entity Recognition - to extract so-called named entities from the text. Named entities are the chunks of words from text, which refer to an entity of certain type. The types may include: geographic locations (countries, cities, rivers, ...), person names, organization names etc. Before going into NER task you must understand what do you want to get and, possible, predefine a taxonomy of named entity types to resolve.

Coreference Resolution - to group named entities (or, depending on your task, any other text units) into clusters corresponding to a single real object/meaning. For example, ""B. Gates"", ""William Gates"", ""Founder of Microsoft"" etc. in one text may mean the same person, referenced by using different expressions.

There're many other interesting NLP applications/annotators (see NLP tasks category), sentiment analysis, machine translation etc.). There're many books on this, the classical book: ""Speech and Language Processing"" by Daniel Jurafsky and James H. Martin., but it can be too detailed for you."
Coreference Resolution for German Texts,"Here is a couple of tools that may be worth a look:

Bart, an open source tool that have been used for several languages, including German. Available from the website
Sucre is a tool developed at the University of Stuttgart. I don't know if it's available easily. You can see this paper about it."
Construct word2vec (CBOW) training data from beginning of sentence,"Here is a great answer to this question. I'll summarize:

The code example was taken from a ""buggy"" repository on GitHub and is not typical of robust solutions.
Robust solutions actually do use the first word as a target word. If the context window is length 10, then the method uses the next 5 words as the context and the first word as the target (it won't actually have a context of size 10 since the first half of the context doesn't exist).
Even though the first few words in the sentence are used as target words, they still will not appear in as many contexts. This issue is mitigated because they appear in smaller contexts. Since the contexts they appear in are smaller, they have more impact on the context and, therefore, more significance in back propagation.
Many of the more robust implementations will use an entire paragraph or document as opposed to a sentence (some even include punctuation). This make sense because the ending of one sentence may give context for the beginning of another sentence. When this approach is implemented, there are far fewer start/ending words, which reduces the issue.

The answer linked above has some other helpful details and is worth reading."
Public dataset for news articles with their associated categories,"Here is a massive dataset of news with categories which I created for exactly such a reason.

Includes all the headlines published by Times of India from 2001-2019 with categories.

Contains ~3 million entries."
How to automatically find the sentiment?,"Here is a tutorial, that should help you get started."
What is the positional encoding in the transformer model?,"Here is an awesome recent Youtube video that covers position embeddings in great depth, with beautiful animations:

Visual Guide to Transformer Neural Networks - (Part 1) Position Embeddings

Taking excerpts from the video, let us try understanding the ‚Äúsin‚Äù part of the formula to compute the position embeddings:

Here ‚Äúpos‚Äù refers to the position of the ‚Äúword‚Äù in the sequence. P0 refers to the position embedding of the first word; ‚Äúd‚Äù means the size of the word/token embedding. In this example d=5. Finally, ‚Äúi‚Äù refers to each of the 5 individual dimensions of the embedding (i.e. 0, 1,2,3,4)

While ‚Äúd‚Äù is fixed, ‚Äúpos‚Äù and ‚Äúi‚Äù vary. Let us try understanding the later two.

""pos""

If we plot a sin curve and vary ‚Äúpos‚Äù (on the x-axis), you will land up with different position values on the y-axis. Therefore, words with different positions will have different position embeddings values.

There is a problem though. Since ‚Äúsin‚Äù curve repeat in intervals, you can see in the figure above that P0 and P6 have the same position embedding values, despite being at two very different positions. This is where the ‚Äòi‚Äô part in the equation comes into play.

""i""

If you vary ‚Äúi‚Äù in the equation above, you will get a bunch of curves with varying frequencies. Reading off the position embedding values against different frequencies, lands up giving different values at different embedding dimensions for P0 and P6.

Correction

Thanks to @starriet for the correction. ""i"" is not the index of the element ""within"" each vector, but the ""sequence"" of the vector. It is used for making alternate even and odd sequences (2i and 2i+1)."
"Is there ""Attention Is All You Need"" implementation in Keras?",Here is an implementation from PyPI.
How is padding masking considered in the Attention Head of a Transformer?,"Here is how the fairseq implementation works.

First it turns the BH x T x S matrix into B x T x S, where B is batch size, H is number of heads, T is target length, and S is source length. You are not using multihead so can skip this. (Also when doing self-attention T == S, what you have called L). As can final step to split the data back into BH x T x S after applying the mask.

Their key_padding_mask is what you are calling src_key_padding_mask, and is B x L in size. So the next step is to turn that into B x 1 x 1 x L. Again, as you are not using multihead, you only need to use unsqueeze(1) once to give you B x 1 x L.

They then use masked_fill. So it becomes a one-liner:

out = out.masked_fill( src_key_padding_mask.unsqueeze(1), float(""-inf"") )


It goes after your current mask code and just before the call to softmask.

BTW, I wrote the above referencing my own local copy which I've hacked on and simplified a lot. Looking at the github version (linked above) I see they use view() to add a 5th dimension, and then use unsqueeze() three times. I can't tell you why; something to do with when kv_bsz is different to bsz. It is only in the not is_tpu block, so just some clever optimization?"
Get row wise frequency count of words from list in text column pandas,"Here is one way to approach the core logic:

def count_phrases(string: str, phrases: str) -> dict:
    ""Find the number of occurances of phrases in a string.""
    return {phrase: string.count(phrase) for phrase in phrases}

string = ""I was going to buy new house last week but it was raining since then. Once the rain stops I'll go and buy new house""
phrases = [""rain"", ""buy new house"", ""tornado""]

assert count_phrases(string, phrases) == {'rain': 2, 'buy new house': 2, 'tornado': 0}


The function then could be used in a Pandas DataFrame with .apply"
Algorithms and techniques for spell checking,"Here is what I built...

Step 1: Store all the words in a Trie data structure. Wiki about trie.

Step 2: Train an RNN or RNTN to get seq2seq mapping for words and store the model

Step 3: Retrieve top n words with levenshtein distance with the. Wiki about LD with the word that you are trying to correct.

Naive Approach: Calculating the edit distance between the query term and every dictionary term. Very expensive.
Peter Norvig's Approach: Deriving all possible terms with an edit distance<=2 from the query term, and looking them up in the dictionary. Better, but still expensive (114,324 terms for word length=9 and edit distance=2) Check this.
Faroo's Approach: Deriving deletes only with an edit distance<=2 both from the query term and each dictionary term. Three orders of magnitudes faster. http://blog.faroo.com/2012/06/07...

Step 4: Based on the previous 2 or 3 or 4 words, predict the words that are retrieved from Step 3 above. Select the number of words depending on how much accuracy you want (Of course we want 100% accuracy), and the processing power of the machine running the code, you can select the number of words you wanna consider.

Check out this approach too. It's an acl paper from 2009. They call it - language independent auto correction."
What is the difference between word-based and char-based text generation RNNs?,"Here is what I learnt recently.

Obviously, when talking about text generation RNNs we are talking about RNN language models. When asking about word/char-based text generation RNNs, we are asking about word/char-based RNN language models (LM).

Word-based LMs display higher accuracy and lower computational cost than char-based LMs.

This drop of performance is unlikely due to the difficulty for character level model to capture longer short term memory, since also the Longer Short Term Memory (LSTM) recurrent networks work better with word-based input.

This is because char-based RNN LMs require much bigger hidden layer to successfully model long-term dependencies which means higher computational costs.

Therefore, we can say that

one of the fundamental differences between the word level and character level models is in the number of parameters the RNN has to access during the training and test. The smaller is the input and output layer of RNN, the larger needs to be the fully connected hidden layer, which makes the training of the model expensive.

However, char-based RNN LMs better model languages with a rich morphology such as Finish, Turkish, Russian etc. Using word-based RNN LMs to model such languages is difficult if possible at all and is not advised.

The above analysis makes sense especially when you look at the output text, generated by char-based RNNs:

The surprised in investors weren‚Äôt going to raise money. I‚Äôm not the company with the time there are all interesting quickly, don‚Äôt have to get off the same programmers.

While simple char-based Maximum Likelihood LM with a 13-character window delivers this:

And when she made many solid bricks. He stacked them in piles and stomped her feet. The doctor diagnosed him with a bat. The girl and her boyfriend asked her out.

Of course I cherry-picked the example (actually most ML LM examples looked better than any RNN generated text I've read so far) and this tiny ML LM was trained on a simpler corpus but you get the idea: straightforward conditional probability generates better texts than far more complex char-based RNN.

Char-based RNN LMs can mimic grammatically correct sequences for a wide range of languages, require bigger hidden layer and computationally more expensive while word-based RNN LMs train faster and generate more coherent texts and yet even these generated texts are far from making actual sense."
Paper proving neural networks with more data are better in NLP than traditional methods,"Here's a blog post providing a view to the history of the process with several popular references in it. In particular, consider the Natural Language Processing section and its conclusion:

On NLP tasks that have a long enough history to graph, there seems to be no clear indication that deep learning performs above trendline.

Also, worth noting from the general conclusion of the article:

Deep learning provides discontinuous jumps relative to previous machine learning or AI performance trendlines in image recognition and speech recognition; it doesn‚Äôt in strategy games or natural language processing, and machine translation and arcade games are ambiguous (machine translation because metrics differ; arcade games because there is no pre-deep-learning comparison.)"
How do I load FastText pretrained model with Gensim?,"Here's the link for the methods available for fasttext implementation in gensim fasttext.py

from gensim.models.wrappers import FastText

model = FastText.load_fasttext_format('wiki.simple')

print(model.most_similar('teacher'))
# Output = [('headteacher', 0.8075869083404541), ('schoolteacher', 0.7955552339553833), ('teachers', 0.733420729637146), ('teaches', 0.6839243173599243), ('meacher', 0.6825737357139587), ('teach', 0.6285147070884705), ('taught', 0.6244685649871826), ('teaching', 0.6199781894683838), ('schoolmaster', 0.6037642955780029), ('lessons', 0.5812176465988159)]

print(model.similarity('teacher', 'teaches'))
# Output = 0.683924396754"
Is NLP suitable for my legal contract parsing problem?,"Here's what I would have done.

extract raw data from the the pdfs.
use rule based NER from spacy(it's quite fast), but it will require some manual pattern making. Also apply multiprocessing if you can. https://spacy.io/usage/rule-based-matching
You can also write a document classification to segregate the templates and send the templates to appropriate code,running parallely.
Also checkout blackstone library for legal texts, it might be of some use https://spacy.io/universe/project/blackstone"
UniLM - Unified Language Model for summarization,"Here's what you should do

Prepare your dataset: Follow similar instructions as described in the paper and preprocess your dataset. This will be your major task as after this you will only have to fine-tune the model. If you don't have a dataset, you can use the dataset used in this research paper, which can be downloaded from here.

Download the pre-trained model. Or you can choose to start from the provided fine-tuned model checkpoint (from the link). You will have to check which version of the model works best for your dataset. If you select model fine-tuned for summarization task and your dataset is similar to CNN/DailyMail dataset [37] and Gigaword [36] you can skip fine-tuning.

Fine-tune model: In this step, you will be using the command mentioned in the readme of the Github repository. Note that, there are some parameters that should be according to the language model you have downloaded in the previous step. Based on the size of your dataset you can change the number of epochs in the following command. You should also note that this will require GPU. The repository readme recommends 2 or 4 v100-32G GPU cards for finetuning the model.

    OUTPUT_DIR=/{path_of_fine-tuned_model}/
    MODEL_RECOVER_PATH=/{path_of_pre-trained_model}/unilmv1-large-cased.bin
    export PYTORCH_PRETRAINED_BERT_CACHE=/{tmp_folder}/bert-cased-pretrained-cache
    export CUDA_VISIBLE_DEVICES=0,1,2,3
    python biunilm/run_seq2seq.py --do_train --fp16 --amp --num_workers 0 \
      --bert_model bert-large-cased --new_segment_ids --tokenized_input \
      --data_dir ${DATA_DIR} \
      --output_dir ${OUTPUT_DIR}/bert_save \
      --log_dir ${OUTPUT_DIR}/bert_log \
      --model_recover_path ${MODEL_RECOVER_PATH} \
      --max_seq_length 192 --max_position_embeddings 192 \
      --trunc_seg a --always_truncate_tail --max_len_a 0 --max_len_b 64 \
      --mask_prob 0.7 --max_pred 48 \
      --train_batch_size 128 --gradient_accumulation_steps 1 \
      --learning_rate 0.00003 --warmup_proportion 0.1 --label_smoothing 0.1 \
      --num_train_epochs 30


Evaluate your model: Use biunilm/decode_seq2seq.py to decode (predict the output of evaluation dataset) and use the provided evaluation script to evaluate the trained model.

Use the trained model: In order to use this model to make a prediction, you can simply write your own python code to:

load Pytorch pre-trained model using pytorch_pretrained_bert library as used in decode_seq2seq.py file
Tokenize your input
predict the output and detokenize the output.

Here is the logic which you can use:

model = BertForSeq2SeqDecoder.from_pretrained(long_list_of_arguments)
batch = seq2seq_loader.batch_list_to_batch_tensors(input_batch)
input_ids, token_type_ids, position_ids, input_mask, mask_qkv, task_idx = batch
traces = model(input_ids, token_type_ids,position_ids, input_mask, task_idx=task_idx, mask_qkv=mask_qkv)


Note that this is not the complete logic. This code just show how github repository code has handled the saved model and used it to make predictions. Use traces to convert ids to tokens and detokenize the output tokens (as used in the code here). The detokenization step is necessary as the input sequence is tokenized to subword units by WordPiece.

For reference here is the code which loads the pre-trained model. You can go through the loop to understand the logic and implement it in your case. I hope this helps."
High / low resources language : what does it mean?,"High resource languages are languages for which many data resources exist, making possible the development of machine-learning based systems for these languages. English is by far the most well resourced language. West-Europe languages are quite well covered, as well as Japanese and Chinese. Naturally low-resource languages are the opposite, that is languages with none or very few resources available. This is the case for some extinct or near-extinct languages and many local dialects. There are actually many languages which are mostly oral, for which very few written resources exist (let alone resources in electronic format); for some there are written documents but not even something as basic as a dictionary.

There are many different types of resources which are needed in order to train good language-based systems:

a high amount of raw text from various genres (type of documents), e.g. books, scientific papers, emails, social media content, etc.
lexical, syntactic and semantic resources such as dictionaries, dependency tree corpora, semantic databases (e.g. WordNet), etc.
task-specific resources such as parallel corpora for machine translation, various kinds of annotated text (e.g. with part-of-speech tags, named entities, etc.)

Many types of language resources are costly to produce, this is why the economic inequalities between countries/languages are reflected in the amount (or absence) of language resources. The Universal Dependencies project is an interesting effort to fill this gap."
ChatGPT with multilingual language,"How it can answer with multilingual language?

The training set is multilingual, and if a text starts in some language, it'll most likely continue with the same language.

See {1} for an evaluation of ChatGPT in the multilingual setting.

References:

{1} Lai, Viet Dac, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. ""ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning."" arXiv preprint arXiv:2304.05613 (2023)."
Accuracy of Stanford NER,"http://en.wikipedia.org/wiki/Named-entity_recognition#Formal_evaluation :

To evaluate the quality of a NER system's output, several measures have been defined. While accuracy on the token level is one possibility, it suffers from two problems: the vast majority of tokens in real-world text are not part of entity names as usually defined, so the baseline accuracy (always predict ""not an entity"") is extravagantly high, typically >90%; and mispredicting the full span of an entity name is not properly penalized (finding only a person's first name when their last name follows is scored as ¬Ω accuracy).

In academic conferences such as CoNLL, a variant of the F1 score has been defined as follows:

Precision is the number of predicted entity name spans that line up exactly with spans in the gold standard evaluation data. I.e. when [Person Hans] [Person Blick] is predicted but [Person Hans Blick] was required, precision for the predicted name is zero. Precision is then averaged over all predicted entity names.
Recall is similarly the number of names in the gold standard that appear at exactly the same location in the predictions.
F1 score is the harmonic mean of these two.

It follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, ""scores no points"", i.e. does not contribute to either precision or recall."
Sub topics with Latent Dirichlet Allocation,"I actually do not think your method is a good way to find subtopics. Consider a document X with a distribution of topics z. X is made up of a mixed model distribution of topic Z. If you just give a document the most domiant distribution, and then run lda again, you might find subtopics but you'll also refind the topics that should should perhaps not be considered subtopics.

For example, let's consider a document that talks about food and exercising and why certain foods and cardo is good for heart health. Let's assume that we find topics 1 and 2 such that the main topics are food, and exercise. Suppose that the domiant distribution is topic 1. Then running LDA on that particular document again, you're going to refine food, exercise and perhaps nutrition, viatims, etc. However, there's no real reasons what should be considered the topic without inspecting each one and making an inference regarding it. Also you'll have no real way to discern how deep you should go down this tree.

The original paper on hLDA can prove useful and exploits the chinese restraurant process and its relationship to dirichlet distribution to form the topic relationship and tree depth problem. Also on David Blei github (bleilab) has an implentation in c++. You can essentially setup a little shell script after processing the data in a langauge you may be more familar with and use his code."
What is the difference between NLP and text mining?,"I agree with Sean's answer. NLP and text mining are usually used for different goals. Also, there is indeed an overlap and both definitions are vogue.

Other than the difference in goal, there is a difference in methods. Text mining techniques are usually shallow and do not consider the text structure. Usually, text mining will use bag-of-words, n-grams and possibly stemming over that.

In NLP methods usually involve the text structure. You can find there sentence splitting, part-of-speech tagging and parse tree construction. Also, NLP methods provide several techniques to capture context and meaning from text.

Typical text mining method will consider the following sentences to indicate happiness while typical NLP methods detect that they are not

I am not happy
I will be happy when it will rain
If it will rain, I'll be happy.
She asked whether I am happy
Are you happy?"
Can we combine multiple K-Means Models as a single model?,"I agree with your assumption, the vector space is the same so I don't see any major problem with this approach. Still this approach might cause some more subtle bias, depending on the differences between the models (sets of terms, number of clusters). I could imagine the following problems happening:

if there is a big difference in number of clusters between models, a model which has more clusters is more likely to contain the closest match, simply because it has more centroids. This might favour the most precise clusters (this might actually be a good thing, depends).
if there are many models sometimes there might be many close centroids across the models, and this would probably make the selection of the closest among them almost random: the exact position of a centroid is significant with respect to other centroids within the same model, not necessarily with respect to other centroids outside the model."
BertPunc (punctuation restoration with BERT),"I am also confused by the method of BertPunc. Actually I can't believe it really works. Beacause the method use BertLanguageModel to do the job, and LanguseModel's main capabiltiy is to predict the next word of a word. So also use your example, BertPunc inputs 8 words, the output of bert should be 8 next words of the inputs. But BertPunc added a Linear layer to only get 1 output value, which is to predict punctuation.

But I think we should use bert the way like the graph below:"
Custom Named-Entity Recognition (NER) in product titles using deep learning,"I am currently working on a similar project but limited only to brand detection in product titles, the task is a named entity recognition task and can be solved by different models, the most used ones are BI-LSTM + CRF (Bidirection LSTM with a CRF layer on top). You could try to use spaCy for the task which has a nice documentation and good workflow to train NER models. Or you can build the model yourself using tensorflow (here is a good tutorial) or PyTorch (there is a tutorial here)

Obviously, you will need annotated data to train your models.

Here are some papers that might help you get some insights (at least they did for me) :

Sequence tagging with BI-LSTM CRF
Product name detection in user generated content (a bit old)

Edit

You can find my project which is similar to yours here with pretrained models: https://github.com/annis-souames/brand-ner

Here's a second project on Github : https://github.com/maciej-cecot/brand-detection"
Algorithm for document retrieval in QA system,"I am still not quite sure how you are solving in the problem with tf-idf for a QA system. However, there have been lots of improvements that has been done in the QA domain over the years, with the usage of deep learning for natural language processing.

I would urge you to look at the following approaches that might help you to reach the accuracies you are looking for:

Bidaf Model that was used to solve SQuAD dataset.
BERT model that was used to solve the SQuAD dataset.

You have multiple open source implementations to train the network and use it for prediction. You could also learn about them in the papers published by the creators of these networks."
Finding associated words to a named entity,"I am unable to add a comment (not enough reputation points), so please do not consider this as an answer.

The closest I can recommended based on the question is to checkout out DBPedia Spotlight. Demo available here https://demo.dbpedia-spotlight.org/

You can also experiment by choosing the type of annotation required by modifying Select Types option"
Issues with audio embedding using wav2vec,"I am wondering if this is because I am using a general XLSR model without fine-tuning it for emotion recognition.

That might be still true but your approach contains a fundamental error you should eliminate first. You are using the class Wav2Vec2FeatureExtractor to extract the features from an audio file, but this class is not a neural network. It is a preprocessor that pads and normalizes the floating point time series from librosa. As stated by the documentation the normalization makes sure that the array has:

zero mean and unit variance

These features, will therefore only makes sense for the model that was trained with it. When you trained an SVM with it, you actually compared if an SVM can beat wav2vec2 and not if wav2vec2 is better than MFCC!

To get the actual embeddings from the wav2vec2 model, you can use the following code:

import librosa
import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model

input_audio, sample_rate = librosa.load(""/content/bla.wav"",  sr=16000)

model_name = ""facebook/wav2vec2-large-xlsr-53""
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
model = Wav2Vec2Model.from_pretrained(model_name)

i= feature_extractor(input_audio, return_tensors=""pt"", sampling_rate=sample_rate)
with torch.no_grad():
  o= model(i.input_values)
print(o.keys())
print(o.last_hidden_state.shape)
print(o.extract_features.shape)


Output:

odict_keys(['last_hidden_state', 'extract_features'])
torch.Size([1, 1676, 1024])
torch.Size([1, 1676, 512])


Please refer to this StackOverflow post for the difference between last_hidden_state and extract_features. As you can see, the features are multi-dimensional for my file ([bacth_size, seq_len, hidden_size]), which means you probably want to apply some pooling (e.g. mean).

P.S.: Another point that comes to my mind when I look at your question, is if those embeddings are actually meaningful by themselves. For the pure BERT, we know that the sentence embeddings:

I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors.

I assume you will probably need to fine-tune wav2vec2 a bit. You can use huggingfaces Wav2Vec2ForSequenceClassification class for that."
Where can I get Insurance claim data for practicing NLP(Natural Language) processing?,"I am working in the same industry for a few years now and I can tell you that there are no publicly available datasets because of the nature of the documents. They are quite private and contains sensitive information, that are bound by rules and regulations."
Why ELMo's word embedding can represent the word better than glove?,"I am wrong. ELMo also use the output of LSTM for context-dependent representation.

The output only from word embedding is the context-independent representation.

Why the representation is useful?

I think it is because, it is learning the difference between words and the representation is not the real meaning for the word."
Implementation of Tree Kernels in Python,"I assume that the ones from here are Moschitti's version, as the author is associated with Moschitti in some papers. I'm unable to confirm, though."
How to evaluate Natural Question-Answer Generation pairs?,"I assume you are using a seq2seq transformer like T5. And the input -> output is probably defined as <context> -> question: ..., answer: ....

Since you do not have any ground truth, I would use another model, a Gold Standard/SOTA if you may, in Question Answering, where its input -> output is question:... , context:... -> <answer>. Hence, I would use the answers of your LLM and evaluate it against the Gold standard where I would use a BLEU score (or something else for text generation) between the answer of your LLM and the answer of the QA Gold Standard.

Of course, in the limitations you should acknowledge the fact that this evaluation is not ideal, since you do not have ground truth. Or maybe you could try using a few QA models and average. You should be careful with the hyper-parameters. Imagine one's sequence output length is less than the other."
How to discard trash topics from topic models?,"I assume you have inspected your topic-by-word matrix, and that is why you say you have 'trash' topics. If no words are strongly associated with a given topic, it may not be a useful topic. If you find many of these, you can re-run the experiment with fewer topics.

If instead there are words strongly associated with these topics, you just don't see the cohesive meaning behind the words associated with the topic, then these are not exactly ""trash"" topics. They are still capturing the latent structure of the data, it just isn't something easily deemed a ""topic"" in the traditional sense of the word.

Another thing to remember is that the hyperparameters also control the shape of your resulting distributions. This answer does a great job explaining alpha and beta."
Matching similar strings,"I assume you have some training data with labels, i.e. data where the titles are already linked to a given class? This is then supervised learning (as opposed to unsupervised learning), and so you could folow the following steps:

Step 1: you have words as input, so you will need a method to create numerical representation (vectors). For that you could look into algorithms such as Word2Vec, Doc2Vec, GLoVE or something like TF-IDF. If you go for the first, you might consider trying the spaCy library in python. Here is a tutorial on Word2Vec using spaCy.

Step 2: once you have your numerical representations for each of your titles, you need to somehow classify them. You could do this a few ways. Perhaps the simplest would be something like a clustering algorithm, e.g. the DB-Scan algorithm in SciKit Learn - here is a demo. You could try more complicated methods, such as Support Vector Machines or Neural Networks, but probably best to start with a method that will get you to some results more quickly. You are classififying titles, so be sure to form your problem as a classification as opposed to a regression problem.

Step 3: assess your results and try changing a part of the loop above.

In the above, I assumed you are talking about the semantic meaning of the conference titles, and not similarity between literal word/letter combinations. That could of course be computed analytically, without the use of a model that learns.

In response to OP's comment: From my experience, using TF-IDF or something called minimal new sets might be a good way to get your titles into representations that allow clustering. Once clusters are formed, it would be up to you to then interpret them and assign labels. If you know that there are e.g. only 10 conference, it shouldn't be too difficult to reach results. Have a look at this master thesis that does a similar thing - instead of conferences, they want to detect topics. Disclaimer: I supervised that thesis."
How to calculate perplexity of language model?,"I believe he meant: you need to identify/predict a sequence of 4 consecutive things. First: an operator, then a sales person, then a technical support person, and finally one name out of 30,000 names. One and only one sequence is correct.

The probability of the correct sequence:

(1/4)‚àó(1/4)‚àó(1/4)‚àó(1/120,000)
=0.0000001302083333
(
1
/
4
)
‚àó
(
1
/
4
)
‚àó
(
1
/
4
)
‚àó
(
1
/
120
,
000
)
=
0.0000001302083333

If you get the 4th root, that gives you the geometric mean (in some sense that's the average per step for four steps)

(0.0000001302083333
)
.25
=0.01899589214‚âà(1/53)
(
0.0000001302083333
)
.25
=
0.01899589214
‚âà
(
1
/
53
)

So:

(1/53)‚àó(1/53)‚àó(1/53)‚àó(1/53)‚âà(1/4)‚àó(1/4)
‚àó(1/4)‚àó(1/120,000)
(
1
/
53
)
‚àó
(
1
/
53
)
‚àó
(
1
/
53
)
‚àó
(
1
/
53
)
‚âà
(
1
/
4
)
‚àó
(
1
/
4
)
‚àó
(
1
/
4
)
‚àó
(
1
/
120
,
000
)

It was however, not clear in the slides or the explanation."
transfer learning with sentiment analysis?,"I can not fully answer your questions, but would like to offer a couple of my thoughts here: 1) Transfer learning for sentiment analysis can be hard given that knowledge learned from one topic may not be not broad or general enough to perform well on the target or downstream tasks. For example, I have recently trained a neural network along with Word2Vec embedding using Twitter airline customer review data and get an prediction accuracy of 77%. However, when I use the same Word2Vec and neural network to classify some general customer review data, I got an prediction accuracy of only 35%.

2) Transfer learning in natural language processing is a hot topic and many researchers are working on it recently. 2018 sees some breakthroughs in transfer learning, i.e., Google Universal Sentence Encoder, BERT algorithm etc. I can not give you a comprehensive list here since I'm also learning. I would suggest you dive into some blog articles or even the original research articles to get a better understanding.

May it helps."
Automatically finding business opportunities in text documents,"I can only answer your first part, if you want to automatically label document if they are talking about funding opportunities, you can train a classification model to classify which document belong to your defined class and which is not. But to train such a model, you need data, and in your data you have to define and label manually documents that belong to your class of interest and documents that do not as examples to your model."
How to give name to topics created using LDA?,"I can suggest several papers on this topic:

Automatic Labelling of Topic Models
Automatic Labeling Hierarchical Topics
Representing Topics Labels for Exploring Digital Libraries

You can find more by looking at their citations."
Alternative Hunspell dictionary for stemming,"I can't comment on your question (my reputation isn't high enough), but I've got a few clarifying questions to ask: Why do you want to stem words? Text classification? Sentiment analysis? Something else?

I've used both the 'tm' package and the 'RTextTools' package for stemming when doing text classification. Both have some built-in functions for stemming.

Whether they would be useful to you or not depends on why you want to stem words ...

FWIW, here are some links related to those packages:

tm: https://cran.r-project.org/web/packages/tm/index.html
RTextTools: http://www.rtexttools.com/documentation.html"
Document similarity,"I did something similar a while ago. We wanted to classify several types of pdf.

We first extracted the text of the documents.
We created NLP features with the text
Then added pdf metadata: size of the file, number of pages, name of the document...
We then built a classification model with a few samples and did Active Learning

I guess that you could also do unsupervised learning but I like it more when you can do supervised learning."
"Given one language ngram model, how do I compare likelihoods of two texts of different length?","I didn't find any good explanations or papers on this topic, other than things about category based models and multigrams (parts of words). So, I came up with one myself. I'm using Java, but here's the code for the length normalization translated to Python. Also, this is assuming that you're adding the log probabilities for each word together, like KenLM does for the total sentence (or phrase) score.

# Get likelihood normalized by length and number of words.
# Since the likelihoods of each word are added together, phrases with more words tend to 
# have a lower probability.
def getNormLikelihood(phrase):
    score = getLikelihood(phrase) # The normal score
    numWords = phrase.count(' ')+1 # The number of words in the phrase.
    # Get the absolute difference between the 
    #   average word length of all words and the average word length of the phrase.
    # Then, weight that weight by the reciprocal of the normal score, such that the 
    #   lenWeight is less if the overall score is less.
    # Intuitively, this means that we care less about normalizing by word length the more 
    #   common the phrase is.
    # This effectively solves the problem of giving a greater likelihood for long words 
    #   that aren't even in the language model (and are given the default ""<unk>"" score).
    lenWeight = abs(len(phrase) / numWords - avgLen) * (-1/score)
    return score / (1/lenWeight)


How you want to use this normalized score, and how effective it is, depends on your application. In my case (splitting words), I used this normalized score to score each splitting option, but then also added (logarithm) the normal score of the option with context, which you could consider the prior for the option (in Naive Bayes terms). I then added an overall prior (manually picked, but could be tuned) to the original, unsplit option, for the probability of needing to split a word. The normalized score greatly helped, and with these probabilities together, I was able to get 100% accuracy for my test cases."
Idf values of English words,"I don't believe that there are any precalculated idf values out there. Inverse Document Frequency (idf) is the inverse of the number of documents in which a particular word appears in your corpus. If you only have one document, I'm afraid that value is simply 1.

However, if you are looking to get rid of words such as the, as, and it which don't carry much meaning, nltk in Python has some useful tools to remove these ""stop words"" from your document and might help you.

Here is a helpful example."
using LSTM encoder decoder framework to generate sentences,"I don't know about any research on this topic, but I have thought about similar ideas before. Let's assume the decoder is some RNN that outputs probabilites conditioned on the encoder vector and on what it has generated so far, 
P(Word|EncodedVector,DecodedSoFar)
ùëÉ
(
ùëä
ùëú
ùëü
ùëë
|
ùê∏
ùëõ
ùëê
ùëú
ùëë
ùëí
ùëë
ùëâ
ùëí
ùëê
ùë°
ùëú
ùëü
,
ùê∑
ùëí
ùëê
ùëú
ùëë
ùëí
ùëë
ùëÜ
ùëú
ùêπ
ùëé
ùëü
)
. What you can do is a type of beam search (there is research on this), which is basically greedily take the most likely option so far, but keep all the paths in memory while they still have the potential for a likely enough sentence. The probability of a sequence is just the product of all the predictions that we have sampled. Let's say our model has the following properties (we ignore the encoder):

P(A)=0.3
ùëÉ
(
ùê¥
)
=
0.3
, 
P(B)=0.5
ùëÉ
(
ùêµ
)
=
0.5
, 
P(End)=0.2
ùëÉ
(
ùê∏
ùëõ
ùëë
)
=
0.2

Now, condtioned on one sample:

P(A|A)=0.6
ùëÉ
(
ùê¥
|
ùê¥
)
=
0.6
, 
P(B|A)=0.1
ùëÉ
(
ùêµ
|
ùê¥
)
=
0.1
, 
P(End|A)=0.3
ùëÉ
(
ùê∏
ùëõ
ùëë
|
ùê¥
)
=
0.3

P(A|B)=0.7
ùëÉ
(
ùê¥
|
ùêµ
)
=
0.7
, 
P(B|B)=0.2
ùëÉ
(
ùêµ
|
ùêµ
)
=
0.2
, 
P(End|B)=0.1
ùëÉ
(
ùê∏
ùëõ
ùëë
|
ùêµ
)
=
0.1

Now let's say when we have two samples the probability of ending is always 1.

This means we get the following probabilities:

P(End)=0.2
ùëÉ
(
ùê∏
ùëõ
ùëë
)
=
0.2

P(A)=0.3‚àó0.3=0.09
ùëÉ
(
ùê¥
)
=
0.3
‚àó
0.3
=
0.09

P(B)=0.5‚àó0.1=0.05
ùëÉ
(
ùêµ
)
=
0.5
‚àó
0.1
=
0.05

P(AA)=0.3‚àó0.6‚àó1.=0.18
ùëÉ
(
ùê¥
ùê¥
)
=
0.3
‚àó
0.6
‚àó
1.
=
0.18

P(AB)=0.3‚àó0.1‚àó1.=0.03
ùëÉ
(
ùê¥
ùêµ
)
=
0.3
‚àó
0.1
‚àó
1.
=
0.03

P(BA)=0.5‚àó0.7‚àó1.=0.35
ùëÉ
(
ùêµ
ùê¥
)
=
0.5
‚àó
0.7
‚àó
1.
=
0.35

P(BB)=0.5‚àó0.2‚àó1.=0.1
ùëÉ
(
ùêµ
ùêµ
)
=
0.5
‚àó
0.2
‚àó
1.
=
0.1

But we don't want to iterate over all possibilities, this grows way too quickly. What we can do is build a tree which we can bound by our threshold. Let's say we only want samples with at least probability 
0.15
0.15
. Our root node is empty, with probability 1. Our terminal nodes are the ones that have 
End
ùê∏
ùëõ
ùëë
 as the last part of the sequence. We start growing our tree with the most likely scenario, which is 
B
ùêµ
, which means we have 
P(empty)=1
ùëÉ
(
ùëí
ùëö
ùëù
ùë°
ùë¶
)
=
1
, 
P(B)=0.5
ùëÉ
(
ùêµ
)
=
0.5
. For our next option, we can either look at another start than B or by expanding upon B. 
P(A)=0.3
ùëÉ
(
ùê¥
)
=
0.3
 but 
P(BA)=0.35
ùëÉ
(
ùêµ
ùê¥
)
=
0.35
 so we first expand 
B
ùêµ
. We now have 
P(empty)=1
ùëÉ
(
ùëí
ùëö
ùëù
ùë°
ùë¶
)
=
1
, 
P(B)=0.5
ùëÉ
(
ùêµ
)
=
0.5
 and 
P(BA)=0.35
ùëÉ
(
ùêµ
ùê¥
)
=
0.35
. The next step is the highest probability, staying at 0.35 by ending 
P(AB‚àíend)=0.35
ùëÉ
(
ùê¥
ùêµ
‚àí
ùëí
ùëõ
ùëë
)
=
0.35
. The next step is expanding our root by going to 
P(A)=0.3
ùëÉ
(
ùê¥
)
=
0.3
. The next step is expanding root with 
P(End)=0.2
ùëÉ
(
ùê∏
ùëõ
ùëë
)
=
0.2
. Our next step would be 
P(AA)=0.18
ùëÉ
(
ùê¥
ùê¥
)
=
0.18
 but this is below our threshold. We know there are no sequences that are more likely because of the way we have built our tree, which means we have all sequences above the given threshold. You can use something similar to generate the top k most likely samples.

EDIT: Be aware that this does bias towards shorter sentences if your model allows for them, because every probability decreases the total probability. I have not really thought about how much of this is a problem, it also depends a lot on your generating RNN. You could think of some 'penalties' for short ones but you want the metric to be monotonically decreasing because else you cannot bound your tree properly."
Classification of skills based on job ads,"I don't know about RapidMiner, but for a beginner, Orange seems an awesome open-source tool for the job.

Particularly if you want to do unsupervised learning, i.e. clustering. You'd probably want to preprocess your textual data in some bag of words model and save it as CSV, I guess.

I also hear Orange has a text mining add-on, but I haven't tried it yet."
Intent Classification in Question Answering,"I don't know about the granularity of the intents. If they are just Person/Loc/Official, why not use question words(where,who,what etc.) to identify the intent? For example, ""where"" corresponds to ""Loc"", ""Who"" corresponds to ""Person"" etc. You can also use this as a feature in you classifier. Hope it helps!!"
Python stemmer for Georgian,"I don't know any Georgian stemmer or lemmatizer. I think, however, that you have another option: to use unsupervised approaches to segment words into morphemes, and use your linguistic knowledge of Georgian to devise some heuristic rules to identify the stem among them.

This kind of approach consists of a model trained to identify morphemes without any labels (i.e. unsupervisedly). The most relevant Python package for this is Morfessor. You can find its theoretical foundations in these publications: Unsupervised discovery of morphemes; Semi-supervised learning of concatenative morphology.

Also, there is a Python package called Polyglot that offers pre-trained Morfessor models, including one for Georgian. Therefore, my recommendation is for you to use Polyglot's Georgian model to segment words into morphemes and then write some rules by hand to pick the stem among them.

You should be able to evaluate the feasibility of this idea by adapting this example from Polyglot's documentation from English to Georgian (by changing the language code en and the list of words):

from polyglot.text import Text, Word

words = [""preprocessing"", ""processor"", ""invaluable"", ""thankful"", ""crossed""]
for w in words:
  w = Word(w, language=""en"")
  print(""{:<20}{}"".format(w, w.morphemes))"
How to perform Grid Search on NLP CRF model,"I don't know how could we resolve it in sci-kit version 0.24 or later but when I downgraded it to 0.23.2 version, same piece of code seems to be working fine."
Paraphrasing a sentence and changing the tone of it,"I don't know how they do it, but https://www.wordtune.com/#rewrite-demo is the most impressive paraphraser I've seen. Maybe they explain their approach somewhere?"
which open-source frameworks exist for fact-extraction,"I don't know many examples but I'm aware of at least one such tool, specialized for the medical domain: SemRep

SemRep is a UMLS-based program that extracts three-part propositions, called semantic predications, from sentences in biomedical text. Predications consist of a subject argument, an object argument, and the relation that binds them. For example, from the sentence in (1), SemRep extracts the predications in (2).

1.We used hemofiltration to treat a patient with digoxin overdose that was complicated by refractory hyperkalemia.

Hemofiltration-TREATS-Patients, Digoxin overdose-PROCESS_OF-Patients, hyperkalemia-COMPLICATES-Digoxin overdose, Hemofiltration-TREATS(INFER)-Digoxin overdose

In general, this is closely related to the problem of semantic role labeling:

semantic role labeling (also called shallow semantic parsing) is the process that assigns labels to words or phrases in a sentence that indicate their semantic role in the sentence, such as that of an agent, goal, or result.

Apparently there are some implementations available: https://framenet.icsi.berkeley.edu/fndrupal/ASRL"
What explains T5's recent resurgence?,"I don't see FLAN-T5 in the list as well as the other T5 variants, so my guess is that all T5 variants got conflated into T5. Since fine-tuning (eg, FLAN-T5) has become very important in language models, this likely explains T5's recent resurgence according to the plot in the question details."
Method for solving problem with variable number of predictors,"I don't see the problem. All you need is a learner to map a bit string as long as the total number of contestants, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would have a learning-to-rank problem.

If the contestant landscape can change it would help to find a vector space embedding for them so you can use the previous embeddings as an initial guess and rank anyone, even hypothetical, given their vector representation. As the number of users increases the embedding should stabilize and retraining should become less costly. The question is how to find the embedding, of course. If you have a lot of training data, you could probably find a randomized one along with the ranking function. If you don't, you would have to generate the embedding by some algorithm and estimate only the ranking function. I have not faced your problem before so I can't direct you to a particular paper, but the recent NLP literature should give you some inspiration, e.g. this. I still think it is feasible."
NLP Transformers: How to get a fixed sentences embedding vectors size?,"I don't think there will be a definitive answer, but I suspect that you'll get better results using the averaging method rather than the padding method.

One big problem with the padding method is that it's sensitive to word order. For example, the sentences ""Gibbons are one type of ape"" and ""One type of ape is the Gibbon"" look very different if we do a word-by-word comparison. ""Gibbons"" is very different than ""One"", ""are"" is very different than ""type"", etc. This problem will be offset slightly because RoBERTa embeddings are context-sensitive, but you get the idea.

Another big problem is that the padding can dominate the similarity comparison for sentences with different lengths. Suppose we want to compare the following two sentences which have very similar meaning:

Sentence1: ""Very furry"" would be an apt description for most mammalian species.

Sentence2: Mammals have hair.

We would add 8 padding tokens to sentence2 in order to give it the same length as sentence1. But then the similarity computation is dominated by the padding tokens and not by the actual content of the sentence.

To me, the averaging approach seems superior because it avoids these two problems. Of course it also has drawbacks. The ""sentence embeddings"" obtained by averaging the word embeddings will be noisy because each word is given equal weight.

You could try running a keyword detector over the sentences you wish to compare. Then compare the means of the top-k keywords instead of the whole sentence."
Method to assess text credibility,"I don't think there's anything close to doing this:

It would be very hard to even define the task objectively, as different humans wouldn't agree about what is credible or not.
It would require a complex system to represent reliable background knowledge... and again people wouldn't agree what should be considered ""reliable"" or not.
Generally the state of the art in NLP is still far from solving tasks related to Natural Language Understanding in a satisfying way. Juding the credibility of a text requires not only a real understanding of the text but also an ability to reason at a higher level. It's not clear whether this level of AI can ever be reached.

If you find a package which pretends to achieve this task, try to apply it to its own documentation because it's not credible ;)"
Is there a tokenizer to tokenize Swift language code in python,"I don't think this has anything to do with NLP, your approach to treat this code as text is unlikely to work imho.

The first thing to do is to properly define the task. You said that you want the system to return ""only the relevant tokens such as animation, Animation.linear, duration etc."". So what should be defined as a ""relevant token""?

This is a programming language, so normally the language is finite. Maybe you can prepare a list of all the possible relevant tokens? In this case the task becomes simply about searching the subset of ""relevant tokens"" contained in the code. Note that you could also have a list of ""non-relevant tokens"", if more convenient.
If ""relevant"" is based on the semantics of the code, the task is much more difficult. Basically you need to design the task similarly to what a human would do: what would be the clues you would use to decide if a token is relevant or not? How to represent these clues as features?"
Google NLP AutoML,"I don't think you will find details of this resource hanging around - it is trade secret. My bet is Google first trains a huge language model as base, and let users fine-tune it with their own data, just like any usual transfer learning.

As for how they train the base model, the closest thing I find is the Pathways Language Model PaLM, developed around 2021, which is described as 'a 540-billion parameter, densely activated, Transformer language model', trained on '6144 TPU v4 chips'. Though Google may be keeping more dark magics, PaLM is already the state of the art."
The reason behind using a pre-trained model?,"I don't understand how a pre-trained model can adapt to my given corpus

You are correct in thinking this way. It is not a magic wand. It learns the embedding values based on the underlying context of the corpus(e.g. news) which may work in the broad sense but not in a specific case.
Two cities may get the embeddings based on their geographical location but that might not be the embedding we may like if we are comparing cities on crime rate/GDP etc.

One of the best posts on Word Embedding [Blog post by Sebastian Ruder] has mentioned all the limitations in detail. An excerpt from the post,

""...One of the major downsides of using pre-trained embeddings is that the news data used for training them is often very different from the data on which we would like to use them. In most cases, however, we do not have access to millions of unlabelled documents in our target domain that would allow for pre-training good embeddings from scratch....""

If I have new words not present in the pre-trained model will I be able to use this pre-trained model to learn the embeddings for the new words?

It will get a value from a bucket for all the out-of-vocabulary(OOV) list. Sometimes it can be the same value for all the new words. But there is some known strategy to deal with the scenario. Check the post for relevant papers.
From the same posts,

""....One of the main problems of using pre-trained word embeddings is that they are unable to deal with out-of-vocabulary (OOV) words, i.e. words that have not been seen during training. Typically, such words are set to the UNK token and are assigned the same vector, which is an ineffective choice if the number of OOV words is large....""

But this issue will prevail even if you learn embeddings with your own corpus. So I believe it is not just limited to the pre-trained embeddings."
Comparing soccer teams name,"I don‚Äôt know why your question was voted down! Fuzzy matching is such a common challenge. The best approach I‚Äôve seen is this one: https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536 It gives similar results to something like Levenshtein distance but it‚Äôs much faster. If you augment the matching approach with hand-coded regex features to spot e.g. female versus male teams, you should be able to match teams pretty well."
Confidence Intervals for Multi-Categorical Votes,"I found a concise description of how to do this.

(But note that this doesn't correct for sentence length, so it's not a complete solution bcs longer sentence will have more ""votes"".)"
Feature selection or Dimension reduction in unsupervised learning,"I found Auto-encoders to be the best solution for this. Performing auto encoder before clustering reduces the dimensionality of the high dimensional data and then the encoder results can be extracted and used for whatever method we want to implement like,

supervised classification unsupervised clustering etc.

Autoencoders using CNN (encoder and decoder) for image: https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial#comment-6125

Autoencoders - its types and usage - LSTM: https://machinelearningmastery.com/lstm-autoencoders/

for my usecase I should be performing an embedding before feeding it into the autoencoder (CNN/LSTM) with only the encoder so that the reduced dimensions can be used for kmeans clustering.

Hope this helps people who have same question, I wonder how none has come across this situation or haven't replied for this.

Thanks Arav"
Using Subsequent Mask in Transformer Leads to NaN Outputs,"I found out what the problem was. It was not from subsequent mask. It was caused by bad key_padding_mask. PyTorch expects the key_padding_mask to be true wherever there is a padding token and false wherever there is none. I was generating the padding mask in exactly the opposite way. So the correct mask will be:

def generate_padding_mask(self, seq, pad_idx):
    return (seq == pad_idx).to(self.device)"
"Machine learning or NLP approach to convert string about month ,year into dates","I got my answer , NLTK is good to go for this problem. You may use sutime with python wrapper :

Python wrapper for Stanford CoreNLP's SUTime

The usual approach in NLP is to collect a dataset required for training. Process that dataset so that the words in the dataset are converted into numbers.

One simple example of converting it into numbers is to make a large dictionary of words from the dataset and use the index of each word in the dictionary as the representing number"
A Derivation in Combinatory Categorial Grammer,"I guess I got it. It is simply:

(X/Y)(Z\(X/Y)) which Z. 


In the example,

X = VP/PP, Y = NP, and Z = VP."
Feature extraction from the text,I guess if you know what you want to extract you can just find it using regular expressions for integers or car specifications.
Where can I find dataset for word analogy task?,I guess that you want a word analogy dataset for the english language to test your word embeddings. Check the following link where you can find Mikolov's dataset Word Analogy Dataset.
Fuzzy name and nickname match,"I had a similar problem in my last job. My solution was to build features via (transformation(s) + comparison) * many combos and feed to models, then aggregate and model, i.e. 2 layer model. The key is encoding and similarity scores as features.

Transforms: remove vowels (great for certain roots), remove end vowels, remove double characters, convert to phonetic string (IPA, soundex, https://pypi.org/project/Fuzzy/), replace characters that either sound similar or have different sounds in other languages (
J
ùêΩ
 in East Europe sounds like 
Y
ùëå
 in US, 
C
ùê∂
 can sound like 
K,D¬†T,T‚àºTH
ùêæ
,
ùê∑
¬†
ùëá
,
ùëá
‚àº
ùëá
ùêª
, etc), ... The strategy is to handle lots of weirdness/irregularity in people's names.

Comparisons (similarity and difference): try [character level, block/root/[pre/suf]fix level, word level (may not apply to you)] similarity and difference scores. Try Dice's coefficient, Levenshtein, Needleman‚ÄìWunsch, Longest common (non)contiguous substring, character histogram similarity, # characters matching, not matching (each left and right), etc. You could try using an RNN/LSTM and have it learn similarity for each transform. Use the output of the trained model(s) as another feature.

Experiment with different combos of the above and select a few that seem to have value. You could simply take all the scores and fit with Logistic Regression (or Neural Net) or you could build statistical models and output percent rank based on a small training set to normalize it. Another way to preprocess the raw scores is to use calibration encoding via logistic function. Then add summary stats from the normalized scores as additional features. Push all this into the final model.

Will you handle names that are derived from Arabic, Spanish, French, etc names? This is just extra, but consider downloading the Social Security and US Census name stats data to enhance your project with more name variations. I'll leave the how to you, but it helps knowing about the likely possibilities. Be aware that simply using Levenshtein doesn't work so well with William->Bill, Dianne->Di, Larry->Lawrence, Mohammed->Muhamed and Hamed, Danielle->Daniela, Thomas->Tom, and Jimmy->James. The strategy I mentioned should help you with all the variation.

Additional resources to explore: https://github.com/jamesturk/jellyfish https://nameberry.com/list/276/If-You-Like-Danielle-You-Might-Love https://pypi.org/project/phonetics/"
Creating pronunciation dictionary for ASR,"I had posted a similar question in reddit as well and I got a response from Nikolay Shmyrev :

""If you want to convert latin script, you can write simple rules yourself. Something like this. Or you can use epitran as is. ""

Thanks to Nikolay Shmyrev who originally answered in reddit."
BERT has a non deterministic behaviour,"I had the exact same problem using the pytorch implementation, until I realised I didn't set the model in eval mode. Hence, dropout was still activated. I guess this is also the source of your undeterministic behavior.

Fix using pytorch:

model.eval()


Using tensorflow with an Estimator, ensure you call estimator.evaluate or estimator.predict during testing. If you exported the model in some ways, then you should check the tensorflow documentation."
"what is the reason behind the bad outputs gained by RNN, LSTM when using GloVe pretrained model in text classification?","I have already dealt with RNN and LSTM codes and finally I found tow solutons to achieve better result for accuracy measure which are as follows: at first I change the activation function, optimizers and learning rate but it is worth noting that activation function has the most impact and the second trick which I have used is to removing stop words, now the code works fine. I hope it is useful."
Datasets in NLP research papers,"I have come across such a dataset in CS 20SI GitHub repo: it's a collection of abstracts from 7200 research papers.

If you need even more, you can always write a simple crawler of arXiv web-site. The abstracts are outlined right on the webpage in a <blockquote> tag, no need to download and parse a pdf."
prepare email text for nlp (sentiment analysis),"I have done something similar in the past. I'll sketch an outline for you.

First you break the text into paragraphs and tokenize. Then write some regexp rules to capture the data you want to remove. For instance, if an email signature commonly contains a phone number, paragraph, and a website, you can count those features and flag it based on some threshold you decide.

Next, do likewise with the other features you mentioned. My experience is it's highly domain dependent so you really need to look at the data and use your best judgement.

The result of this process should be a data consisting of tokenized paragraphs where the paragraph has been labeled 'noise' or 'clean' based on the feature count.

From there, convert your token representation using tf-idf or another type of embedding. You should be able to use this as input to your favorite classifier, and I have had success using SVMs to that end.

The result is going to be biased towards your rules but you are also leveraging features that are in the labeled examples that are not explicitly in the rules. Particularly so for longer paragraphs.

It might seem a bit a bit janky but believe it or not it works."
A tool like Matlab for NLP?,"I have just finished my Ph.D. and have used some NLP in it. My university didn't offer any NLP courses. So I ended up teaching myself NLP. I used this book.

Which serves as a great introduction to NLP using NLTK (Natural Language Tool Kit). It gives a good introduction into programming with Python. So handy if you've never programmed in Python before too.

I would highly suggest using nltk from nltk.org (sorry can't post more than two links)

The book I used is now out of date as NLTK is now on version 3.0, the book mentioned previously is for NLTK 2.x. But the Authors are working on a new version of the book for NLTK 3.X, you can view the unfinished book here.

I would highly suggest using NLTK and if you're new to natural language processing. I would highly suggest you try and get yourself a copy of the following book:

Foundations of Statistical Natural Language Processing by Manning and Shutze

Even though it doesn't contain any code, it servers a great introduction to natural language processing."
How to implement Brown Clustering Algorithm in O(|V|k^2),"I have managed to resolve this. There is an excellent and thorough explanation of the optimization steps in the following thesis: Semi-Supervised Learning for Natural Language by Percy Liang.

My mistake was trying to update the quality for all potential clusters pairs. Instead, you should initialize a table with the quality changes of doing each merge. Use this table to find the best merge, and the update the relevant terms that make up the table entries."
NLP SBert (Bert) for answer comparison STS,"I have used Siamese Bert and I can say it does a pretty good job. However, the issue is that the data that it has been fine-tuned atop of Bert may not necessarily, entirely represent the same semantic distance as with the answers between the true and the student's one. For instance, if there is a question about engineering, where a small change of word may mean a totally different thing; SBert would still find them quite similar cause they are related to the topic. Unless it's fine-tuned.

Moreover, you will not be able to interpret the similarity. Should a student ask you why my peer's answer is better you won't be able to explain.

My opinion: I believe you could use this tool as a way to reduce totally incoherent answers, but at some point, human evaluation will be needed. And maybe use interpretable metrics such as ROUGE or BLEU. I aware as well, that this topic is quite trendy in NLP, I won't be surprised if there is or will be good off the shelf tool for that, but I am not aware of one currently."
How does PV-DBOW (doc2vec) work?,"I haven't read the paper you linked, but I have followed a lecture notes by Richard Socher.

So, basically that mapping matrix is called weight matrices. There are two weight matrices W1 and W2 for input and output mapping. Thus for each word, vectors in both the matrices are updated via backpropagation.

To answer your question, a word is represented by one-hot sparse vector which has the size of Vx1 (V is size of vocabulary), with a value of 1 in one of the position. and when this vector is multiplied with the weight matrix W1 with size NxV, the corresponding embedding vector of size Nx1 (N is size of the required embedding vector) is used in the hidden layer.

Document has no one-hot represented vectors. So, basically there is a document matrix D of size Nxd (d is number of documents) where each column represents a document. In other words, the matrices W1 and W2 need the one-hot representation only for mathematical steps, other than that they are representing each word in each column just as the document matrix D."
Is there a way to cluster words based on how similarly they sound?,"I haven't tried it myself, but you could try the IPA (International Phonetic Alphabet) version of your words and then calculate the Levenshtein distance.

There is a Python library called panphon. I have no affiliate with it. Just found it with Google Search.

import panphon.distance
dst = panphon.distance.Distance()
dst.dogol_prime_distance(u'pops', u'bobz')
>> 0
dst.dogol_prime_distance(u'father', u'mother')
>> 1
dst.dogol_prime_distance(u'school', u'fool')
>> 3"
"How Exactly Does In-Context Few-Shot Learning Actually Work in Theory (Under the Hood), Despite only Having a ""Few"" Support Examples to ""Train On""?","I highly recommend you read Microsoft's recent paper about In Context Learning. Although the focus is on LLM I think it can be generalised to other models.

The idea is to consider models as mesa|meta-optimisers (optimisers at inference time).

They approximately show that the model performs implicit gradient descent (and thus implicit fine-tuning) at inference time. Obviously, the gradient descent doesn't modify the models' weights, but it modifies the attention mechanism (as would fine-tuning by modifying the weights)."
How to compute unseen bi-grams in a corpus (for Good-Turing Smoothing),"I just remembered that we create a table with all possible words as the header of each row and of each column. As a result, the list of all bi-grams would be all possible bi-grams formed by concatenating any 2 words."
How to perform entity level train-val-test split for NER task?,"I know it's a bit late, but I had the same question and developed a method which is available here:

!pip install deep_utils
from deep_utils import stratify_train_test_split_multi_label


The code is as follows:

from typing import Union
from deep_utils.utils.algorithm_utils.main import subset_sum
import numpy as np


def stratify_train_test_split_multi_label(x: Union[list, tuple, np.ndarray], y: np.ndarray, test_size=0.2,
                                          closest_ratio=False):
    """"""
        A handy function for splitting multi-label samples based on their number of classes. This is mainly useful for
    object detection and ner-like tasks that each sample may contain several objects/tags from different classes! The
    process of splitting starts from classes with the smallest number of samples to make sure their ratio is saved
    because they have small numbers of samples, retaining the ratio for them is challenging compared to those classes
    with more samples
    :param x: A list, Tuple or ndarray that contains the samples
    :param y: A 2D array that represents the number of labels in each class. Each column is representative of a class.
    As an example: y = np.array([[2, 3], [1, 1]]) says that sample one has
    two objects/tags for class 0 and 3 objects/tags for class 1 and so on
    :param test_size: size of the test set
    :param closest_ratio: For huge arrays extracting the closest ratio requires an intensive recursive function to work
     which could result in maximum recursion error. Being set to True will choose samples from the those with the smallest difference to the target number to ensure the best ratio. Set this variable to True if you are sure. by default is set to False.
    :return:
    >>> y = np.array([[1, 2, 0], [1, 0, 0], [1, 2, 0]])
    >>> x = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])
    >>> stratify_train_test_split_multi_label(x, y, test_size=0.3)
    (array([[2, 2, 2],
           [3, 3, 3]]), array([[1, 1, 1]]), array([[1, 0, 0],
           [1, 2, 0]]), array([[1, 2, 0]]))
    >>> x = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
    >>> y = np.array([[0, 0], [0, 0], [0, 1], [0, 1], [1, 1], [1, 1], [1, 0], [1, 0]])
    >>> x_train, x_test, y_train, y_test = stratify_train_test_split_multi_label(x, y, test_size=0.5, closest_ratio=False)
    >>> x_train
    array([[1, 2],
           [3, 4],
           [1, 2],
           [3, 4]])
    >>> x_test
    array([[1, 2],
           [3, 4],
           [1, 2],
           [3, 4]])
    >>> y_train
    array([[0, 1],
           [0, 1],
           [1, 0],
           [1, 0]])
    >>> y_test
    array([[1, 1],
           [1, 1],
           [0, 0],
           [0, 0]])
    >>> print(""class ratio:"", tuple(y_test.sum(0) / y.sum(0)))
    class ratio: (0.5, 0.5)
    >>> print(""sample ratio:"", y_test.shape[0] / y.shape[0])
    sample ratio: 0.5
    """"""
    assert len(y.shape) == 2, ""y should be 2D""
    assert test_size > 0.0, ""test_size cannot be a zero or negative value!""
    x = np.array(x, dtype=np.object_) if not isinstance(x, np.ndarray) else x

    # excluding samples with no objects/tags
    non_objects = np.any(y.sum(1) == 0)
    if non_objects:
        y_no_objects = y[y.sum(1) == 0]
        x_no_objects = x[y.sum(1) == 0]
        x = x[y.sum(1) > 0]
        y = y[y.sum(1) > 0]

    available_samples = np.ones((y.shape[0]), dtype=np.bool8)
    test_samples = np.zeros((y.shape[0]), dtype=np.bool8)
    train_samples = np.zeros((y.shape[0]), dtype=np.bool8)
    class_sample_counts = y.sum(axis=0)
    ideal_train_size = np.floor(sum(class_sample_counts) * (1 - test_size))

    # stratify starts from a class with the lowest number of samples
    class_indices = np.argsort(class_sample_counts)
    for class_index in class_indices:
        test_number_samples = y[:, class_index][test_samples].sum()
        n_test = np.ceil(class_sample_counts[class_index] * test_size)
        n_test = max(0, n_test - test_number_samples)
        input_labels = y[:, class_index].copy()
        input_labels[np.invert(available_samples)] = 0
        if n_test == 0 or len(input_labels) == 0:
            continue
        if closest_ratio:
            chosen_indices, *_ = subset_sum(input_numbers=input_labels, target_number=n_test)
        else:
            sorted_indices = np.argsort(input_labels)
            cum_sum_values = np.cumsum(input_labels[sorted_indices])
            chosen_indices = sorted_indices[cum_sum_values < n_test].tolist()
            if len(chosen_indices) < len(sorted_indices):
                chosen_indices.append(sorted_indices[len(chosen_indices)])
        # Update available_samples, train_samples, test_samples
        for update_index, n_label in enumerate(input_labels):
            if n_label == 0:
                # samples that have no elements are ignored ...
                continue
            if update_index in chosen_indices:
                test_samples[update_index] = True
                train_samples[update_index] = False
            else:
                test_samples[update_index] = False
                train_samples[update_index] = True
            available_samples[update_index] = False
    # Allocating all the remaining samples to train because the code structure ensures the ratio of test
    # samples to the whole dataset.
    train_samples = np.bitwise_or(train_samples, np.bitwise_not(test_samples))

    if non_objects:
        # splitting samples with no objects trying to save the balance between train and test numbers

        train_left = int(ideal_train_size - sum(train_samples))
        indices = np.arange(len(y_no_objects))
        np.random.shuffle(indices)

        x_no_objects_train, y_no_objects_train = x_no_objects[:train_left], y_no_objects[:train_left]
        x_no_objects_test, y_no_objects_test = x_no_objects[train_left:], y_no_objects[train_left:]

        return np.concatenate([x[train_samples], x_no_objects_train]), \
               np.concatenate([x[test_samples], x_no_objects_test]), \
               np.concatenate([y[train_samples], y_no_objects_train]), \
               np.concatenate([y[test_samples], y_no_objects_test])
    else:
        return x[train_samples], x[test_samples], y[train_samples], y[test_samples]



Two examples are provided in the code's description.

Link to stratify_train_test_split_multi_label code

Link to deep_utils library"
Needed: Java library to calculate text readability/complexity,"I looked for this as well and found only the unmaintained RAT library.

But basically all of these algorithms (most are described as formulas in this paper) can be directly translated from the single Python file with no extra dependencies.

One exception was the Linsear Write formula because of its needing a syllable counting method. But if you use the CMU pronunciation dictionary or equivalent, you can easily build the countSyllables method and the rest of the algorithm is trivial."
How to justify the usage of 200 dimensions in word vectors instead of the 300 dimensions?,I recommend this paper. The authors treat the size of embeddings as a hyperparameter and provide a detailed study on it. They show that this dimensionality should depend on the corpus.
NLP text representation techniques that preserve word order in sentence?,"I recommend working with parts of speach (POS), more specifically with the RDF-Triple of Subject, Predicate and Object.

It both acts as the major structure of the sentence and preserves the order (i.e. the Subject Predicates upon the Object).

See if you can go with that alone. If not, you can add to it from the techniques you mentioned (bagging, tf-idf, etc..).

See my answer here for a suggested combined tf-idf score upon an rdf-triple, to check whether the triple itself is ""unique-enough""."
Framing Sentences based on keywords,"I resolved the issue by making use of the Natural Language Generation, Python Library , nlglib

The solution I came up with is depicted through the Python Code below.

import nlglib
realise_en = Realiser(host='nlg.kutlak.info', port=40000)

p = Clause(NP('jackets'), VP('achieve','highest sales'))
p['TENSE'] = 'PAST'
q = Clause(NP('sweaters'), VP('exhibit','lowest sales'))
q['TENSE'] = 'PAST'
print(realise_en(p))
print(realise_en(q))
r=Clause(realise_en(p)[:-1],'while',realise_en(q))
print(r)


This prints the statements

Jackets achieved highest sales.
Sweaters exhibited lowest sales.
Jackets achieved highest sales while Sweaters exhibited lowest sales.


This is the best solution I found so far."
"Configure BRAT so that if no `.ann` file is found, then an empty `.ann` file is created","I run into the same problem with BRAT from time to time. The way I solved it for a folder with .txt files is to run this Bash one-liner directly in the shell when inside the directory with these files:

for f in *.txt; do touch $(basename $f .txt)"".ann""; done


What is does is the following: With a for-loop, it loops over every .txt file, and creates an empty file with touch having the same basename, but a .ann extension instead of a .txt extension."
Extracting NER from a Spanish language text file,I suggest you take a look at the Python library Spacy it has a model for spanish language that includes NER.
classification of similar text input features with text output label,"I suggest you use the state of the art for this kind of problems: a BERT-based approach. This kind of approach is well documented and very accessible, given the large amount of examples available online.

The approach consists of taking a pre-trained neural network model from the BERT family (Transformer encoders normally trained on a masked language model task over a large dataset), and fine-tuning it on your data.

This would allow you to profit from:

BERT's subword vocabulary, which will avoid having out-of-vocabulary words, because it decomposes words into smaller fragments.
The power of transfer learning, which would mitigate the situations where you don't have a lot of data.
State of the art performance. You can check at paperswithcode that BERT-based approaches are frequent top-performers in standard text classification benchmarks.

One of the most well documented and maintained python libraries for this is Hugginface Transformers. You can have a look at some examples on how to do text classification on a custom dataset here.

About the computational resources to train the system, you may use your own GPUs if you have, you may also train on CPU (which may be feasible given your short sequence lengths), or you may use Google Colab, which is very handy if you don't need a lot of training time."
Grouping domain specific words/phrases with same meaning,"I suggest you use word2vec for that task. Word2vec is an unsupervised algorithm that calculates N-dimension embeddings for the words in the corpus used for learning. Basically, it gives you a numerical representation in the form of an n-dimension array for each of the words you use in your inputs.

Once the model is built and the emeddings are available, to find similar words is as easy as calculate the similarity between arrays. This is done in word2vec either by looking for the most similar:

>>>result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])
>>>print(""{}: {:.4f}"".format(*result[0]))
queen: 0.7699


or directly comparing terms:

>>> result = word_vectors.similar_by_word(""cat"")
>>> print(""{}: {:.4f}"".format(*result[0]))
dog: 0.8798


Have a look here for more examples.

To answer this :

I want to do this for texts that contain domain specific and colloquial jargon so existing NLP solutions may not be suitable?

If you train the model with your own domain specific corpus, you should't have any problems; provided of course that it is long and rich enough."
NLP data cleaning and word tokenizing,"I summarize your questions and then try to answer under each bullet point:

How to remove punctuation marks (e.g. # for hashtags which is used in social media)

The first goto is a regular expression that is used in data preprocessing very frequently. But if you are looking for all your punctuation to be removed from the text you can use one of these two approaches:

import string
sentence = ""hi; how*, @are^ you? wow!!!""
sentence = sentence.translate(sentence.maketrans('', '', string.punctuation))
print(sentence)


output: 'hi how are you wow'

or use a regular expression, example:

import re
s = ""string. With. Punctuation? 3.2""
s = re.sub(r'[^\w\s]','',s)
print(s)


output: 'string With Punctuation 32'

What are the possible python library for data cleaning?

generally, NLTK and SciPy are very handly. But for specific purposes, other libraries also exist. For example library contractions, inflect.

Which libraries are efficient in data cleaning when using pandas dataframes?

You can apply any function from any library to your pandas dataframe using the apply method. Here is an example:

mport pandas as pd 
s = pd.read_csv(""stock.csv"", squeeze = True) 
  
# adding 5 to each value 
new = s.apply(lambda num : num + 5) 


code source"
Encode a set of skills into a feature,"I think ""one hot"" is the obvious thing to do. 500 features is usually not a problem (if you don‚Äòt have too few observations). In any case you could look into ""shrinking"" features by using Lasso/Ridge for instance.

Probably you could also look into dimensionality reduction, e.g. by using principle components (PCA).

You could also do some feature selection in the sense that you ""kick out"" features (skills) which to not have much predictive power or which are redundant. You could for instance check for (very) high correlation among skills or remove skills with little importance after fitting some random forest."
How to approach TF-IDf based analysis?,"I think here you must maintain the actual tf-idf and create corpus over it.. Assuming you already have lables for documents available. You can rum classification over it.

Best classification I am anticipating for this problem would be naive bayes.."
Does spaCy support multiple GPUs?,"I think I have figured out how to do this:

The key is to use spawn not fork, and use cupy to select GPU.

import multiprocessing as mp
mp.set_start_method('spawn', force=True)
from joblib import Parallel, delayed
from itertools import cycle
import cupy
import spacy
from thinc.api import set_gpu_allocator, require_gpu


def chunker(iterable, total_length, chunksize):
    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))

def flatten(list_of_lists):
    ""Flatten a list of lists to a combined list""
    return [item for sublist in list_of_lists for item in sublist]

def process_entity(doc):
    super_word_ls = []
    for s in doc.sents:
        word_ls = []
        for t in s:
            if not t.ent_type_:
                if (t.text.strip()!=""""):
                    word_ls.append(t.text)
            else:
                word_ls.append(t.ent_type_)
        if len(word_ls)>0:
            super_word_ls.append("" "".join(word_ls))
    return "" "".join(super_word_ls)

def process_chunk(texts, rank):
    print(rank)
    with cupy.cuda.Device(rank):
        set_gpu_allocator(""pytorch"")
        require_gpu(rank)
        nlp = spacy.load(""en_core_web_trf"")
        preproc_pipe = []
        for doc in nlp.pipe(texts, batch_size=20):
            preproc_pipe.append(process_entity(doc))
        rank+=1
        return preproc_pipe


def preprocess_parallel(texts, chunksize=100):
    executor = Parallel(n_jobs=2, backend='multiprocessing', prefer=""processes"")
    do = delayed(process_chunk)
    tasks = []
    gpus = list(range(0, cupy.cuda.runtime.getDeviceCount()))
    rank = 0
    for chunk in chunker(texts, len(texts), chunksize=chunksize):
        tasks.append(do(chunk, rank))
        rank = (rank+1)%len(gpus)
    result = executor(tasks)
    return flatten(result)

if __name__ == '__main__':
    print(preprocess_parallel(texts = [""His friend Nicolas J. Smith is here with Bart Simpon and Fred.""]*100, chunksize=50))"
Word2Vec: Using pre-trained models,"I think in a lot of cases, when people are using pre-trained models, they force their words to lower and singular case and then test to see if the lowercase and singular (not plural) version were trained. Sometimes, you have to check yourself like A.M. Bittlingmayer said.

To provide some insight into why sometimes people remove stopwords versus not --- I think it has to do with whether they want to value infrequent words more or less. Because you don't want to value too highly very frequent words, the algorithm 'punishes' extremely frequent words. So, one could argue that because this already happens, you don't need to remove stopwords since they will be 'punished' anyways. So, not everyone removes stopwords."
Is there a reference dataset for contextual similarity?,"I think some datasets used for word sense disambiguation (WSD) would be an option.

WSD is the task of classifying an ambiguous word into its correct meaning. For instance ""apple"" would have meaning 1 the fruit and meaning 2 the tech company. As a consequence a labelled dataset identifies the correct context for the meaning.

I don't know any specific dataset but I assume that state of the art papers mention and use these datasets."
Advice on movie per topic classification and relation with rating,"I think that this experiment makes some sense, keeping in mind that:

The dialogues (subtitles) are probably a decent indicator for the main topic of the movie, but not a perfect one.
Similarly, there might be some correlation between dialogues/topics and ratings, but obviously the ratings don't only depend on the topic (or dialogues).

Note: in case the goal is to see the relation between dialogue and rating, you could also consider training a supervised regression model which predicts ratings directly from the subtitles text.

Topic modelling calculates a distribution of topics for every movie, so in general you can have a movie which belongs to different topics at varying degrees. Of course, it's common to assign a single topic to an instance (here a movie) by taking the maximum probability topic. If you do this, you can easily obtain the set of movies for every topic, and then calculate any statistic related to rating for the topic, for example the mean rating for this topic.

In terms of visualization, a common way to represent a topic is with the set of N words which are most strongly associated with it. You could do a word clouds with these. You can also represent the histograms of the ratings distributions by cluster."
NLP - Identify Tagged Words,"I think the aspect of NLP you're looking for is Named Entity Recognition (NER). It's already built into Python libraries like NLTK and SpaCy. Some videos that might be of interest:

How to train a new 'DRUGS' entity type: https://prodi.gy/docs/video-new-entity-type Practical example: https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da

The great thing for you is that out-of-the-box models are probably going to be extremely accurate in detecting place names (either LOC or GPE entity tags). They may even be able to detect drugs as a 'PRODUCT' tag. If you need to be more specific, the first video shows you how you would go about training a model for a new tag. For instance, in spaCy:

import spacy
nlp = spacy.load(""en_core_web_lg"")
doc = nlp(""""""When I lived in Paris last year, France was experiencing arecession. The nightlife was too fun, I developed an addiction to Adderall and Ritalin."""""")
print([(e.text, e.label_) for e in doc.ents])


will print something like this: [('Paris', 'GPE'), ('last year', 'DATE'), ('France', 'GPE'), ('Adderall', 'PRODUCT'), ('Ritalin', 'PRODUCT')]"
Extracting events with attributes from unstructured text,"I think the closest standard NLP task would be relationship extraction. In general it's a quite complex task which involves NER, syntactic analysis and semantic role labeling.

Note that there are various works using the term ""event extraction"" (for example this), but as far as I know there is no clear definition of the task. It's often related to putting events on a timeline, this would be quite different from your goal but possibly related.

A basic approach would be to treat the problem as a sequence labeling task like NER: given some annotated ""events"" in a training corpus, the model might be able to learn the patterns and detect any new ""event"" in a text."
Algorithms to get intent of an article?,"I think the easiest way would be to cluster them with wordvectors. The ""20 Newspaper"" dataset would be a good test for the algorithm http://qwone.com/~jason/20Newsgroups/

On the Sklearn Website, you will find some easy example for using wordvectors and classification: http://scikit-learn.org/stable/datasets/twenty_newsgroups.html

If you go deeper into this space you could start using LSTM networks and neural networks, but this is a little bit harder to develop. So first, try to use the problem statistically. If you want to analyze the text semantically you will need LSTM.

I hope I could help you."
nlp - opinion mining vs sentiment analysis,"I think the key is that most Recurrent Neural Networks problems are formulated in terms of either a regression (with low values indicating negative sentiment, and high values positive) or a binary classification (is this text positive?).

What you seem to be interested in is a much more nuanced definition of sentiment. This doesn't present any inherent problem, as the same algorithms might well work to predict more complex sentiments. The issue is simply labeled data. Because this kind of classification is difficult even for humans, it isn't easy to reliably gather data on, say, how stressed a writer is.

However if you're interested in assembling a dataset of that nature, you'd be able to apply the same methods (Recurrent Neural Networks are a popular option) to do the classification. Many researchers in the field use Amazon Mechanical Turk or something similar to gather labeled data at a reasonable cost."
how to run bert's pretrained model word embeddings faster?,"I think the main problem is how you are using BERT, as you are processing your text sentence by sentence. Instead, you should be feeding the input to the model in mini-batches:

Neural networks for NLP are meant to receive not only one sentence at a time, but multiple sentences. The sentences are stacked together in a single tensor of integer numbers (token IDs) with dimensions number of sentences 
√ó
√ó
 sequence length. As sentences have different lengths, normally the batch has as sequence length the length of the longest sequence in it, and all the sentences that are shorter than that, are filled with padding tokens. You can have a look at batching in huggingface's library here.

Using batch-oriented processing would allow you to profit from parallel processing of all the sentences in the batch.

The problem is that, while HuggingFace Transformer supports batch-oriented for training, it does not support batch-oriented inference in some cases. For instance, FeatureExtractionPipeline, which extracts token embeddings like you want, does not support batch processing (unlike TableQuestionAnsweringPipeline which has the sequential parameter).

This way, in order to have batch-oriented inference you would need to feed the data manually instead of relying on the pipeline API. You can find examples on how to do that in this thread.

If you try using GPU instead of CPU, using batch-oriented processing would also be key to enable performance gains.

If you decide to stay on CPU, ensure that your Pytorch build is using MKL-DNN, which is a major performance booster in CPU. You can check this thread on how to do it. If you are not using it, install a newer version that includes it."
What is the best way to split a sentence for a keyword extraction task?,"I think the tagging approach has some merit here. The frequency drop you're observing as a consequence of using this is to be expected, I think. After all, keywords are the words that help differentiate a document from others in a corpus. If you have access to an ontology related to the subject matter of your corpus, you could try to map your rare keyword tags to the ontology, and use parent-level information for each to get a slightly more general set of keyword tags! If this is an approach that interests you, Stanford's open source prot√©g√© system is a good framework for working on this."
Evaluating Language Model on specific topic,"I think there are (at least) two parts to take into account in evaluating such a model:

Whether the generated text correctly relate to the input topic
Whether the generated text is grammatically and semantically acceptable

In my opinion the first kind of evaluation could reasonably be done with an automatic method such as the one you propose. Note that cosine scores should not be interpreted absolutely: you should probably compute cosine similarity with a random sample of topics, and normally one expects the similarity to be much higher with the input topic than any other. You could also think of other variants, for instance training topic models on the generated text together with a sample of documents from various known topics, then check that the generated text belongs to the target topic (i.e. it should be grouped with the documents known to belong to this topic).

For the second kind of evaluation, it would be difficult and unreliable to use an automatic method. As far as I know the only reliable way would be to ask human annotators to assess whether the text is grammatically correct and whether its content makes sense. If you're going to do that you might as well ask them to annotate to what extent the text is related to the topic.

[added following comment]

if you check whether the generated text is similar to the topic only by computing similarity with this target topic, what you obtain is for instance an average cosine score. Then you would probably select a threshold: for instance if the similarity is higher than 0.5 then consider that the text is indeed related to the topic. But there are two problems with this option:

In some cases the average similarity will be lower than the threshold even though the text is correctly related to the topic. This could happen for example with a very ""broad"" topic which covers a large vocabulary.
On the contrary you might have cases where the average similarity is higher than the threshold, but actually comparing to another topic would give an even higher similarity value.

These issues are due to interpreting the similarity score ""absolutely"", as opposed to interpreting it relatively to other similarity scores. Instead you can calculate the similarity not only against the target topic but also against other topics, and then just check that the target topic is the most similar topic (or at least one of the top similar). This way :

The target similarity score may be low, as long as it's higher than the other topics
you can detect the case where another topic happens to have higher similarity than the target topic"
How can I convert text data to CoNLL format?,"I think there's a bit of confusion here: the sample you're showing is not a full ""conll"" format, at least not any recent one. It's simply a BIO format for NER.

As far as I know conllu has been the standard ""conll"" format for probably at least 10 years, so if you're using some old data it's possible that it used the name ""conll"" for something different. Or maybe somebody just used the name ""conll"" because the data was related to conll even though the format is not a standard conll format. Btw the library you link is for parsing the conllu format, not for generating it.

Normally The conllu format includes several columns for every token: at least token, lemma, POS and usually some dependency tree information (index of the head of the dependency). It is used in particular by the Universal Dependencies project.

So as far as I know there's no particular conll standard in this format. But it's a pretty simple conversion that can be implemented manually: you can simply iterate over the words and the tags in parallel and print the token and BIO tag as columns. The only posssible issue is whether the words are already tokenized or not."
averaging multiple scores on small chunks of data or raw score on single collated data,"I think this is 'mood' vs. 'character'. And as such, it depends on the question you ask. E.g. did I make my customer upset? Or is my customer loyal?

Though mood and character correlate, as someone with a certain character will be more likely to be in some mood, there is also variation.

I think you will measure 'mood' with the 'tone' analyzer, and 'character' with the 'personality' analyzer.

For the 'personality' you will need all the data you can get to get the 'average', for the 'mood', a single data point can be sufficient."
Word taxonomies for Facebook likes categories,"I think total number of default categories in Facebook is somewhat static. There is around 140-160 categories.

ref: https://stackoverflow.com/questions/4216648/facebook-pages-authoritative-list-of-categories/8576572#8576572 and http://www.marketinggum.com/types-of-facebook-pages-for-business/

You can cluster (manually) them in to 10-12 generic groups.

eg: Sports Event, Sports League, Amateur sports team, Sports Venue, Athlete, Sports/Recreation/Activities etc all belongs to Sports category.

Similarly cluster and form generic groups.

And then you can assign a user to any of these generic group based on the like category."
How to extract entities from text using existing ontologies?,"I think you are looking for Spacy, Polyglot and AllenNLP to find your NERs."
Doc2vec to calculate cosine similarity - absolutely inaccurate,"I think you are missing the model.infer_vector(new_sentence). You need to infer the new vectors based on your trained model. You can find more details in Assessing the Model section here.

The similarity is between vectors but not the normalised tokens. So, you have to infer them first using your model."
How word embedding work for word similarity?,"I think you have it mostly correct.

Word embeddings can be summed up by: A word is known by the company it keeps. You either predict the word given the context or vice versa. In either case similarity of word vectors is similarity in terms of replaceability. i.e. if two words are similar one could replace the other in the same context. Note that this means that ""hot"" and ""cold"" are (or might be) similar within this context.

If you want to use word embeddings for a similarity measure of tweets there are a couple approaches you can take. One is to compute paragraph vectors (AKA doc2vec) on the corpus, treating each tweet as a separate document. (There are good examples of running doc2vec on Gensim on the web.) An alternate approach is to AVERAGE the individual word vectors from within each tweet, thus representing each document as an average of its word2vec vectors. There are a number of other issues involved in optimizing similarity on tweet text (normalizing text, etc) but that is a different topic."
How to use a one-hot encoded nominal feature in a classifier in Scikit Learn?,I think you should try pd.get_dummies to code the categories; which will create new columns in dataframe and then use that df to pass it to the classifier.
Extract First names from usernames,"I think your best bet is a gazetteer approach. If you go to this link, you will find plenty of datasets containing lists of existing first names. This should help you detect a big majority of first names using regular expressions.

Now, there are some preprocessing steps you could take:

remove any digits, punctuation, etc.
if you have upper cases, you could split the username such that ""MichaelScott"" becomes ""Michael"" and ""Scott"". Now, in this case, you end up with two things that could be the first name. You could use the assumption that the first one is the first name and the second one is the surname.

Once you've done this, look at usernames where no first names were detected. Now, it could very well be that the username doesn't contain any first names (i.e. user123213543), or the user is bit cheeky and could use special characters or digits to write their first name (i.e. $am, B0b, etc.). If these situations exist, you could create strategies to deal with them."
How to build a machine translation system for a new language,"I think your case would benefit from tunning a existing language to the new one but that is only a good approach if you plan to use it commercially. Also Google accepts help to improve their translation algorithm and you could petition for them to assemble a team for this and donate data.

Google uses a Neural Network that they call Google's NMT (Neural Machine Translation) for translation and it works as a encoder-decoder pair. You can read more on their paper.

Also, Google's NMT is available in tensorflow and can be trainned and improved. It is license under Apache License 2.0 and has a nice tutorial and explanation available on their GitHub:

Check on this GitHub folder here"
How to create domain rules from raw unstructured text using NLP and deep learning?,"I think your question can be solved using Case Based Reasoning .

Basic principle how it works is, you need to train the model using whole lot of different cases which you have. Based on the symptoms which you give the outcome is predicted(which disease).

 Please refer to the links below for deep diving into the topic, appended couple of links with respect to health care industry:

Link -1
Link -2
Link -3
Link -4
Link -5
Link -6
Link -7

Do let me know if you need any additional information."
What is the difference between CountVectorizer() and Tokenizer() or are they the same?,"I thought that they both use one-hot encoding

These are utility to preprocess your text. Like any other utility, it has multiple options to tweak your text. You should explore all the parameters using the official docs.
I will explain one of these i.e. OHE vs Count

from sklearn.feature_extraction.text import CountVectorizer
corpus = [ 'This movie is bad.Too Bad', 'Awesome Movie. Too Awesome']
vectorizer = CountVectorizer(binary=True) #binary=False will make it Count
x = vectorizer.fit_transform(corpus)

import pandas as pd
df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())
df


Our end goal is to create Features and each Feature has an indicator for its contextual meaning.

what I don't understand is why CountVectorizer is not used on Deep Learning models such as RNN and Tokenizer() is not used on ML Classifiers such as SVM,

When we are modeling a simple ML algorithm, we generally use scikit-learn.
So there is no point in adding Keras there.

Same is True for Deep learning.
Though, in this case, we have another reason too i.e. Deep Learning generally works on a large dataset. So, we mostly use the idea of embedding on our features. So it's better to use a Framework that provides an end-to-end solution"
How to deal with imbalanced text data,"I too am working on same problem, found these below links very useful in getting started on oversampling and under sampling-

https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/

https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis"
Scraping financial web data,"I too would fall back on parsing either the HTML or the entities using regular expressions. My experience is though that this always gets unelegant quickly.

Do you have a somewhat clear idea of the relevant sources? If the better part of the relevant data comes from a limited number of pages, you could maintain a list of sources with matching wrappers.
Then within those relevant documents, I would search for the least complex most valuable features to extract.
Example

For instance, if you'd be interested in the quarterlies of Alphabet, I would scrape this link. You're smart, you can figure out the next one.

A quick glance learns me that the first hit on revenue(s) $ returns me the revenue for the quarter.

So something like this:

(?:revenue[s]?)(?:\s[\w]+\s)(\$[\d]+\.?\d\s[\w]+)


Testing that one the reports on the site seems to work on q1, q2 and q3 while q4 yields the annual revenue. Easy enough to fix.

My experience is that thee patterns hold for a while, and then change. No big deal, just add a couple of tests! Fi: Is the result not empty and is it in a believable range?"
Evaluation metric for Information retrieval system,"I want to know how to come up with ground truth(relevancy label) if it's not available?

There's simply no way to properly evaluate a system if nobody knows what the output is supposed to be. However there are ways to work around a lack of annotated data:

Ask a panel of annotators to grade the quality of the output on a sample. Disadvantage: if a relevant instance is never predicted, the annotators are unlikely to notice it.
Compare the output to a state of the art system. Disadvantage: the evaluated system can only be as good as the reference system, any error by the reference system is considered correct.
Generate artificial data with an automatic method. Disadvantage: the evaluation relies on the quality of the artificial data, so in theory one has to prove that the artificial data is as good as real data... which is usually harder than actually collecting real data.

In that case, how researchers calculate precision and recall? or how do they generate it?

They can't. It would be like grading an exam paper without knowing the correct answers.

This is why benchmark datasets are so important for the research community and are published as proper scientific contributions."
Confidence Score For Trained Sentiment Analyser Model,"I was able to solve it myself after some further research. I will be briefly describing my approach here. Cheers!

The idea is to find the confidence interval which was also same as finding the distance from the decision boundary / hyper-plane, in my case.

If you are using the Scikit Learn API, there is a method called predict_proba() for several classification models like Logistic Regression, SVM, Random Forest, etc. If your classifier does not provide one, you can wrap it with the CalibratedClassifierCV which can be found in sklearn.calibration, then use the above method to calculate the distance from the decision boundary.

If you are looking for custom in-depth implementation, here are some papers / references that might help.

Ref - 1
Ref - 2"
Using HashingVectorizer for text vectorization,"I was recently checking some things out. Thought would leave a working code here, in-case its helpful.

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction import FeatureHasher
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import numpy as np

categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
]
newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, 
categories=categories, random_state=91)
newsgroups_test = fetch_20newsgroups(subset='test', shuffle=True, 
categories=categories, random_state=91)

vectorizer = FeatureHasher(input_type='string')
X_train = vectorizer.fit_transform(newsgroups_train.data)
X_test = vectorizer.fit_transform(newsgroups_test.data)

Y_train = newsgroups_train.target
Y_test = newsgroups_test.target
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

rf = RandomForestClassifier(n_jobs=-1, n_estimators=100)
rf.fit(X_train, Y_train)
pred = rf.predict(X_test)

score = metrics.accuracy_score(Y_test, pred)
print(""accuracy: {:.3f}"".format(score))"
Help with reusing glove word embedding pretrained model,"I was stuck in a similar problem while working with glove. Assuming that you have a dataset in text form, from which you want to collect the topmost 100000 words, you'll have to make a list of those words. In the glove file, each embedding is on a separate line, with each line starting with the word itself and then the embedding. You'll have to write a code to compare your list of words with the words in glove file and extract the lines which make a hit. Have a look here for example code."
How to calculate perplexity in PyTorch?,"I was surfing around at PyTorch's website and found a calculation of perplexity. You can examine how they calculated it as ppl as follows:

criterion = nn.CrossEntropyLoss()
total_loss = 0.
...
for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
    ...
    loss = criterion(output.view(-1, ntokens), targets)
    loss.backward()
    total_loss += loss.item()
    log_interval = 200
    if batch % log_interval == 0 and batch > 0:
        cur_loss = total_loss / log_interval
        ...
        print('ppl {:8.2f}'.format(math.exp(cur_loss)))
        ...


As @SpiderRico reminded, I got it from this link"
How to implement multi class classifier for a set of sentences?,"I was thinking if Apriori might be more suited for your purpose.

For your consideration: 1) Tokenize the training sentences into bag of words: Review Sentence | Upside | Calm | Swimming | Cat

2) Tag the correct consequents for bag of words.

3) Apriori should produce 1 rule for ambiance and 1 rule for Entertainment.

Hope this helps."
what is the training phase in N-gram model?,"I will start with an advice - just google ""n gram language model"" and you will find a lot of good detailed explanations.

With that being said I will give a short explanation about the ""training phase"" of n-gram language models (answer to question 2). The simplest way to build an N-gram language model strats with finding a big corpus - a set of many sentences. the words of the model will be the words that appear at least once in the corpus. The probability of the word xn given a past context of the words x1,x2,...,xn-1 will be the number of occurrences of the sequence x1,x2,...xn-1,xn in the corpus / the number of occurrences of the sequence x1,x2,...,xn-1 in the corpus.

This is the simplest way and it has problems, especially What happens if the sequence x1,x2,...,xn does not appear in the corpus? It will always get propability zero. Therefore there are smoothing techniques to handle this problem (read about it).

And now for question 1 - In the simplest case, without smoothing, the candidates are the words that appear in the corpus. In models with smoothing the candidates may be all of the words in the sense that every word might get a positive probability."
Proper masking in the transformer model,"I will take as reference fairseq's implementation of the Transformer model. With this assumption:

In the transformer, masks are used for two purposes:

Padding: in the multi-head attention, the padding tokens are explicitly ignored by masking them. This corresponds to parameter key_padding_mask.
Self-attention causality: in the multi-head attention blocks used in the decoder, this mask is used to force predictions to only attend to the tokens at previous positions, so that the model can be used autoregressively at inference time. This corresponds to parameter attn_mask.

The weight mask, which is the combination of the padding and causal masks, is used to know which positions to fill with 
‚àí‚àû
‚àí
‚àû
 before computing the softmax, which will be zero after it.

You don't need to preserve any zeros in the output, as the attention blocks take care of that (see answer (1)). In the original Transformer article, the attention works without bias, but the bias does not change performance. Actually, in fairseq the bias are used by default.

Yes, padding_idx is certainly used to zero out padded tokens."
help finding research discussion on HTS classification,"I would approach such a problem with a method a bit similar to record linkage: trying to match every product description with the most relevant HTS description. The traditional approach would be to use textual similarity measures such as cosine TF-IDF, but many variants can also be considered, e.g. with embeddings or other ways to take semantic similarity into account. At the end the code corresponding to the most similar HTS description is predicted.

Initially this could be done with vectors of words (unigrams), but it's true that comparing vectors of 
n
ùëõ
-grams is likely to be more precise. However 
n
ùëõ
-grams don't work the way you describe: in a case like your example you would have for instance 
n=2
ùëõ
=
2
 and extract all the sequences of 2 consecutive words. This ""bag of 
n
ùëõ
-grams"" is what the vector represents. Combining different lengths of 
n
ùëõ
-grams is possible but not in the same representation: for example you could measure cosine similarity over 2-grams vectors only, then measure cosine similarity of 3-grams vectors only, and take the mean of the two scores (or even build a regression model using different similarity scores as features).

In case it helps, here is an example of computing a very simple similarity score with bigrams (one could certainly find better examples online)."
Detect named entities inside words,"I would recommend to look into character level named entity recognition. For example: Kuru et al, CharNER: Character-Level Named Entity Recognition, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers (2016)

The authors evaluate on many highly inflected languages including Turkish, so this should be adequate for your Finnish use case

The code is here: https://github.com/ozanarkancan/char-ner

You should hopefully be able to download and get it running out of the box for training. Of course I am assuming you have a tagged NER corpus in Finnish, which you would need to preprocess to get into the same format as the CSV file that they use for Czech in the repo."
Suggestions for guided NLP online courses - Beginner 101,"I would recommend two course which focus on code first approach and which will help you understand concepts by getting your hands dirty. Both of these courses contains code and video resources.

Fast.ai NLP
Hugging Face NLP

Happy Learning :)"
how to encode labels for relationship extraction,"I would reommend parsing the sentences using something like spaCy, which will be able to build such relationships for you.

It will then be a matter of extracting the realtionships and deciding how you want to label them. Let's work through a small example. Imagine your big block of text contains many sentence. We first parse the big block, then could (optionally) extract just the sentence that we care about:

import spacy

nlp = spacy.load('en_core_web_sm')      # this defines our 'parser' for English
doc = nlp(big_block_of_text)            # you may want to clean the text beforehand


Assume we only want sentences that contain the word 'went':

sents = [sent for sent in doc.sents if 'went' in sent.string]


Here is an example of grammatically walking down the parse tree. I say semantically because we are using the grammar parts-of-speech to conditionally navigate (using e.g. nouns):

for sent in sents:                          # loop over each sentence
    for word in sent:                       # analyse each word
        if word.pos_ == 'NOUN':             # if it is a noun
            for child in word.children:     # find children (go down the tree)
                if child.pos_ == 'NOUN':    # select only nouns from the children
                    print(word, child)


I believe this snippet would extract the pairs in your given example, so the output would hopefully indeed be:

""He movie""
""they school""


There are many other parts-of-speech that you will be able to use to be more specific or match other use cases - check out the relevant spaCy docs.

How to actually encode the relationships between your target words will depend on their final purpose, i.e. what you eventually want to do with the encodings. You now have the noun pairs. If you want to retain the knowledge of where they actually came from (from which sentences), you could indeed build a hierarchy, that keeps a code for the origin sentence and then a code for each pair of words."
Lemmatization Vs Stemming,"I would say that lemmatization is generally the preferred way of reducing related words to a common base.

This Quora question is a good resource on the subject: Is it advisable to choose lemmatization over stemming in NLP? The top answer quotes another good resource that motivates why lemmatization is usually better, Stemming and lemmatization, from Stanford NLP:

Why lemmatization is better

Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.

Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.

But that is just generally, it is not always better. Stemming still has some advantages and it will depend on the use case. Some reasons you would use stemming over lemmatization could be:

Some possible exceptions when stemming can be better

Simplicity
Speed
Memory constraints"
What's the best way to train a NER model?,"I would start by training some very strong Named Entity classifier on available datasets for NER. One is the Annotated Corpus for Named Entity Recognition available on Kaggle.

Additionally, you can find a good list of datasets here. I know they have nothing to do with cybersecurity, but I think it's important to incorporate very different sources in a big, final dataset, in order to make a model that is good at generalizing on texts it has never seen before.

Another source of data for NER tasks is the annotated corpora available from nltk library, such as the free part of the Penn Treebank dataset, and Brown corpus.

Please beware that different datasets might use different categories for classification (i.e. the set of Named Entities can be different from dataset to dataset). Make sure you make all your data compatible to your classifier before training

After that, I suggest you to go with seq2seq models. Every state-of-the-art RNN is some form of seq2seq. Once you trained a classifier, you could try to annotate few articles manually, and check the performance of your model on those. It's time consuming, but I personally like these ""qualitative"" checks, I think they can tell you a lot."
Music corpus sentence level clustering,"I would suggest hierarchical clustering. It's unsupervised, and you don't need to predefine number of clusters. How it works (for the bottom up version) is each sentence (or object) is initialized as its own cluster. At each iteration of the algorithm, the two clusters of the smallest intra-cluster distance are joined, all the way until there's a 'root' where everything is one cluster. The result of this is a big dendrogram, and you can cut the dendrogram at whatever point you want to define clusters. Or you can just inspect it. Here's a nice numerical example of the alg in action.

It should detect the same types of clusters you're already finding, and you wouldn't need to redefine a distance metric. And, I anticipate you'll get the same amount of class separation you seem to already be getting; in fact, the nice thing about the dendrogram from hierarchical clustering is that it illustrates class separation nicely. it's 'hclust' in R, not sure for Python."
Mastering NLP: Reading List,"I would suggest instead of trying to get many sources, get one source that goes through concepts first (fairly robustly), then seek out sources to refine or deepen your knowledge. One source comes from Stanford's NLP group, and is Introduction to Information Retrieval. The only thing I don't like about this books is that it tends to orient documents as columns (where data science has more or less agreed that they are rows), but that's a pretty trivial concern (as long as you can take a matrix transpose). Aside from that, this book has excellent explanations, and the proper depth and breadth to be considered an exhaustive base for NLP."
Document classification - optimal classifier & embedding,"I would suggest to have a look at fasttext, it will learn word embeddings for you from your corpus (advantage over word2vec embedding is that it learns representations based on character ngrams, so if a word is there, but its plural is not, it will still have a similar representation) and then in the supervised learning mode it uses something very simple like regression for classification, provided you have examples of labelled data. I tried it with trivial 20newsgroups and it works well for distinct categories. Maybe not a production-friendly solution (it's in c++, but can be stretched to python, albeit not without pain), but can give an idea. Given the nature of the algorithm, it's quick, compared to nets."
Removing junk sentences,"I would suggest training your own custom Dialog Act Classification Model. Detecting a dialog act is a Natural Language Understanding (NLU) problem. Using a dialog act classification model you can detect if a sentence is used for ""greeting"", ""question"", ""opinion"", ""promise"", etc.

Detailed approach:

Use a pre-trained language model (for example BERT), to train your own Dialog act classification model. You can create your own custom Dialog act class based on your requirement (as you want to remove specific types of sentences). You can class them as junk or use ""greeting"", ""question"", ""opinion"", etc as dialog class.

After you are done with the training you can loop through the list of sentences and filter them based on their predicted classes.

You might have to research if there are similar corpus which you can use or have to manually label and then train the model. It will be a more reliable approach as compared to the word2vec or rule-based POS approach.

For more details on state-of-the-art approaches and dialogue act corpus follow this link. If you have no idea how to use a language model you can use the transformers library from huggingface. I hope you find this helpful."
Python - Check if text is sentences?,"I would try a semi-supervised learning technique where it passes you scraps and asks you to label them. What you're looking for will likely be kind of domain specific depending on the type of site. In the end you'll probably have a bunch of heuristics like:

If length < 50 and contains ""LOGOUT"", ""REGISTER"",""SIGN IN"",""LOGIN""
If count of ""|"" > 1
If count of all upper case words > 1"
Are there libraries or techniques for 'noisifying' text data?,"I you want some kind of data-sets like Google spell checking data I suggest you look into the The WikEd Error Corpus dataset. The corpus consists of more than 12 million sentences with a total of 14 million edits of various types, this edits include: spelling error corrections, grammatical error corrections, stylistic changes. All these from the Wikipedia correction history. The owners (authors) of the data-set describe the data mining process in this paper. Also check this question in quora it contains links to various data-sets with spelling errors. Finally this page can also be useful."
Finding and ranking best semantic matches between two sets of phrases,"I'm experienced primarily in recommender systems, but I've done enough work in NLP to have some ideas on how to approach this problem.

I don't know of any formal name for the problem that you are proposing, but I do know that it's going to be really hard to train a model to learn from just that data, even if you had dense labels. There's just too much unstated human context embedded into the problem to train a model on that from scratch.

Like for your example of Fruits traditionally grown in Germany. You have to find some Knowledge graph or embedding that has learned the relationship of fruits and where they grow geographically. There is a limited set of things out there that would be capable of this.

So what you need to do is apply another model or language embedding to the data, and try to engineer a solution from that.

The first thing I thought of is any large-scale pre-trained skip-gram or CBOW embeddings (these embeddings are often called ""word-vectors"" or ""thought vectors""). A resource to the basics of that here.

The idea here is to use some pre-trained language model and calculate embeddings for your inputs and outputs. Then you just do cosine similarity between the embeddings for each input and output, and see if you can get good matches.

Because you're using sentences you're going either going to have to use a model that embeds sentences or documents (doc2vec is an example), or you're going to have to find some way to aggregate token embeddings. I would pick the former if you try this approach.

But to build on that, once you have these pretrained embeddings, you could also train your own neural net on top of those embeddings to classify for matches. I would read up on the task of question-answer for neural networks for inspiration, as I imagine you might take some ideas from that for how to associate the queries and matches for a neural net (Google ""QANet"" for some leads on this).

Another approach, which depending on the data you're working with, would be a knowledge graph. This solution would be more complicated, but basically you would break up the documents into different pieces (POS/NER tagging) and then search for semantic equivalents in a knowledge graph. Some example KGs can be found here.

Another thing to google would be the field of ""Ontology"". Very much related to knowledge graphs but may get you some more niche results that could prove to be helpful.

Some more context about the exact problem you're trying to solve or what kind of dataset you're working with may help illuminate other solutions. Hope that some of the terms I gave you pushes you into the right direction. Good luck!"
How do I visualize data for a natural language processing project?,"I'm not an expert but let me try to think with you. What's your vocabulary size?

I think certainly starting with a small machine learning model is a good idea, but I think that a decision tree would quickly suffer with even a medium-sized vocabulary. You would need a huge tree to do anything. So I think I would start with pretrained word embeddings, and use a small neural net to predict the starting point. This helps, because words that are close in meaning have similar vectors, and the decision tree wouldn't be able to use that kind of information.

Your suggestions for histograms don't seem bad, but you would have a histogram that is as wide as your vocabulary, which seems like it defeats the purpose of visualizing it... If you went with word embeddings, how about using a technique like UMAP to plot the questions and articles in 2D?"
extract names in a list of names,"I'm not an expert here, so here's my (brute force?) method.

SeatGeek has open-sourced a python library called fuzzywuzzy which is great at text matching. It has a function called token_set_ratio that compares two multi-word strings and scores their distance. It can consider just the intersection of individual words and score only that intersection. eg ""Barack Obama Bill Clinton Madeleine Albright"" will get a score of 100 with ""Bill Clinton"", but also with ""Bill Barack"".

You'll need to build a list of known names, judging by the list they're famous people, which a quick google search shows hundreds of sites that have that ready for you. With that list, you can just loop your way through, computing a score with token_set_ratio. If the famous name beats some threshold score against your string, you append it to a list of captured names."
Word representation that gives more weight to terms frequent in corpus?,"I'm not aware of any standard representation which increases the importance of document-frequent words, but IDF can simply be reverted: instead of the usual

idf(w,D)=log(
N
|d‚ààD¬†|¬†w‚ààd|
)
ùëñ
ùëë
ùëì
(
ùë§
,
ùê∑
)
=
log
‚Å°
(
ùëÅ
|
ùëë
‚àà
ùê∑
¬†
|
¬†
ùë§
‚àà
ùëë
|
)

you could use the following:

revidf(w,D)=log(
N
|d‚ààD¬†|¬†w‚àâd|
)
ùëü
ùëí
ùë£
ùëñ
ùëë
ùëì
(
ùë§
,
ùê∑
)
=
log
‚Å°
(
ùëÅ
|
ùëë
‚àà
ùê∑
¬†
|
¬†
ùë§
‚àâ
ùëë
|
)

However for the task you describe I would be tempted to try some more advanced feature engineering, typically by using features which represent how close the distribution of words in the current document is from the average distribution."
How to get the tagset for hindi pos tagging?,"I'm not familiar with NLTK but the tagset must come from the annotated corpus which was used to train the tagger. According to https://www.nltk.org/book/ch05.html (section 2.2), if you can find the name of the Hindi corpus you should be able to access the words with their tags with:

nltk.corpus.<corpus>.tagged_words()


Apparently this is the list of corpora available in NLTK: http://www.nltk.org/nltk_data/, it might help finding the one used to train the Hindi POS tagger.

Once you identify the corpus, it's likely that you can find explanations about the tagset and annotation process by searching for papers about its creation by the original authors."
Dealing with diverse text data,"I'm not sure how you are applying a regression framework for document classification. The way I'd approach the problem is to apply a standard discriminative classification approach such as SVM.

In a discriminative classification approach the notion of similarity or inverse distance between data points (documents in this case) is pivotal. Fortunately for documents, there is a standard way of defining pairwise similarity. This is the standard cosine similarity measure which makes use of document length normalization to take different document lengths into account.

Thus, practically speaking, in cosine similarity you would work with relative term weights normalized by document lengths and hence document length diversity should not be a major issue in the similarity computation.

One also has to be careful when applying idf in term weights. If the number of documents is not significantly large the idf measure may be statistically imprecise thus adding noise to the term weights. It's also a standard practice to ignore stopwords and punctuations."
Sentiment analysis of the target in articles,"I'm not sure I completely get the idea, but it looks to me like what you're actually interested in is the sentiment of a word in a particular context: a content word like ""car"" might not carry a stable sentiment by itself, but its usage in a specific context might.

So I'd suggest a method like this: for any target word you extract either the sentence or a context window, i.e. N words on the left and N words on the right of the target word. Then you could use predefined sentiment analysis tools to extract a sentiment value for this instance. From there you could:

measure the mean sentiment for a word by averaging over the instances
compare the distribution of sentiment or average sentiment for two different words"
What is the best practice to classify category of named entity in sentence,"I'm not sure I fully understand your question, but it seems to me that you're trying to determine the category of the string/entity ""titanic"" out of context. Your data tells you that ""titanic"" could be a book, a movie, or a product, and you want to figure out which one is correct -- is that what you're trying to do?

If so, the problem is that you've dropped the context in which the string/entity ""titanic"" appears in your original text. For example...

In the sentence ""I couldn't stop reading Titanic,"" the word ""titanic"" refers to a book.
In the sentence ""Titanic was one of the highest-grossing films of all time,"" the word ""titanic"" refers to a movie.
In the sentence ""The Titanic was the world's largest ocean liner,"" the word ""titanic"" refers to a product.

Without that context, there's no way to know which is the correct category. I'd suggest looking into how named entity recognition tools like Stanford NER work -- that will help you better understand how to do something like this. You'll see that the input to an NER tool generally needs to be a sentence, in order to take advantage of the context to properly categorize the extracted entities."
"How to train a supervised sequence classifier like CRF, if we have to extract start date and end date from a user query in python","I'm not sure that building a model using CRF is the best approach here. It's going to require quite a bit of training data and effort to get it working like you want. Dates are pretty structured in most cases so there are more straightforward approaches to extracting them. Stanford's SUTime library, for instance, does exactly what you're describing in your question. Although is is Java based there is a Python wrapper for it. For example, for the following input:


query = 'I need a desk for tomorrow from 2pm to 3pm'


Using the SUTime Python wrapper, this is the parsed result you'd get:


[
    {
        ""end"": 26,
        ""start"": 18,
        ""text"": ""tomorrow"",
        ""type"": ""DATE"",
        ""value"": ""2016-10-14""
    },
    {
        ""end"": 42,
        ""start"": 27,
        ""text"": ""from 2pm to 3pm"",
        ""type"": ""DURATION"",
        ""value"": {
            ""begin"": ""T14:00"",
            ""end"": ""T15:00""
        }
    }
]"
How to i get word embeddings for out of vocabulary words using a transformer model?,"I'm not sure there is a need for aggregation, or in other words you may have a pipeline mismatch. BERT sentencepiece tokenization is specifically meant to be passed to some set downstream pipelines, with the aim of the sentencepiece thing being to be able to cater to OOV words. By aggregating the sentencepiece tokens, you might be doing away with the benefit of being able to cater to OOV in your later pipeline.

If you are looking for whole word vector tokens, and want to work with OOV words, I would recommend looking at FastText instead. This algorithm more or less uses subwords, and it will also build tokens for OOV words by pretty much aggregating the subword information for that OOV word during a custom training step. The benefit here is that the aggregation step need not be part of your pipeline, and you can use these new vectors in any downstream task (except, of course, the pipelines that accept BERT sentencepiece tokens)"
"Word Embedding for Item Names(integer, one-hot encoding)","I'm not sure, if it's possible with this data set. Word2Vec is used to generate word embedding, which works on the principle of ""words association"" in a sentence.

So I dont think you can apply Word2Vec on this dataset which looks like doesn't have any association, except on some places where you can match(perform clustering) some parameters like:

Units
Size/dimension of the item-name

Interested to know some solution for such types of problems."
What should be the ratio of True vs False cases in a binary classifier dataset?,"Ideal true vs false ratios don't exist and they should reflect the the reality the best they can, you can always remove negatives if the ratio is too skewed to improve training speed though. Let me explain it with an example. Ads CTR is as old as the internet and it's skewed to less than 1% positives vs. plus 99% negatives. Yet, data scientists prefer to train it on the entire dataset because many negatives will include information that models couldn't find otherwise. They might not provide a lot of information as a positive one but they are still somewhat important. There are approaches where CTR ratios get artificially rebalanced by sampling in case you want a swifter training and it will still work. In your case, positives are 0.4% which resemble CTR on ads so you can: gather more data to increase the number of positives in order to better understand what makes an article interesting. In case that is not possible trying ensembles which often improve prediction performance.

Clustering is an unsupervised approach so you would be losing information by doing so (training labels) besides, sentence embeddings (representations) of one big cluster of negatives and a tiny cluster of positives do not convey information as well as word embeddings which have already been trained on billions of documents.

In addition, running k-means on categorical variables will yield anomalous clusters because it's meant to be used with continuous variables. You can find more information about the topic on the following links:

Kmeans: Whether to standardise? Can you use categorical variables? Is Cluster 3.0 suitable?

My data set contains a number of numeric attributes and one categorical

Kaggle

Why does K means clustering perform poorly on categorical data The weakness of the K means method is that it is applicable only when the mean is defined one needs to specify K in advance and it is unable to handle noisy data and outliers

Therefore, you should use high dimensional embeddings or representations to cluster meanings together, this has been explored in word meanings but for sentences or articles, a vector representation becomes more complicated to implement. One possible approach is the Word Movers‚Äô Distance but there are many more possible approaches, you should google them. In addition a non-linear clustering algorithm such as t-sne will probably yield better results than k-means using the embeddings approach.

A better approach is:

to use multiple models and compare their performance on this dataset. I have the impression that there will be certain keywords that make articles interesting, so a bag of words will still be helpful, even as a starter model.

Use feature engineering. Your model might be overloooking important features, such as article length, reading time, number of paragraphs, ratio of complex words (measured by length), etc. Feature engineering is always important in case you haven't used it yet.

Use pretrained embeddings. CNN and RNN models can use pretrained embeddings such as GloVe, Word2Vec or FastText so you use better representations plus other complex layers later on in the architecture. This is extremely important to increase accuracy.

Use metrics to measure improvement and ranks to check for the best predicted interesting articles."
How to Extract Information from the Image(PNG),"If all of your images are similar to this one(or have a small set of possible designs), you can simply reference the location (pixel-wise) on the image where this fields are and slice it.

After slicing you can use any OCR algorithm to extract that data.

If your data has more variation than that, you can use OCR on the entire image, which is usually a slow algorithm.

If you have less than 50 or so images to do that, it is more efficient to do this by hand (not worth writing code)"
LSTM not learning with extra nontemporal data added after LSTM layer - Keras,"if anyone stumbles upon this - it seemed like my dimension of nontemporal data was too high and sparse. After filtering only for the most common/important variables in the nontemporal data, the model learned."
Classifying objects based of a varying number of the same type of feature vector for each object,"If I did not get you right please comment me. I would not go for doc2vec as you do not want to discriminate docs but persons. So better to concatenate speeches of each person to a single document and then feed it to word2vec/doc2vec (I assume you would like to use ANNs otherwise there are other options e.g. TF-IDF, etc.) In this manner each person will have a 300-d feature vector including a everything. (I would still try TF_IDF and CountVectorizer as well!)

If you insist on your current option, you may discard person info (you want to get a speech and say what kind of political-party the speaker is coming from, right?!) and set up your data as a 300-d feature vector for each speech whose tag is a political view (instead of taking mean of all speeches of 1 person). Then your feature vectors are all the same size (in this approach, each person might have several entry in the data which may bias the data so you may add a column including the speaker e.g. ""speaker a"", ""speaker b"", etc. Here you end up with a 301-d data).

Hope it helped. Good luck!"
Building a tag-based recommendation engine given a set of user tags?,"If I got your problem description correct, you are looking for a recommender system like for example used by Netflix or Amazon. State of the art solution would be to use Latent Dirichlet Allocation topic modeling to make recommendations based on topics (in your case, topics would be the tags). Here is a very good video tutorial on this topic: https://youtu.be/3mHy4OSyRf0

In the case of the standard version of LDA, you don't even have to define the tags, you just define a value of different tags among all your documents. If you have for example 10000 documents and you want to use 100 different tags, the method will transform your words/documents matrix into a topics/documents matrix.

The entries of the words/documents matrix are simply all documents as columns and all words (from all your documents) as rows, then for each document you have the counts of each word.

The entries of the topics/documents matrix are all documents as columns and all possible topics as rows, then for each document you have entries like 78% topic1, 12.5% topic95, 0% topic99 on each topic.

Once you have this data and you want to recommend a new document to a user based on his interests(tags), or in other words you have a user_interests vector 
u
‚Éó¬†
ùë¢
‚Üí
 with 100 entries which have values between 0 and 1, and you have topics/documents matrix 
M
topics√ódocuments
ùëÄ
ùë°
ùëú
ùëù
ùëñ
ùëê
ùë†
√ó
ùëë
ùëú
ùëê
ùë¢
ùëö
ùëí
ùëõ
ùë°
ùë†
 you calculate a new matrix by multiplaying 
M
topics√ódocuments
‚àó
u
‚Éó¬†
ùëÄ
ùë°
ùëú
ùëù
ùëñ
ùëê
ùë†
√ó
ùëë
ùëú
ùëê
ùë¢
ùëö
ùëí
ùëõ
ùë°
ùë†
‚àó
ùë¢
‚Üí
, from this matrix you calculate the sum from each row and recommend those documents with the highest sum.

If you just want to use predefined tags, you can skip the step where you use the LDA method to calculate the topics/documents matrix and simply use your data to represent your documents as tags/documents matrix and your users as tag_vectors, proceeding from here the same as above: multiplying the matrix with a user_vector, calculating the sum from each row and recommending the documents with the highest sum."
How to estimate probabilities of different classes for a Text,"If I understand correctly, we are interested in soft multilabel classification, where a single text can have multiple correct genres.

According to your comment, we don't have any training data, just a list of keywords associated with each genre.

We can try computing the similarity between each document and each keyword list:

Normalize the document (convert to lowercase, remove punctuation, diacritics, non-alphanums, etc)
Remove stopwords
Convert the document to tf-idf vector over our genre keyword vocabulary: Each document gets an n-length vector where each entry is the frequency of the ith genre keyword in the document. Normalize this vector to magnitude 1.
Convert each genre keyword list to a tf-idf vector in the same way (again over the keyword vocabulary for all genres).
Compute the cosine similarity between the document vector and each genre vector.

For each document, this will give us a number in the range [0,1] for each genre. For example:

         Comedy Drama Fiction Romance Mythology Adventure
Text #1: 0.15   0.11  0.03    0.00    0.00      0.07


If we were doing single label classification we could normalize each row to add up to 1 and we might have a working model. However there is no such trick for multilabel classification here. We don't have a good way to calibrate these values into probability estimates.

At this point the only solution I see is to build a small training set so we can fit our model to actual data.

After gathering some training examples, we can run a multilabel regression with sigmoid activation and binary crossentropy loss with the cosine similarities as input features to get a probability estimate for each class.

Using this method our list of genre keywords will at least save us having to build a large training set to solve the problem directly with bag-of-words or similar approaches."
Classify sentences containing typos into groups,"If I understand correctly, you're looking for string similarity. There are several techniques available, the most simple is ""edit distance"" (aka levenshtein distance), which is the count of the minimum insertion/deletion/substitution/transposition operations needed to get from one string to the other.

For your particular task, I suspect ""jaro-winkler similarity"" would be better. JW is similar to ED, but was specifically designed for ""entity resolution"" (i.e. ""record linkage""), which it looks like is what you're trying to accomplish. You can see a short demonstration of how this would work here"
Approach to semantic similarity between documents,"If I understand correctly, you're trying to map abstracts to their research papers.

Here is a simple starting point:

Compute a TF IDF model using the entire corpus (all abstracts + research papers). Use this model to transform your abstracts and research papers into a weighted vector representation. Under the TF IDF weighting scheme, these documents will be represented by vectors that point in the direction of the words that are most discriminating for them. Put another way, these vectors point towards the words that are the best at telling you what the document is about. So if two vectors are close to each other, then the two documents are likely to be about the same topic because they are using similar words.

This is where cosine similarity comes in. Take an abstract, and then iterate across all the research paper vectors, computing the cosine similarity between them. Then, map that abstract to whichever research paper has the highest cosine similarity. Repeat this for each abstract.

Once you have this simple baseline approach, you can start looking at more sophisticated models for capturing semantic similarity."
Algorithms and tools for ranking text as a job description,"If I understand the problem correctly, there are several approaches that can be taken.

We'll begin with the least deep learning oriented solution first and slowly move into the spectrum of Deep Learning. This choice is primarily for cost and difficulty reasons. The amount of bugs that you can find with a deep learning solution can be very difficult to understand in certain situations and that results in slower prototyping and ultimately slower time to production.

At the very base, your problem deals with ranking the jobs with respect to how relevant they are to a particular job. The literature for ranking problems is vast and I will suffix my answer with links to these papers or studies. I will also say that instead of dealing with this as a ranking problem there may be another way of going about it. However, this is hinged on the type of data you possess. If you know that each query is certainly looking for a particular type of job. Then, you can break this down into a multi-task problem, where one of the tasks is figuring out whether there is a vacancy or not and the second is figuring out if the vacancy relates in any way to the search query.

Solution 1: Take the dataset and handcraft a set of features that you think are relevant to the process of classifying the output. To do this effectively, you should know certain core features that you think relate the input features to the final label (here I use input features to represent the features that you think represent your features well. For example, some features that may be relevant is information about the person or entity posting the title (in stage 2), if they are a well known company in the domain (this can be accesses through some intelligent web scraping and text retrieval methods). Then, it is possibly more likely for it to be a posting, right? Let us assume now that you have constructed this set of features. Now, for each of the set of features, you have a corresponding label to whether there is a vacancy or not. To turn this approach into a ranking problem is very simple. All you have to do is observe the confidence (the softmax probability is good enough) and the one which has a higher probability is ranked higher). This solution is an easy one to accomplish using even simple feed forward neural networks. You would also benefit from testing out simple classifiers like SVMs or Random Forests. It must also be remembered that the actual sentence must be embedded into some numeric space, like a sequence of vectors in 
R
n
ùëÖ
ùëõ
. This solution does not detail all of the explicit steps but provides a general framework with which you should be able to attack the problem.

Drawbacks The above method requires a lot of time spent in handcrafting the feature set and there is going to be a lot of iteration because the feature set is never going to be perfect. There is always going to be something that you possibly missed out. Secondly training a good word embedding is not a very easy task. You may have to spend a lot of time getting that running, especially if you want the embedding to be somehow related to your words.

Solution 2 This solution begins to touch the fence of Deep Learning. This solution is a comparison of the attention encodings within the source utterance and the target utterance. The source utterance is, a phrase like ""need janitor"". Now you should train an attention model over the sequence of words and observe what is the word(s) being given the maximum attention to. Once you know this, the problem becomes a question of consolidating the attentions in some meaningful manner and comparing the distance between the source and target through some metric space. So you should certainly pay attention to attention based models (haha!). Again, there are going to be hiccups in the process of figuring out how much attention actually affects the output and whether it is a good model for your data. Distances between attention will return a vector and you can consider the 
L
2
ùêø
2
 norm or any other map from 
R
n
‚ÜíR
ùëÖ
ùëõ
‚Üí
ùëÖ
.

Drawbacks Attention distances are a good metric, however, they are extremely data centric. For example if the ads are very long, it may be true that the model does not pick up the appropriate words to attend to.

Now we get to some resources that may be useful to go through in order to understand exactly how the different components of the answer work.

https://en.wikipedia.org/wiki/Learning_to_rank
http://www.shivani-agarwal.net/Events/SDM-10-Tutorial/sdm10-tutorial.pdf
https://towardsdatascience.com/learning-to-rank-with-python-scikit-learn-327a5cfd81f
http://times.cs.uiuc.edu/course/598f16/l2r.pdf
https://www.stat.uchicago.edu/~lekheng/meetings/mathofranking/slides/agarwal.pdf

Now for the posts about attention:

http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

http://akosiorek.github.io/ml/2017/10/14/visual-attention.html

https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129

Finally, my last component of the answer will deal with evaluating exactly how well you have done using metrics. I have added some metrics above in my Answer, however there is another metric which in particular represents the similarity, syntactically between the input query and the output query.

https://en.wikipedia.org/wiki/BLEU"
Web page data extraction using machine learning [closed],"If I understand your question properly, this seems to be a scraping problem which you can do using Beautifulsoup in python."
Correcting ALL CAPS for human and algorithmic consumption,"If sensitivity to case is breaking your models you have two options:

Train or find a new model that's case-insensitive. This is probably the easiest thing to do. The Stanford parser has one.

Train a model to correct the case of your input, this is sometimes called truecasing. The Stanford Parser has this functionality too."
How to deal with spelling errors in NLP classfier (low resource language),"If the language doesn't have any spelling correction tool and you want to take care of spelling errors, practically you'd have to build one by using string similarity measures, and using frequency as an indication of the correct spelling.

In my opinion, very often it's not worth the effort and there's a risk that it would introduce new errors. Statistically, if a spelling error is frequent then it's better to train the model with it so it recognizes it when the model is applied (i.e. exactly as if the spelling was correct), and if the spelling error is rare then it's just noise: it's impossible to get rid of all the noise and it shouldn't affect performance significantly."
why do transformers mask at every layer instead of just at the input layer?,"If the masking were only applied in the first layer, the self-attention in the subsequent layers would bring to each position information from future tokens.

Let's break it down with numbers:

At layer 
i
ùëñ
, if causal masking is applied, the output at position 
t
ùë°
 contains information about layer 
i‚àí1
ùëñ
‚àí
1
 at positions 
1..t‚àí1
1..
ùë°
‚àí
1
, that is, 
L
i,t
=
f
i
(
L
i‚àí1,1
,...,
L
i‚àí1,t‚àí1
)
ùêø
ùëñ
,
ùë°
=
ùëì
ùëñ
(
ùêø
ùëñ
‚àí
1
,
1
,
.
.
.
,
ùêø
ùëñ
‚àí
1
,
ùë°
‚àí
1
)
.

If no causal masking is applied, then the output at position 
t
ùë°
 contains information about layer 
i‚àí1
ùëñ
‚àí
1
 at all positions in the sequence of length 
T
ùëá
, that is, positions 
1..T
1..
ùëá
 
L
i,t
=
f
i
(
L
i‚àí1,1
,...,
L
i‚àí1,T
)
ùêø
ùëñ
,
ùë°
=
ùëì
ùëñ
(
ùêø
ùëñ
‚àí
1
,
1
,
.
.
.
,
ùêø
ùëñ
‚àí
1
,
ùëá
)

If causal masking is applied at layer 1 (the first layer) but not at layer 2 or 3, we obtain that for position t at layer 3 we would have: 
L
3,t
=
f
3
(
L
2,1
,...,
L
2,T
)
=
f
3
(
L
2,1
,...,
f
1
(
L
1,1
,...,
L
1,T
))
ùêø
3
,
ùë°
=
ùëì
3
(
ùêø
2
,
1
,
.
.
.
,
ùêø
2
,
ùëá
)
=
ùëì
3
(
ùêø
2
,
1
,
.
.
.
,
ùëì
1
(
ùêø
1
,
1
,
.
.
.
,
ùêø
1
,
ùëá
)
)
, which means that position 
t
ùë°
 contains information from future tokens, as 
T>t
ùëá
>
ùë°
.

Note: The original answer was wrong and was completely edited. To check the original answer, refer to the post timeline."
NER with Unsupervised Learning?,"If we treated NER as a classification/prediction problem, how would we handle name entities that weren't in training corpus?

The goal of a NER Tagger is to learn patterns in language that can be used to classify words (or more generally, tokens), given a pre-specified set of classes. These classes are defined before training and remain fixed. Classes such as: PERSON, DATETIME, ORGANIZATION, ... you name it.

A good NER Tagger will learn the structure of a language and recognize that ""Fyonair is from Fuabalada land."" follows some linguistic rules and regularities, and that from these regularities (learned autonomously during training) the classifier can attribute Fyonair class PERSON and to Fuabalada the class LOCATION.

How would our model identify it if it wasn't included in billions of corpus and tokens?

In fact, Deep Learning models tend to work better than others with very large datasets (the so called ""big data""). On small datasets they are not extremely useful.

Can unsupervised learning achieve this task?

NER tagging is a supervised task. You need a training set of labeled examples to train a model for that. However, there is some unsupervised work one can do to slightly improve the performance of models. There is this useful paragraph that I took from Geron's book:

Suppose you want to tackle a complex task for which you don't have much labeled training data [...] If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network [...] Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN's discriminator, add the output layer for your task on top, and fine tune the final network using supervised learning (i.e. the label training examples).

It is this technique that Geoffrey Hinton and his team used in 2006 and which led to the revival of neural network and the success of Deep Learning.

[ p. 349, 2nd edition. ]

(Best book on Machine Learning ever, IMHO.)

This unsupervised pretraining is the only way to use unsupervised models for NER that I can think of.

Good luck with your task!"
Sentiment retriving from text (Russian),"If you are seeking a working solution, I know of an API that supports many languages, including Russian: indico.io Text Analysis sentiment()

>>> import indicoio
>>> indicoio.config.api_key = YOUR_API_KEY
>>> indicoio.sentiment(u""–≠—Ç–æ –∫—Ä—É—Ç–æ, —É–±–∏–≤–∞–µ—Ç!  –•–æ—á—É."", language='ru')
0.6978093435482927
>>> indicoio.sentiment(u""–¢—ã –∫—Ç–æ —Ç–∞–∫–æ–π?  –î–∞–≤–∞–π –¥–æ—Å–≤–∏–¥–∞–Ω–∏—è"", language='ru')
0.13258737684773209


Note that the language parameter is optional

(Obviously it's not a lib, but they offer a Python client and their free tier is generous enough.)

Update: As of Q2 2018, Google and IBM sentiment analysis APIs still do not support Russian."
How to combine sparse text features with user smile for sentiment classification? [closed],"If you are using a RNN and the 5000 features refers to size of your dictionary, I would recommend you consider an embedding of your words, eg word2vec. Each word would then result in a maybe 200-300 - sized vector, to which you can concatenate the single smile feature, and then run that through your RNN.

If that is not doing much, you can run the embedded text through an RNN, and then take the result of that (lets say 10-20 features per time-step) and concatenate the smile feature, and run those through a second RNN. The idea is the first one is a further compression over time, and the second is your 'final' RNN which works at a higher level of abstraction.

In general, this is a classic case of combining a high-dimensional feature set with a low dimensional one, to which there are many approaches, the question being, when do you join the features. Take a look at this paper, which covers a few ideas on multi-modal features aka when and how to merge them (diagram 3.2), though I think it is more instructive than useful since their features are more rich than what you have with the single additional feature.

Another thing to try is to soften the 'smile' feature if it is not so already, aka if it is a 0 or 1, make it a random (0, 0.1) and (0.9, 1) respectively."
How to learn irrelevant words in an information retrieval system?,"If you are working with TF-IDF then it's important to experiment with min_df and max_df parameter. I guess you are on Python since you linked a Python tutorial. Here is the TF-IDF documentation and the related text to the above parameters.

max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.

min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.

You might find several rules of thumb on the web. Some of them suggest using a flat number on the min_df close to 5-7 documents and a percentage on the max_df about 80-85%. Maybe even lower. With this, you will be able to get rid of garbage, misspelt or unwanted tokens. Keep in mind that you need to try different combinations to get the right balance in your model."
why an advanced LSTM model produce the same results as a simpler one?,"If you can achieve high results with a simpler model, it is great news! Always choose simpler models because they may be wrong on fewer things than complex ones (Occam's Razor).

Nothing to be worried about, this can happen.

Obviously, it is always possible that there is an implementation problem, so always make sure your code works better.

Reproducing results from papers is always difficult since many things can differ between your work and theirs:

Code might be different
Data might be different
Package versions may be different
Randomness might be controlled differently
Weights may be initialized differently
Data might be fed into the model differently
Hardware may be different which, especially with GPU can cause slight difference in results"
How can I infer psychological intentions from email corpus using text mining?,"If you have a broad meaning of ""intentions"" in mind, you might be interested in research showing that a person's personality (in the sense of the ""big five"" psychological personality theory) can be inferred with remarkable accuracy from facebook likes. Original research showing this can be found here and here. Obviously, facebook likes are not the same as the textual information you work with, but it may be possible to infer certain likes and dislikes from the text."
Probability or score on how likely an article is motivated by content marketing,"If you have a broad set of testing data, I think this is feasible. I've had luck in getting basic models to identify abstract concepts like someone's politics or real news vs fake news, so I think this could work if people write content marketing different from normal news.

I just used this link in another post, but here is a tutorial in Python of taking unstructured articles, passing them through an NLP pipeline, and classifying them into multiple groups."
Sentence to word similarity,"If you have a list of words for every topic you can indeed try to directly measure the similarity of this list against a sentence, but it's likely that a sentence doesn't always contain one of the topic words so it might not work very well.

A more advanced method would be to obtain a semantic representation for every topic (or topic word) from an external corpus. Any large corpus can be used for that, it doesn't have to be related to your input data. The traditional way to do that is to extract a context vector for every target word by counting the co-occurrences of the target in the corpus: for every occurrence of the target word, take for instance a window of 5 words to its left and 5 words to its right. Then count how many times every context word appears in the window of the target across the whole corpus. This way the final context vector contains the distribution of the context words, i.e. a representation of the meaning of the target word. Comparing this context vector against a sentence is likely to produce a more accurate semantic similarity score.

There are many variants about the exact definition of the context vector: usually stop words are removed but one could also use TF-IDF and/or other kinds of normalization. The more modern version of this method is probably to use word embeddings, but I'm not knowledgeable enough about this."
"Based on transformer, how to improve the text generation results?","If you have a lot of data available to train, you should apply the techniques used in large transformer models, like GPT-2: very deep models (48 layers for the 1.5B parameters), modified initialization, pre-normalization, and reversible tokenization. You could also apply GPT-3's locally banded sparse attention patterns.

If you have very small training data, you can apply the ""unwritten"" aggressive techniques described in this tweet, namely data augmentation, discrete embedding dropout, normal dropout and weight decay, and lots of patient training time.

Update: I feel like the tweet thread I referred to is important, so here are the most relevant tweets:

How can you successfully train transformers on small datasets like PTB and WikiText-2? Are LSTMs better on small datasets? I ran 339 experiments worth 568 GPU hours and came up with some answers. I do not have time to write a blog post, so here a twitter thread instead.

To give a bit background: All this came about by my past frustration with replicating Transformer-XL results on PTB and having very poor results on WikiText-2 (WT2). On WT2, my best model after 200+ experiments was 90ish ppl which is far from standard LSTM baselines (65.8 ppl).

...

The key insight is the following: In the small dataset regime, it is all about dataset augmentation. The analog in computer vision is that you get much better results, particularly on small datasets, if you do certain dataset augmentations. This also regularizes the model.

The most dramatic performance gain comes from discrete embedding dropout: You embed as usual, but now with a probability p you zero the entire word vector. This is akin to masked language modeling but the goal is not to predict the mask ‚Äî just regular LM with uncertain context.

The second most important factor is regular input dropout: You take the embeddings and dropout elements with probability p. This also has a data augmentation effect very similar to dropping out random pixels for images. What is a good way to think about this? 1/2

Remember that we can do King-man+woman=Queen? Now imagine input dropout removes the ""man"" component of ""King"". This forces the model to distribute specific information (gender in this case) into multiple dimensions to improve generalization making it more robust. 2/2

Otherwise, it is a game of further regularization (more dropout + weight decay) and of patience. I can train a good model without these tricks in 15 minutes and get 97 ppl. If I apply all these dropouts the model underfits after 7h of training to 63.4 ppl (better than LSTM).

You can also apply these data augmentation recipes to large datasets, but nobody would like to train for months on WT-103 for a couple of ppl points. In my opinion, techniques that require so much extra compute are more harmful to the community than useful. 1/2

Here the code changes to the public Transformer-XL repo that my results are based on: https://github.com/TimDettmers/transformer-xl/tree/wikitext2

With my changes to the public Transformer-XL repo, you can run this script to get down to 63.4 ppl on WT2: https://github.com/TimDettmers/transformer-xl/blob/wikitext2/pytorch/replicate_wt2.sh"
Softmax in Sentiment analysis,"If you have a text in positive label, and your model think it is positive then the positive probability your model output will be the largest.

If you ask your model which is the second most likely label that you(your model) think this text sample belong to, your model's answer is the class that has the second largest probability in the output, and so on.

In summary, your model rank the class from most likely to less likely to your sample. So the order of the probabilities depend on your model belief, such that the most likely class will has the largest probability and the least likely class will has the least probability.

My question: suppose that we have a piece of text in Positive label. So, do we have to have these probabilities in this order: P(pos) > P(neu) > P(neg)

Not exactly, it depends on your model belief, which depends on how good is your data to express the idea of positive, neutral and negative. But usually when use logistic regression to classify 3 class positive, neutral and negative, people will set a threshold for positive, neutral and negative in the probability range, for example: > 0.7 is positive, in [0.4, 0.7] is neutral and the remaining is negative. By doing this, we implicitly assume that the probabilities are indeed have order as you said. This is because we assume that there is an order between positive, neutral and negative, such that neutral is between positive and negative. But if we are dealing with another problem for example classify dog, cat and fist, then I don't think we can assume the order.

What does it mean when we have them in this order: P(pos) > P(neg) > P(neu)

It means that the model believe the most likely class to your sample is positive, the second most likely is negative, and the least likely is neutral.

Can we conclude anything from this? For example, can we say with confidence that the label is Positive like as before?

In my opinion, the model is confident with its answer, if we choose to believe it, then we can confidently say that the sample's class is positive as before."
Group similar words under one topic and assign them a title,"If you have already selected the keywords you want grouped, why not write a function that finds all occurrences of words in the list and replaces it with your one, core word?

What you're describing sounds like a more advanced version of stemming."
Abstracted text summarisation and generation from weighted keywords,"If you have data with a good score system, I would start with something simple, because using a neural network like Bert might be complex to set up.

Something simple is to take the scores and build a phrase with meaning, for instance: ""solar panel"" + ""rooftop"" + ""environment-friendly"" = ""Rooftop solar panel, with a low environmental impact (less than 8g of carbon/year)"".

You can achieve this using if/then rules and some basic equations if there is numerical values. For example, 0.2 for the environmental impact would be something like (1-0.2)*10 = 8g.

Then you can improve results with a neural network like Bert, but you would need enough data to train it, using different inputs (""0.2,0.6,0.1"") and their associated outputs (-> ""Rooftop solar panel, with a low environmental impact (less than 8g of carbon/year)"") and this train data should be representative enough of most common use cases.

See: https://chriskhanhtran.github.io/posts/extractive-summarization-with-bert/"
ways to represent document by its keyword vectors,"If you have the vectors for your keywords, you can aggregate those to get the document vectors. The simplest(and probably one of the most effective) way is to average out your keyword vectors to form the document vector. Once you have the vectors for each of the documents, you can use similarity measures like cosine similarity to see how close your documents are. A typical example of such a method is this : 

Edit: Another interesting point would be to evaluate how you are getting your word vectors. Please look at pretrained word embeddings like word2vec, Glove or Fast Text."
Transformer XL - understanding paper's illustration,"If you look at the Github code, there is 2xNxD in the Multi head attention function indeed:

class MultiHeadAttn(nn.Module):
    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0, 
                 pre_lnorm=False):
        super(MultiHeadAttn, self).__init__()

        self.n_head = n_head
        self.d_model = d_model
        self.d_head = d_head
        self.dropout = dropout

        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)
        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)

        self.drop = nn.Dropout(dropout)
        self.dropatt = nn.Dropout(dropatt)
        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)

        self.layer_norm = nn.LayerNorm(d_model)

        self.scale = 1 / (d_head ** 0.5)

        self.pre_lnorm = pre_lnorm 


But kv refers to the key AND value vectors.

Source: https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py"
Word2Vec vs. Doc2Vec Word Vectors,"If you only care about word similarity, then apply Occam's Razor and use word2vec. There is no need to increase model complexity if not going to be used.

Also, the quality of embeddings is primarily increased through the size and diversity of the corpus. The algorithm has a much smaller effect on the quality of the embedding."
What is the meaning of two embedding layers in a row?,"If you print the model layers with the code you included in the comments, you obtain this:

GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 1024)
    (wpe): Embedding(2048, 1024)
    ...


That's where the misunderstanding comes from. Having one layer printed after the other does not imply that they are connected. They are not.

The first embedding is the normal token embedding, with 50257 token IDs. The second embedding is the positional encoding, with 2048 positions.

The model you are exploring is based on GPT-2, which is a Transformer decoder. In such an architecture, the text is encoded as discrete tokens and the resulting embedded vectors are summed together with some special vectors called positional encoding/embeddings, which encodes the position of the token in the sequence:

So, the answer: there aren't two embedding layers in a row, it is a misunderstanding. Actually, one of the embedding layers encodes the tokens and the other encodes the token positions, and the resulting vectors are added together."
Why does the transformer positional encoding use both sine and cosine?,"If you read the mentioned answer, I guess you already have the notion of the need for a encoding way to represent the position of the word in the input.

In order not to use a sequence of integers (1, 2, 3, ... n) because of the lack of boundary in the value and the magnitude, a float friendly is preferred. But, just using a limited (0 to 1) option means you need to know the length of the sequence beforehand.

This is why the authors propose a cyclic solution. Sin and cos functions can benefit from this idea and thus are the choice here. But why using both instead of one?

It is not clearly answered in the original paper, but if you go though this article(which I urge you to) the reason is that you can use a linear transformation to go from the two functions, to the same functions with an offset:

And if you wonder why having this property available is useful, if you dig into the comments below the article you will find the explanation you seek:

I suppose that this encoding framework enables the model to attend to relative positions by simply generating a transformation matrix, which only depends on the k. To be clear, I hypothesize that given any input PE(pos), the model can create the attention query matrix Q that targets PE(pos+k) by multiplying the PE(pos) with a weight matrix T (the transformation matrix). The weight matrix T, which could be parameters of a single feed-forward layer, can be learned during the training process.

Basically the model can learn a matrix not dependant on t as part o the trining to generalize all the range of encoding values.

Can we just use one of the other?

You could encode the values of the position, yes. But you would not have the property of having this linear transformation available, which seems to be a very important part for the learning process to succeed."
Resource and useful tips on Transfer Learning in NLP,"If you use pre-trained models on data that are distinct from the data they were originally trained on, it's transfer learning. Your two-class sentence corpus is distinct from the data that the GloVe embeddings were generated on, so this could be considered a form of transfer learning. This might be a helpful explainer for general ideas around pre-training (and why it's a worthy pursuit).

Recent work in the NLP transfer learning space that I'm aware of is ULMFiT by Howard and Ruder of fast.ai, here's the paper if you prefer that. OpenAI also has recent work extending the Transformer model with a unsupervised pre-training, task specific fine-tuning approach.

As for your task, I think it might be helpful to explore research around sentence classification rather than digging deeply into transfer learning. For your purposes, it seems that embeddings are a means to have a reasonable representation of your data rather than prove that Common Crawl (or some other dataset) extends to your corpus.

Hope that helps, good luck!"
How to build recommendation model based on resume and job description?,"If you want a DL approach, I recommend substituting the tf-idf by some kind of word embeddings.

For instance, you can take a pre-trained word embedding model, like glove, and average its outputs both in resume and job description, and then compute cosine similarity. However, I recommend to use a contextual word embedding (BERT-like), as the terms in resumes might be very dependent on the context.

The following article also introduces sentence-bert, which I think is very suited for your problem."
How to find possible subjects for given verb in everyday object domain,"If you want something quick, I think pattern is the best tool for the job. It provides a ready-to-use multilingual parser that you can use in the following way:

import pattern
from pattern.en import parse
s = 'I put water in the vase'
s = parse(s)
print s
# output = I/PRP/B-NP/O put/VBP/B-VP/O water/NN/B-NP/O in/IN/B-PP/B-PNP the/DT/B-NP/I-PNP vase/NN/I-NP/I-PNP


Once you have a string like output above, you only need regex parsing to extract every sequence of tokens whose tags match the sequence [B-NP, B-VP, B-NP].

NP stands for ""noun phrase"" and VP stands for ""verb phrase"". In English, virtually every sequence consisting of a noun phrase, a verb phrase, and a second noun phrase, all in strict adjacency, is a subject-verb-object sequence, so this should give you what you're looking for.

pattern's parser will also be able to handle some non-strict adjacencies (e.g. intervening adverbs and adjectives between the three phrases in the subject-verb-object sequence).

However, pattern is not terribly sophisticated -this will give you some Precision and some Recall, but not terribly high numbers. If you need high-quality parsing, you should try the Stanford parser's Python implementation or spacy.

Hope this helps!"
Natural Language to SQL query,"If you want to tackle the problem from another perspective, with an end to end learning, such that you don't specify ahead of time this large pipeline you've mentioned earlier, all you care about is the mapping between sentences and their corresponding SQL queries.

Tutorials:

How to talk to your database

Papers:

Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning salesforce

Neural Enquirer: Learning to Query Tables in Natural Language

Dataset:

A large annotated semantic parsing corpus for developing natural language interfaces.

Github code:

seq2sql
SQLNet

Also, there are commercial solutions like nlsql"
GloVe vector representation homomorphism question,"If you're asking if the group homomorphism makes the the process symmetric then no it doesn't directly. However, they use the fact that they require a group homomorphism to show that 
w
T
i
w
~
k
=log(
P
ik
)=log(
X
ik
)‚àílog(
X
i
)
ùë§
ùëñ
ùëá
ùë§
~
ùëò
=
ùëô
ùëú
ùëî
(
ùëÉ
ùëñ
ùëò
)
=
ùëô
ùëú
ùëî
(
ùëã
ùëñ
ùëò
)
‚àí
ùëô
ùëú
ùëî
(
ùëã
ùëñ
)
 This nearly gives us symmetry. Finally by adding 
b
~
k
ùëè
~
ùëò
 into the equation you restore symmetry.

So in short 
w
T
i
w
~
k
+
b
i
+
b
~
k
=log(
X
ik
)
ùë§
ùëñ
ùëá
ùë§
~
ùëò
+
ùëè
ùëñ
+
ùëè
~
ùëò
=
ùëô
ùëú
ùëî
(
ùëã
ùëñ
ùëò
)
 is what ensures symmetry, and the group homomorphism is a tool to get there.

Update:

Some more details Essentially, what we want is the ability to peform a label switch. Group homomorphism helps with this process because it perseves a mapping between the 
(R,+)
(
ùëÖ
,
+
)
 and 
(R,x)
(
ùëÖ
,
ùë•
)
.

F((
w
T
i
‚àí
w
T
j
)
w
‚Ä≤
k
)=F(
w
T
i
w
‚Ä≤
k
+(‚àí
w
T
j
w
‚Ä≤
k
))=F(
w
T
i
w
‚Ä≤
k
)
√óF(‚àí
w
T
j
w
‚Ä≤
k
)=F(
w
T
i
)√óF(
w
T
j
w
‚Ä≤
k
)
‚àí1
=
F(
w
T
i
w
‚Ä≤
k
)
F(
w
T
j
w
‚Ä≤
k
)
ùêπ
(
(
ùë§
ùëñ
ùëá
‚àí
ùë§
ùëó
ùëá
)
ùë§
ùëò
‚Ä≤
)
=
ùêπ
(
ùë§
ùëñ
ùëá
ùë§
ùëò
‚Ä≤
+
(
‚àí
ùë§
ùëó
ùëá
ùë§
ùëò
‚Ä≤
)
)
=
ùêπ
(
ùë§
ùëñ
ùëá
ùë§
ùëò
‚Ä≤
)
√ó
ùêπ
(
‚àí
ùë§
ùëó
ùëá
ùë§
ùëò
‚Ä≤
)
=
ùêπ
(
ùë§
ùëñ
ùëá
)
√ó
ùêπ
(
ùë§
ùëó
ùëá
ùë§
ùëò
‚Ä≤
)
‚àí
1
=
ùêπ
(
ùë§
ùëñ
ùëá
ùë§
ùëò
‚Ä≤
)
ùêπ
(
ùë§
ùëó
ùëá
ùë§
ùëò
‚Ä≤
)

The group homomorphism here allows for that to occur. Therefore we can see that by setting 
F(
w
T
i
w
i
k
)=
X
ik
X
i
ùêπ
(
ùë§
ùëñ
ùëá
ùë§
ùëò
ùëñ
)
=
ùëã
ùëñ
ùëò
ùëã
ùëñ

Now finally we can say that 
w
T
i
w
‚Ä≤
k
=log(
P
ik
)=log(
X
ik
)‚àílog(
X
i
).
ùë§
ùëñ
ùëá
ùë§
ùëò
‚Ä≤
=
ùëô
ùëú
ùëî
(
ùëÉ
ùëñ
ùëò
)
=
ùëô
ùëú
ùëî
(
ùëã
ùëñ
ùëò
)
‚àí
ùëô
ùëú
ùëî
(
ùëã
ùëñ
)
.

So as far as your comment, it is the most sensible chocie for their method and of which they buld the core mathematicals to GloVE. Changing it, I imagine wouldn't be a trivial thing. I imagine if you did, much of what is derived, including the loss function would change. But with that said, I imagine there are otherwise to achieve label switching."
How does Alexa utterance parsing work?,"If you're interested more generally in speech understanding or speech-to-text, some approaches to natural language parsing and speech-to-text use recurrent neural networks or Hidden Markov processes for learning, as well as a number of signal-processing algorithms to extract more data from the input stream that just raw audio. Keep in mind people have spent their whole careers on this work, so it's not a good problem to just pick up and run with unless you're a MS/PhD candidate looking for a capstone/dissertation project. Here's the iconic paper from Bell Labs that inspired a lot of the DFA/HMM solutions. I've yet to find a paper that does a good job of explaining how to actually implement the RNN style solutions, but here's one in case you're interested.

It's likely that Alexa uses some combination of these methods, but I doubt you're going to get any good answer out of anyone here. After all, it's an important Amazon project and it's not like their engineers are going to come on Stack Overflow and start giving away trade secrets."
Unsupervised Text Classification with Python: Kmeans,"If you're questions are relatively short and the number of labels are not excessive, I would strongly suggest you take a look at zero-shot classification which can be used to classify data to categories without training a model. Both multi-class and multi-label classification are possible.

Specifically I recommend using a pre-trained language model such as BART MNLI

You can try it out of the box with some of your questions and labels and see how it works: TRY!

From my experience I think that unsupervised approaches won't give the desired results. So if zero-shot classification is no option for you for whatever reason, I would suggest you try something like snorkel to programmatically build some training data."
How should I engineer features for Named Entity Identification task?,"If you're using a regular classification algorithm like SVM, it's not very surprising that the model fails to find any indication in the features if the features consist only of the word and this boolean feature.

This kind of task is usually done with a sequence labelling model like CRF: such models learn indications from the context of the sentences in order to detect entities. For example in the sentence ""X said that ..."" the model can use the next word ""said"" to detect ""X"" as an entity.

In case you want to keep using SVM, you could try to add some context features. This would probably help a bit but it wouldn't work as well as a CRF model."
Binary classification and numerical labels,"If you‚Äôre going to have more than two labels, you need to go with a softmax activation and a loss for multi class classification, ie cross entropy loss.

Also, be cautious for multi-class versus multi-label (below).

Multi-class

One-of-many classification. Each sample can belong to ONE of 
C
ùê∂
 classes. The model will have 
C
ùê∂
 output neurons that can be gathered in a vector 
s
ùë†
 (Scores). The target (ground truth) vector 
t
ùë°
 will be a one-hot vector with a positive class and 
C‚àí1
ùê∂
‚àí
1
 negative classes. This task is treated as a single classification problem of samples in one of 
C
ùê∂
 classes.

Multi-label

Each sample can belong to more than one class. The model will have as well 
C
ùê∂
 output neurons. The target vector 
t
ùë°
 can have more than a positive class, so it will be a vector of 0s and 1s with 
C
ùê∂
 dimensionality. This task is treated as 
C
ùê∂
 different binary 
(C'=2,t'=0¬†or¬†t'=1)
(
ùê∂
‚Ä≤
=
2
,
ùë°
‚Ä≤
=
0
¬†
ùëú
ùëü
¬†
ùë°
‚Ä≤
=
1
)
 and independent classification problems, where each output neuron decides if a sample belongs to a class or not.

source: https://gombru.github.io/2018/05/23/cross_entropy_loss/"
How to prepare data for Named Entity Recognition with BIO annotation?,"If your data always looks like this, there is little reason to use sequence labeling: every token belongs to an entity, so it's just a matter of correctly separating the entities and classifying them. But since the entities are already separated by line breaks, there's no need to train a model to separate them. So in the end you just have to classify the entities by category, and this doesn't require sequence labeling. But even for that, from your example it looks like the skills vs. education entities are already separated, so in the end I'm not sure what you want the model to learn?"
How can I extract numerical information conditional on other information?,"If your data is always as clean as this, you might be able to solve the problem using simple regular expressions. You could simply look for the first number that occurs before the string of interest.

As to the dependency parsing using spacy, this is the output that your sentence would get

There expl are VERB []
are ROOT are VERB [There, people, now, .]
119 nummod people NOUN []
people attr are VERB [119, in]
in prep people NOUN [gym]
the det gym NOUN []
gym pobj in ADP [the]
right advmod now ADV []
now advmod are VERB [right]
. punct are VERB []
100 nummod people NOUN []
people ROOT people NOUN [100, in, ,, 19]
in prep people NOUN [room]
the det room NOUN []
weight compound room NOUN []
room pobj in ADP [the, weight]
, punct people NOUN []
19 appos people NOUN [on]
on prep 19 NUM [treadmills]
treadmills pobj on ADP []

Meaning that you would need to first locate weight room, and then trace back to 100, which in this case might simply be more complicated than using regular expressions."
What are some function/package in R to find similarity of individual words not in the context of sentences?,"If your intent is to find compare similarity in meaning, word2vec is the only appropriate choice. adist measures the edit distance between two words, and cosine similarity compares the similarity of two documents (treating the documents as bags of words). On the other hand, transforming a word into its word2vec embedding captures some of the latent meaning of the word. See this for a detailed explanation of how this is done."
"In the context of natural language processing, can anyone give a concrete example of True Positive, True Negative, False Positive, False Negative?","Imagine a hot news classifier.

True Positive (TP): Reality: a piece of hot news. classifier predicts: hot.

True Negative (TN): Reality: not a piece of hot news. classifier predicts: not hot.

False Positive (FP): Reality: not a piece of hot news. classifier predicts: hot.

False Negative (FN): Reality: a piece of hot news. classifier predicts: not hot."
"What can I do when my test and validation scores are good, but the submission is terrible?","Imho the most likely explanation is that the submission test set doesn't follow the same distribution as the training/validation/test data that you used to train and evaluate the model. In other words the test data that they use to evaluate is not a random sample from the full data, it's a different dataset collected independently, for example at a different period of time. In this hypothesis your model is trained on a particular distribution of topics during a particular period of time, but it doesn't work as well with a different distribution of topics at a different time.

Another possibility is that the dataset you used contains many duplicates, causing data leakage: if the duplicates are found in both the training and test data, this artificially increases the performance. If the final test set that they use doesn't contain any of the training data, the real performance is lower."
How to identify/recognize that a sentence about talks about future?,"import spacy
nlp = spacy.load('en_core_web_trf')

def identify_futuristic(sentence):
    sentence_doc = nlp(sentence)
    if any((token.morph.get('Tense') == [] and
            token.morph.get('VerbForm') == ['Fin'] and 
            token.morph.get('Mood') == [])
           or
           (token.morph.get('Tense') == ['Pres'] and
            token.morph.get('VerbForm') == ['Fin'] and
            token.morph.get('Mood') != ['Ind'])
           for token in sentence_doc):

        return 'F'
    else:
        return 'NF'

sentences_dataframe[""F/NF""] = sentences_dataframe['Sentences'].apply(
    lambda s: identify_futuristic(s))"
How to approach Multilingual Text Classification?,"In a similar situation, -after trying some alternatives- I had to build a language classifier in front of all learning and classification steps.

That is, for learning:

Detect the language of the input (say, an enumeration like ""DE"", ""EN"", etc.)
Apply language specific stemming to the words of the input.
Prepend words in the learning phase with the language identifier (i.e. ""de_du"", ""en_you"")
Use these words in a single training model.

In classification phase, use the same steps."
Interpreting the RGB or HEX value from a description of the color using NLP,"In fact, you want to translate ""yellow that is glossy and sorta dark"" by (170,173,11).

A good way to solve this, is by using a neural machine translation model.

Therefore, you can use a encoder/decoder system like many translation models, but with 3 digits as output. To achieve this, you will want to have training data with plenty of text to color translation, but they don't need to be too exhaustive if you cover enough scenarios. The output sequence could be 3 digits 170 173 11. The neural machine translation should adapt itself to the different results automatically, and make good predictions because we have a similar sequence structure as any language.

You have plenty of neural machine translation code examples, here is one among others: https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/"
Should I keep common stop-words when preprocessing for word embedding?,"In general stop-words can be omitted since they do not contain any useful information about the content of your sentence or document.

The intuition behind that is that stop-words are the most common words in a language and occur in every document independent of the context. Therefore they contain no valuable information which could hint to the content of the document."
Text classification into thousands of classes,"In general this doesn't work well, since it's almost unavoidable that the classifier won't be able to distinguish all the categories from each other. I'd suggest tying to reduce the number of categories (for instance discard the least common ones).

In any case I'm not aware of any specific model to deal with a high number of classes, it's regular text classification. I'd suggest to start with a robust method such as decision trees, but there are many options."
NLP Emotion Detection - Model fails to learn to recognize negations,"In general this is a difficult problem, it's about the problem of Natural Language Understanding which is far from being solved.

The advanced option requires a full syntactic parsing of the sentence, ideally followed by some kind of semantic representation of the sentence, for example by extracting relations. As far as I know this is rarely used because these steps will often cause as many errors as they solve.

Some more reasonable heuristics can be considered, for instance detecting specific negation words and either including this information as feature or modifying the original features accordingly (e.g. when a negation is detected replace the token ""happy"" with ""not(happy)"" in the features).

Note that it's unlikely to be perfect anyway, due to the usual obstacles: hedging (""I would assume that it was quite good""), sarcasm (""Sure, I was very happy with the terrible service""), metaphors (""I was feeling like a fish in water""), etc."
How to expand lists?,"In general this is related to syntactic analysis: one needs to obtain a parse tree of the noun phrase, then it's possible to expand by mapping the head of the phrase with the different parts of the conjunction.

I think you can find dependency parsers for German, for instance in the NLTK library or Spacy.

I don't know if you would find a library which provides precisely the expansion though, I would expect that there is a bit of programming to do from the parse tree."
Which classification algorithms to try for classifying text data into 300 categories,"In general, a decent starting point for problems like these is Naive Bayes (NB) classification using a simple bag of words model. Here are some slides describing NB as applied to natural language processing. There's nothing especially fancy about this approach, but it's pretty easy to implement and will give you a starting point to expand from.

Once you've found some initial results assuming independence among your features and your output labels, you'll probably have a better sense of where the model is weak. From that point forward you can apply some feature engineering (maybe TF-IDF) as well as some post processing to deal with samples that get assigned to related categories."
"Should I open abbreviations/acronyms in the text data, when training transformer model?","In general, it is usual not to do any preprocessing of the text. This, however, depends heavily on your specific case. I would suggest not doing any preprocessing but evaluating the results on the aspects that are important to you and, if you detect any problem that may be solved with some preprocessing step, train a new model with it and compare it with the first model."
"Initial embeddings for unknown, padding?","In my experience, what works well is :

For padding, fill a zero vector embedding (as pixel intensity in image data padding) is the only and best solution.
For words that don't have a pre-trained embedding, you should try to train them: as you do, fill them with random values when initializing, but set them to trainable."
Clustering text data based on sentiment?,"In my opinion there are two main problems with your approach:

The clustering is extremely unlikely to correspond to sentiment, unless the features that you use for clustering are specifically engineered to represent sentiment. In general text clustering tend to group documents by common words, i.e. similar topic. This might lead to different categories of reviews by type of product, for example.
The second and I think most important issue is that without any labelled data, you can't evaluate the system. A common mistake would be to use the classes obtained from the clustering in order to evaluate the classification model: this doesn't evaluate the full task of sentiment analysis since there's no way to know how well the clustering represents sentiment. The proper method is to manually annotate a random subset of documents for the purpose of evaluation.

Also in general the second part with the classification model is not needed because the unsupervised clustering model can directly be applied to new instances."
How to validate a clustering model without a ground truth?,"In my opinion there are two ways:

Ask a few experts to assess the quality of the clusters based on a sample (after the clustering has been done, much easier than pre-annotating the whole data especially in the case of clustering)
If the clustering is done in the perspective of using the result in another task, the performance of this other task will reflect the quality of the clustering.

Imho any measure based on the distance between clusters or other technical measure would be a flawed evaluation, because it would depend on the quality of the representation. Such measures might provide some useful indications though, just not a proper evaluation for the task."
How to feed a Knowledge Base into Language Models?,"In my opinion this is a very difficult question, and it's not sure that this can be done.

Symbolic methods and statistical methods are hard to combine. In fact, statistical ML methods became mainstream because they could solve most problems better than symbolic methods. This is especially true in NLP: the multiple attempts at rule-based representations of languages (in the 80s and 90s) were not only expensive to build but also they never proved capable of covering the full diversity of natural language.

There have been various attempt at hybrid models in specific tasks, but to my knowledge none of these hybrid methods proved good enough compared to pure statistical methods. What can work however is to introduce knowledge represented by resources as some of the features used by a statistical model. In this case the model is not symbolic at all, but it uses information coming from symbolic resources.

also get enhanced through a new commonsense understanding of our physical world

Be careful not to assume that any of these models understands anything at all. Their result can be extremely convincing, but these are not strong AI. Natural Language understanding is far from achieved (and it may never be). You might be able to somehow use symbolic resources in order to enhance the output of a model, but making such a model perform some actual reasoning about what it's talking about is a whole other story (a sci-fi one, for now at least)."
What is the right processing order when working with a dataset that already consists of test and train data?,"In my opinion, it's simpler to always assume that the model is meant to be applied in the future on some unknown data:

In theory, this is always the goal of training a supervised ML model. Even if this is done only for educational purposes, it makes sense to practice with realistic constraints.
Practically, this requires to design the system in a way such that any required preprocessing can be re-run with new data. Doing things this way prevents any risk of data leakage and ensures that we properly evaluate the test set.

Of course it's common to apply some steps on the whole dataset to save time, but the proper way is always to split first (if needed) and work only with the training set until the step of evaluation, considering the test set like as unknown future data.

The preprocessing steps should be coded in a function which can be applied to any subset of data.
There's no need to study the distribution of the evaluation or test set:
in general, these sets should have the same distribution as the training set, so they don't bring any new information.
occasionally the test set is purposefully built with a slightly different distribution because this corresponds to a realistic application: in this case it would clearly be a mistake to use this information before training (data leakage), since the goal is to evaluate the robustness of the model.
K-fold cross validation would be a different setting, since it relies on evaluating with different subsets of data. For example you could use CV on the training set in order to tune some hyper-parameters, and later evaluate the final model with its best parameters on the test set."
meaning of fine-tuning in nlp task,"In my understanding, when you are fine-tuning for any task you use additional data (not used during pre-training) and those examples will change the weights on lower levels so that your model is better prepared for the context in which you will use it. A good example is a Twitter sentiment classifier."
How to structure unstructured data,"In Natural Language Processing it's crucial to choose the representation of the data and the design of the system based on the intended task, there is no generic method to represent text data which fits every application. This is not a simple technical problem, it's an important part of designing the system.

The simplest method to structure text data is to represent the sentence or document as a bag of words (BoW), i.e. a set containing all the tokens in the sentence or document. Such a set can be represented with One-Hot-Encoding (OHT) over the full vocabulary (all the words in all the documents) in order to obtain structured data (features). Many preprocessing variants can be applied: remove stop words, replace words with their lemma, filter out rare words, etc. (don't neglect them, these preprocessing options can have a huge impact on performance).

Despite their simplicity, BoW models usually preserve the semantic information of the document reasonably well. However they cannot handle any complex linguistic structure: negations, multiword expressions, etc."
How to fix these vanishing gradients?,"In order to fix the problem of vanishing gradients, you can use Xavier Initilization. Also, the implementation of Xavier Initialization in tensorflow can be done by following this thread."
How to improve language model ex: BERT on unseen text in training?,"In order to make your model more robust to different wordings, you may try with data augmentation techniques, that is, creating variations of your sentences and adding them to the training set with the same label as the original sentence.

There are frameworks like TextAttack that offer several text augmentation techniques. Another option is using back-translation (i.e. translating your sentence into a second language and then translating that again into English), either locally with publicly available machine translation models or via some API like google translate.

Note that making fine-tuned language models resistant to this kind of (common) problems is an active area of research. For the latest advances, you can check this NeurIPS'21 article."
Complete a Hungarian stem to a real word,"In order to preserve the mapping, you will have to store both the original text and the stemmed version in the frequency table. The frequency counts will be on the stemmed version. The display version will the be the set of original tokens associated with a given stem."
Proof that multihead works better than single head in transformer,"In order to see whether one single attention head is enough, we can simply try it. This is precisely what it's done in the article ""Are Sixteen Heads Really Better than One?"" (publised at NeurIPS'2019). The authors conclude that, for some tasks, having multiple heads is needed especially at training time, while at inference time is it possible to prune a number of heads (depending on the task) without significant performance loss."
How to fine-tune hyperameters of unsupervised training in fasttext?,"In order to tune hyperparameters, you'll need an evaluation metric. One evaluation metric for embeddings is performance on analogies (e.g., man is to king as woman is to _____). There is an analogy test set created by Google. You can adjust embedding hyperparameter values and see which ones perform better on that collection of analogies."
Why shouldn't we mask [CLS] and [SEP] in preparing inputs for a MLM?,"In practice, nothing is preventing one from doing what you propose, masking and predicting the [CLS] or the [SEP] token. But the important question is why the model would need to learn about unmasking these tokens.

My understanding is that language models like BERT are pretrained for giving them a better understanding of the language. Then they can be finetuned for any downstream task. But [CLS] and [SEP] tokens are not part of the language and we add them for our convenience. You do not need to learn about them for getting a better ""understanding of the language"". Learning to predict a masked [SEP] token may not bring any additional performance improvement to the models.

[CLS] is used as a representative of the whole input text sequence which is then used for classification tasks usually. It should not be treated the same way as other tokens because it is serving a different purpose. Similar reasoning may be used for [SEP] tokens."
What is a 1D Convolutional Layer in Deep Learning?,"In short, there is nothing special about number of dimensions for convolution. Any dimensionality of convolution could be considered, if it fit a problem.

The number of dimensions is a property of the problem being solved. For example, 1D for audio signals, 2D for images, 3D for movies . . .

Ignoring number of dimensions briefly, the following can be considered strengths of a convolutional neural network (CNN), compared to fully-connected models, when dealing with certain types of data:

The use of shared weights for each location that the convolution processes significantly reduces the number of parameters that need to be learned, compared to the same data processed through fully-connected network.

Shared weights is a form of regularisation.

The structure of a convolutional model makes strong assumptions about local relationships in the data, which when true make it a good fit to the problem.

3.1 Local patterns provide good predictive data (and/or can be usefully combined into more complex predictive patterns in higher layers)

3.2 The types of pattern found in the data can be found in multiple places. Finding the same pattern in a different set of data points is meaningful.

These properties of CNNs are independent of the number of dimensions. One-dimensional CNNs work with patterns in one dimension, and tend to be useful in signal analysis over fixed length signals. They work well for analysis of audio signals, for instance. Also for some natural language processing - although recurrent neural networks, which allow for different sequence lengths, may be a better fit there, especially ones with memory gate arrangements such as LSTM or GRU. Still a CNN can be easier to manage, and you could simply pad the input to be fixed length."
Vocabulary List From word2vec and GloVe,"In short: Yes, you can.

You need to first load the vectors using the Gensim module in Python.

# Load Google news vectors
word2vec_path = ""path_to_the_vectors/GoogleNews-vectors-negative300.bin""
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)

# contains the list of all unique words in pre-trained word2vec vectors
w2v_vocabulary = word2vec.vocab"
What regressors are recommended with text modeling?,"In sklearn Anything that will take sparse data can take output from TFIDF.

In sklearn basically all data any model can take is either dense (normal array) or sparse (that only stores the location of values != 0). You can convert from sparse to dense, but chances are you'll run out of memory if you try.

I'm quite a big fan of using linear algorithms on text data (sgdregressor for example have a ton of different options you can play with). But other algorithms like randomforest that Sergey mentioned, and naive bayes models can also work with that kind of data. Basically what you are looking for is anything that can take sparse input data.

(One thing I've done in the past when working with text+other data is taking output from an sgd's analysis of the data and feeding that + the other data to another algorithm like randomforest. It's a simple and quite powerful method)"
Relative merits of different open source natural language generators,"In terms of open source NLG components, I'm most familiar with Mumble and FUF/SURGE. They've got both similarities and differences, so it's hard to say which is better...

Mumble:

written in Lisp
EPL license
based on tree-adjoining grammar
focuses on linguistic message planning

FUF/SURGE:

written in Lisp
GPL license
based on functional unification grammar
focuses on syntactic realization

Since it sounds like you're interested in abstractive summarization (which is much harder than traditional extractive summarization), I'd recommend the following academic papers:

Text Generation for Abstractive Summarization
Framework for Abstractive Summarization using Text-to-Text Generation
Towards a Framework for Abstractive Summarization of Multimodal Documents -- full disclosure: I'm the author of this one

Also, consider checking out this textbook to get started: Building Natural Language Generation Systems"
Manipulating noise to get some data in right format and apply it to task using PPO,"In terms of process optimization, RL is an excellent option but the environment definition and its policy could be difficult to implement.

That's why a genetic algorithm is a good alternative as it explores thousands of possibilities without having to define a complex environment or policy, overall if the environment is a conceptual one.

I don't know your process, but you can set all its potential sub-functions and assign them numeric weights (with any range) and the genetic algorithm would explore thousands of possibilities by modifying each weight randomly.

The result might not be so good as RL, but much better than a human thanks to the raw compute power.

PyGAD is a python library for Genetic Algorithm that can be applied to many cases: https://pygad.readthedocs.io/en/latest/

https://blog.paperspace.com/genetic-algorithm-applications-using-pygad/

Otherwise, there is a code to implement GA from scratch: https://machinelearningmastery.com/simple-genetic-algorithm-from-scratch-in-python/"
Pros/Cons of stop word removal?,"In the context of sentiment analysis, removing stop words can be problematic if context is affected. For example suppose your stop word corpus includes ‚Äònot‚Äô, which is a negation that can alter the valence of the passage. So you have to be cautious of exactly what is being dropped and what consequences it can have."
Can you detect source language of a translation?,"In the machine translation research community, the translated text that exhibits some traits from the original language is called ""translationese"".

There are multiple lines of research that try to spot translationese (i.e. tell apart text that has been translated, either by human or machine, from text written directly). Here you can see academic articles related to the matter.

However, I have not been able to find research that studies the feasibility of identifying the original source language of the translation, let alone ready-made solutions."
How can I get the output of a Keras LSTM layer?,"In this case, first each of the text in the 'docs' file will be encoded to certain numbers which are in the range of vocabulary size and then the output array will be padded with zeros to make these to the max_length size. If you check the padded output of one single text, it will look like this

array([6, 2, 0, 0])


You have set the vector dimension for the output array as 100. This means each of the elements in the above padded array will be converted to 100 dimensions. Now you are defining LSTM neural network with keras. If you check the output shape, it will give an array of size (10, 4, 100). This means 10 input samples having length 4 has converted to 100 dimensions.

Finally, after fitting the model with padded_docs as input and labels as target variable, you can predict on some new doc file which should be converted to padded_docs format. Only then the LSTM layer can predict with the trained model. The predicted output will give you values in the range (0,1) but not exactly 0 or 1. Below output shows values like 0.9, 0.8 etc.

array([[9.9962938e-01],
   [8.5913503e-01],
   [9.9966836e-01],
   [9.9902046e-01],
   [5.9763002e-01],
   [5.9763002e-01],
   [2.4047494e-04],
   [4.7051907e-04],
   [6.3633323e-03],
   [3.6294390e-05]], dtype=float32)


Hope it gives some lights to the problem"
Can word embedding be used for text classification on a mix of English and non-English text?,"In this case, I think it should be fine to use word embedding on both languages since word embedding learns the meaning of individual words regardless of languages...Is this reasonable? Or maybe I do need to separate the languages and build different models for each language?

If I think logically you are correct. Word embedding is merely a collection of Tokens, which derived its features on the basis of nearby words in a sentence. So if you have sufficient raw data(mix of both), I think its good to go, though results will explain you more :).

However its good to see how such models will behave in case we have mix of LeftToRight(LTR) and RTL languages."
Organization of layers in Keras for a NLP problem,"In this kind of problems, one needs to be sure what model he/she wants to use. A convolution network would be great with final softmax layer giving 8 outputs. you don't need a bidirectional layer in your network architecture. Try a simple conv network and make sure your data pipeline is working then you can add more layers and make it complex."
Word vectors to Sentence Vectors,"In this paper a state of the art method (unsupervised smoothed inverse frequency) is described, you can find an implementation of this method here."
How to implement LSTM using Doc2Vec vectors to get representation?,"Inputs of LSTMs are vectors and it does really matter what the vectors are, in signal processing there are signal windows, in NLP, these are usually word embeddings, here, they are document representations obtained from Doc2vec. Judging from the scheme, they probably use Doc2vec to obtain sentence embeddings. It is rather unusual, but definitely possible.

If the input vectors carry enough information, LSTM will certainly learn a reasonable representation."
Word2Vec for Named Entity Recognition,"Instead of ""recursive neural nets with back propagation"" you might consider the approach used by Frantzi, et. al. at National Centre for Text Mining (NaCTeM) at University of Manchester for Termine (see: this and this) Instead of deep neural nets, they ""combine linguistic and statistical information""."
"""Rare words"" on vocabulary","Instead of dropping rare words or incorporating them risking their scarcity in the training data leads to poor predictions, you can opt for a third alternative: using a subword vocabulary.

You can use approaches like byte-pair encoding (BPE) to extract a subword vocabulary, that removes the out-of-vocabulary word problem and reduces data sparsity in general. There is the canonical python implementation as well as the popular implementation by Google called sentencepiece."
How to classify text and predict if it belongs to the group or not?,"Interesting problem. The 99 x 3000 = 297,000 possible classes (if I understood you correctly) makes this tricky with normal deep learning approaches: the softmax layer at the end gets very large.

If you want to go down that road, Amazon recommender systems have this issue, when trying to suggest which of their millions of project; see https://pytorch.org/blog/introducing-torchrec/ as a good starting point.

You could also treat it as a fraud detection machine learning problem (google for those keywords will provide plenty of leads).

I'd do a variation on that idea, where a few typos are considered to be fine, but anything more is ""fraud"". I am going to assume you have an ideal string for each category. So ""Hiring of Vehicles"" is the desired string for 110000032. (If not, an SQL query can give you the most common value, can't it.)

For each category, calculate the edit distance from the ideal value. And write that back in as a new column in your database.

(It looks like this can be done from inside Postgres; notice that levenshtein_less_equal is a more efficient version that is ideal for you, as you don't need to know the actual distance when it is high, you only care about if it needs a lot of edits or not.)

You can now define false entries as any over a certain edit distance, all from within the comfort of SQL. Those below an edit distance of say 3 could be UPDATEd, if you like your data clean.

Do you also need to automate fixing the remaining ones? The obvious way is to take each bad entry, and get edit distance against the ideal string for each category. That is potentially very slow, but there is a low-hanging fruit speed-up: try just the adjacent digits, and work out from there. So for each bad entry for 110000032, first try against 110000033 and 110000031. As soon as you a low edit distance, assume that was the typo and move it there. I'm betting you only need to try the other 9 last digits, to fix 90% of your bad entries."
Clustering strings inside strings?,"Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:

Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.

Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product if using the sphere). Assume random initialization, and use a gradient-based optimization method by taking the Jacobian of the error.

Now you have a Hilbert space embedding, so cluster using your algorithm of choice!

Response to deleted comment asking how to cluster multiple substrings: The bulk of the complexity lies in the first stage; the calculation of the LCS, so it depends on efficiently you do that. I've had luck with genetic algorithms. Anyway, what you'd do in this case is define a similarity vector rather than a scalar, whose elements are the k-longest pair-wise LCS; see this discussion for algorithms. Then I would define the error by the sum of the errors corresponding to each substring.

Something I did not address is how to choose the dimensionality of the embedding. The word2vec paper might provide some heuristics; see this discussion. I recall they used pretty big spaces, on the order of a 1000 dimensions, but they were optimizing something more complicated, so I suggest you start at R^2 and work your way up. Of course, you will want to use a higher dimensionality for the multiple LCS case."
How to go about training a NER model to extract book citations in free-form?,"Interesting task :)

I think even with a good amount of training data it will be difficult for a regular NER model to perform well with new books titles and authors:

The book may contain persons names which are not authors.
The book titles are difficult to identify as such in general. For example ""the Republic"" might or might not be about the book, and if the only indication the model can use is the capitalization it's probably going to make some errors.

To be clear, I think it could work to some extent but it would probably make quite a lot of errors.

On the other hand you could obtain a database of books, for instance from Wikipedia (there might be better resources), and you could use this in two ways:

Directly identify the books/authors in the documents by simple string matching. I would imagine that even if the coverage of the resource is not perfect, this method would easily catch a majority of occurrences.
In case the above method is not sufficient, it provides you with some good training data from which you could train a NER model in order collect titles which don't exist in the database. Note that there might be issues due to the unknown books being labelled as negative in the training data, so ideally you would have to go manually through the training data and annotate the remaining cases."
Implementing the Dependency Sensitive CNN (DSCNN ) in Keras,"Intermediately I finished the network and it is working great :-) Anyone who is also looking for this network can download it on my Github page.

To refer directly to my questions:

Question 1: The proposed code actually makes partially sense:

One should use the shared-embedding layer to save memory usage, so this part makes sense
In the initially proposed code I used TimeDistributed layer, this is the wrong way for the purpose of the network. We actually have to use a simple shared LSTM layer. Time distributed layers are needed for many-to-many networks. For more information about this, I refer to karpathy's blog and Brownlees blog.
I applied the second LSTM layer before the concatenation, with this I applied two LSTMs to capture the joint meaning of the sentences. That is not the intention of the DSCNN, so we have to apply the one LSTM layer after the concatenation

So far the code looks like:

sentence_inputs = [Input(shape=(max_sentence_len, embedding_dim,), name=""input_"" + str(i))
                   for i in range(max_sentences_per_doc)]

# LSTMs and Average Pooling (sentence-level)
shared_sentence_lstm = LSTM(units=embedding_dim, return_sequences=True, activation='tanh')
shared_average_pooling = AveragePooling1D(pool_size=max_sentence_len)
sentence_modeling = [shared_sentence_lstm(sentence_inputs[i]) for i in range(max_sentences_per_doc)]
sentence_modeling = [shared_average_pooling(sentence_modeling[i]) for i in range(max_sentences_per_doc)]

doc_modeling = Concatenate(axis=1)(sentence_modeling)
doc_modeling = LSTM(units=embedding_dim, activation='tanh', return_sequences=True)(doc_modeling)


The convolutional layer with multiple filter sizes has to be applied ""manually"", therefore we run the convolutions in a loop and concatenate the resulting layers after we flattened them (you could also apply GlobalMaxPooling, then you would not have to flatten the layer, but with this you would lose many information of your feature vectors).

conv_blocks = []
for sz in kernel_sizes:
    conv = Convolution1D(filters=filters,
                         kernel_size=sz,
                         padding=""valid"",
                         activation=""relu"",
                         strides=1)(doc_modeling)
    conv = MaxPooling1D(pool_size=2)(conv)
    conv = Flatten()(conv)
    conv_blocks.append(conv)
doc_modeling = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]

After this we can normally apply an activation layer (e.g. softmax or sigmoid, with or without dropout) and compile the model

Question 2 and 3: We need either an embedding directly in the batch-data or the embedding layer. If you implement your network to have the embedding already in the batch data, you have to generate it only once. It works with small datasets and small embedding sizes (small depends on your memory size etc.), but with large datasets (e.g. 2000 words per document and an embedding dimension of 300) you rapidly exceed like 30gb of memory. Therefore it definitely makes sense to use the embedding layer, since it stores each word vector only once. The downside with this embedding layer is, that it has to perform the word index to word vector replacement after each batch, this costs a little bit of runtime, but since it is only a lookup operation this is not very expensive. So be encouraged to use the embedding layer.

Question 4: It would not. Firstly, because of some parameter issues (e.g. Conv1D accepts only one integer for the kernel_size), secondly, it implements another architecture, that makes no sense (cf. question 1, TimeDistributed layer).

Question 5: No :-), see question 4"
Does word ordering affect monolingual alignment success,"Is it just about Word2Vec or NLP in general?

About Word2Vec, there is this paper that might be interesting:

Relevant Word Order Vectorization for Improved Natural Language Processing in Electronic Healthcare Records

https://arxiv.org/ftp/arxiv/papers/1812/1812.02627.pdf

Then, other ones could answer your question, but not directly. Maybe they could give some interesting clues:

Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders

https://arxiv.org/pdf/2210.05302.pdf

Tag-Aware Document Representation for Research Paper Recommendation

https://arxiv.org/pdf/2209.03660.pdf

Please let me know if it helps, I may have other interesting papers."
How to validate regex based Resume parser efficiently,"Is there any better way to validate this piece of python code. ?

No, because any automatic method would be equivalent to creating another learner, and there would be no way to know if it's good or bad at the job. So unless you find another annotated dataset (i.e. a set of resumes with labels indicating whether or not they switched jobs in the past year), you must create your own annotated set."
Is each form of word classification also considered to be '(named) entity recognition'?,"Is this a fair assumption?

No: Named Entity Recognition (NER) is a specific task which consists in detecting named entities. The more general term for this kind of task in Machine Learning is sequence labelling, because it's not only about classifying words but annotating a sequence of instances in which order matters (e.g. words).

It's true that NER is certainly the most famous task of this kind, but there are other important ones, for instance Part-Of-Speech (POS) tagging."
Fine tuning BERT without pre-training it on domain specific corpus,"Is your corpus big enough? (= several GBs)

If yes, you could train a model from scratch and have good results.

https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6

If not, fine-tuning should be better. You can always try to train it from scratch but you might have sometimes wrong results. Perhaps you can add some training data from similar sources to reach an optimal result.

https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert"
Detect if word is ¬´common English¬ª word or slang word,"It all depends on your definition of what a common word is in your domain. You are using an NLTK corpus which likely doesn't fit your domain very well. Either you have a corpus containing the domain you want and you do a simple lookup. Or you don't know in advance and you need to compute these common words from your documents (your short phrases). In that case, the more sentences you have the better.

An easy way to do this is using pure Python is to use a counter

from collections import Counter

documents = [] # here add your list of documents/phrases

counter = Counter()

for doc in documents:

    words = doc.split() # assuming that words can be split on whitespaces

    counter.update(words)

counter.most_common() # this will return words ranked by their frequency


Then it's up to you to apply a threshold to define what words are common and which aren't. A more advanced approach could be using their TFIDF weights but in that case, you are not necessarily keeping common words, you are keeping ""important"" ones."
how can i create better sequences for a word prediction model,It appears you are padding sequences to be the same length. Another option could be using TensorFlow / Kera's established function - tf.keras.utils.pad_sequences
Character Level Embeddings,"It completely depends on what you're classifying.

Using character embeddings for semantic classification of sentences introduces unnecessary complexity, making the data harder to fit. Although using n-grams would help the model deal with word derivatives.

Classifying words based on their derivative would be a task that would require character embeddings.

If you're asking whether it would be useful to train a model to embed characters like you would with word2vec- then no. And in fact would probably yield bad results. We use embeddings to implicitly encode that two data points are close together and therefore should be treated more similar to the model. The letter 'd' shouldn't be semantically closer to 'e' than 'q'."
BertTokenizer Loading Problem,"It could be due to an internet connection issue, that's why it is always safer to download your model in a local folder first and then load it directly using the absolute path.

In addition to that bert large is about 2Gb.

To download it, you can use this code:

git lfs install
git clone https://huggingface.co/bert-large-uncased


See also: https://huggingface.co/bert-large-uncased"
NLP Library or routine for Quantitative relationships,"It depends on how ""generalized"" you want your methods. If you have known and fixed set of quantitative relationships (=, <=, >=, ‚Ä¶), then you can consider it a classification problem. Given some input string, which category does it most likely belong to? The same goes for entities. Are you assuming it will always be two entities?

If the system should handle an unlimited number of entities with unknown and possibly infinite number of relationships than the problem quickly becomes intractable.

If you assume there will always two entities, then you can apply Resource Description Framework (RDF). RDF goal is to find semantic entities and their relationships. RDF finds triples: a subject, a predicate, and an object. The predicate is ""quantitative relationship"" between the subject and object. There are a variety of frameworks for extracting RDF from a sentence. The most effective ones depend on building a parse tree of the natural language."
Why would you use word embeddings to find similar words?,"It depends on how similarity is defined. If similarity is defined as human-defined semantics, then a synset (i.e., synonym set) is most appropriate. If similarity is defined as frequent co-occurrence, then word embeddings are most appropriate. Even within semantic similarity, there are many approaches beyond synsets.

One advantage of word embeddings over synsets is the ability to automatically find similarity with multi-word term vocabulary. For example, the common word analogy - Man is to king as woman is to queen."
Q&A answer comparison multiple sentences using,"It depends on how sophisticated you want the system to be:

the most basic way is to compare the user answer with the gold answer using a simple string similarity measure, such as the overlap coefficient. Basically it just counts the words in common, and there would be a minimum threshold to count the answer as correct (e.g. 80% words in common). It's not very good because a small typo is enough to make the score wrong and it gives the same importance to every word.
The same idea but with TF-IDF weights, typically with cosine similarity. This requires a corpus on which to calculate the IDF weights (which reflect the importance of the words in general).
Still based on string similarity measures but more advanced: a hybrid similarity measure which combines character-level similarity between words (e.g. Jaro, Levenshtein edit distance) and similarity across words. Soft-TFIDF is a common example. Disadvantage: can be tricky to properly adapt to the task.
Beyond that there are a lot of fancy options: using semantic similarity with WordNet (synonyms), words embeddings, etc.

Note: fyi this is not related to the task called Question Answering, which is about a computer-generated answer to a question."
Using BERT to extract a list of words and phrases from documents,"It depends on the context imho: where does the list of words/phrases come from? Did some expert compile it thinking about every word carefully, or is it just intended as some rough indication of the actual context to be tagged?

Anyway from your description the output must use these particular phrases as columns, so it wouldn't make sense to use embeddings like BERT since the output would not really match the phrases themselves.

I don't see any point in a supervised model: what would be the target variable? All the phrases?? This would probably result in a very complex method for a very simple problem.

It might be disappointing but sometimes the simple answer is the appropriate one: direct search for these phrases, possibly after lemmatization to cover lexicographic variants."
"Text annotating process, quality vs quantity?","It depends on the context which you consider. For example, suppose there is a situation that all possible states can be covered by 10K different texts. It is trivial, if these texts are all annotated, then for 1000 tests, at least we can classify 500 of them truly (as we have two classes, and probability of wrong annotation for each text is at most 0.5).

Now, suppose 1K of 10K texts are annotated. Then, as annotations are exact, we can classify 1/10 of 1000 texts truly (because we have no idea about the other 9K possible states).

Therefore, in this situation quantity is more important than quality.

Also, we can consider these cases, when the possible states are 1K. It could be straightforward to show that in this case (if the power of annotators is same as the former case) quality can be more important than quantity. However, in the most cases this number is not realistic.

In sum, as in the most cases the variety of texts are more than the power of annotators, we prefer quantity to quality, as we can cover more text space and machine can learn more. Although the accuracy can be less, but for two classes classification is negligible."
"In smoothing of n-gram model in NLP, why don't we consider start and end of sentence tokens?","It depends on the definition of vocabulary (V).

Most teaching examples only include words in the vocabulary for simplicity. Start and stop of sentences tags can also included in the vocabulary.

Vocabulary can also include punctuation, or stop words can be removed from the vocabulary."
How to decide to go with BOW or TFIDF,"It depends on the problem you are trying to solve. If you know the signal in the dataset already, the words which decide your decision then go with Bag of Words. This is useful when you are doing something like text classification.

On the other hand, TF-IDF is useful when you don't know the signal in the dataset. If you want to do text similarity, then, this is a good option."
How many examples needed for named entity disambiguation?,"It depends on the signal-to-noise in the dataset. The amount of data to perform named entity disambiguation will depend on the tf-idf score of the occupation and skills, rare occupations and skills will require less data to build a performant model.

For example, that the sentence ""I am a cook that multitasks well."" ""Cook"" is an occupation and ""multitask"" is a relevant skill. In a similar sentence, ""I multitasked while I cooked."" ""Cook"" is no longer an occupation and ""multitask"" is no longer a relevant skill. However, the phrase ""saturation diver"" is less frequent than ""cook"", thus much easier to build a model to identify as an occupation and find relevant skills.

Annotator performance is easier to measure. Cohen's kappa is a common method of judging inter-rater reliability. Again, the number of needed raters depends on their agreement on the task. If task performance is easy, the number of raters and the number of items per rater can be lower. It is best to benchmark your system and then decide how much data you need to raise the benchmark scores.

One way to automatically create ontologies from a text is the TextRank algorithm."
Why is it convolutional 1D is sometimes better and faster that LSTM at classification and predicting tasks?,"It depends on the task and many other things. CNNs and RNNs have different inductive biases.

CNN applies the same function independently to each input slice and therefore it could be done in parallel (hence the higher speed). The drawback is that the output state of CNN only covers a limited input window. You can increase the size of the span from which an output state gets the information, but it will always be limited. For some tasks that are solvable by searching for some typical word patterns in the text, this might be good enough.

LSTMs are a theoretically stronger model and it can in theory learn to model arbitrarily long dependencies between its inputs (unlike the CNN with the limited window size). In practice, this is not often the case because the training signal that you get from the closeby neighbors is much stronger than the long-distance dependencies. Regarding the speed: computing 
n
ùëõ
-th of an RNN requires knowing what the 
(n‚àí1)
(
ùëõ
‚àí
1
)
-th state is, which makes parallelization impossible."
"Which ML algorithm is best works on text data and the reason behind it? Also, which metrics is used for testing performance of model?","It depends on the type of data. Looks like you have a multiclass classification problem, but is it a balanced or imbalanced dataset?

Binary classification dataset can work with almost all kinds of algo's but multiclass classification does not. For example Logistic Regression does not work well with multiclass classification.

Popular algorithms that can be used for multi-class classification include:

k-Nearest Neighbors.

Decision Trees.

Naive Bayes.

Random Forest.

Gradient Boosting.

Algorithms that are designed for binary classification can be adapted for use for multi-class problems. This involves using a strategy of fitting multiple binary classification models for each class vs. all other classes(called one-vs-rest) or one model for each pair of classes (called one-vs-one).

One-vs-Rest: Fit one binary classification model for each class vs. all other classes.

One-vs-One: Fit one binary classification model for each pair of classes.

Binary classification algorithms that can use these strategies for multi-class classification include:

Logistic Regression.

Support Vector Machine.

For metrics you have to be careful. You can use accuracy when the dataset is balanced but using the same for imbalanced would be catastrophic.

For example you have binary classification with the 2 output classes having frequency 10% and 90%. If you choose accuracy as a metric you would get abnormally high value and you would think your model works good. But it is misleading as your model will predict the majority class most of the time, even when the real output is from the minority class.

So it would be wise to use the F1 score (which is nothing but a combination od precision and recall) for imbalanced dataset."
Start & End Tokens in LSTM when making predictions,"It depends on what you use the LSTM for.

For sequence labeling or sequence classification, the special tokens are not necessary. Although, there might be a slight benefit of informing the network of what is the beginning and the end of a sentence, especially if the initial LSMT state is fixed and learned.

For autoregressive sequence-to-sequence models, the special tokens are crucial. The beginning-of-sentence token serves as an instruction to the decoder to start decoding (it needs a very first state to predict what the next first token is). The end-of-sentence token is an instruction for the decoding algorithm to stop generating more tokens."
Using Google Translate API to create a Translation Dataset,"It depends. Google Translate works very good with some pairs of languages, and not good on others. Based on my personal experience, translating from North-European languages (Dutch, Danish, Swedish) to English worked almost perfectly, while English-Italian translation lead to bad results.

You can find a Spanish-English dataset here, this is a very official source.

Additionally, it seems DeepL is a great German <-> English online translator around."
HuggingFace Transformers is giving loss: nan - accuracy: 0.0000e+00,"It is about the warning that you have ""The parameters output_attentions, output_hidden_states and use_cache cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: config=XConfig.from_pretrained('name', output_attentions=True)).""

You might try the following code.

from transformers import BertConfig, BertModel
# Download model and configuration from huggingface.co and cache.
model = BertModel.from_pretrained('bert-base-uncased')
# Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).
model = BertModel.from_pretrained('./test/saved_model/')
# Update configuration during loading.
model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
assert model.config.output_attentions == True"
Why tfidf of one document is not zero?,"It is because, by default sklearn's TF-IDF vectorizer will normalize the results. See the the Tf-IDF Term Weighting section of the User Guide. For your example,

n = 1
tf = 3
df = 1
idf = np.log(n/df)+1 = 1


You have 3 terms with identical frequency. So, the L2 normalized tf-idf is computed as

abs(1)/sqrt(1+1+1) = 0.577"
Avoid leakage in NLP extraction,"It is best practice to split the data into train and test datasets. Make modeling choices only on the train data set. Evaluate the usefulness of those choices on the test dataset.

Traditional NLP extraction techniques follow the same logic because they often have modeling choices. One example is the number of topics in non-negative Matrix Factorization (NMF). It is best practice to choose the number of topics on the training dataset, and then evaluate the quality of those topics on the test dataset.

The same logic holds true when estimating a statistic and then making modeling choices on that statistic. Tf‚Äìidf (term frequency-inverse document frequency) is a common example. It is best practice to estimate tf-idf on the training set only because later modeling choices are made (or not made) based on tf-idf statistics."
"Topic Modeling - n-grams or 1,2,3,...n-grams?","It is best to use 1,2,3,...n-grams. Giving the model more features allows it to better learn patterns in the data. Often a threshold for the number of occurrences is used to filter out infrequent ngrams."
Updating a genism LDA model with new documents and topics,"It is common for data to change over time. One common name is for this phenomena is dataset shift.

There are two steps when dataset shift is possible: detect and update.

Detection can be done manually or automatically. A person can inspect the new data and decide if a new topic is present. The process of detecting a new target can be also be automated. Latent Dirichlet allocation (LDA) is a probabilistic model. Thus if the probability of topic membership is below a threshold, that is evidence there is possibly a new topic.

After the detection of a new topic, the model needs to be updated. The update can be incremental retraining with the new data, frequently called online learning. Or the update can be a complete retraining of the model."
TF-IDF Features vs Embedding Layer,"It is common for TFIDF to be a strong model. People constantly get high places in Kaggle competitions with TFIDF models. Here is a link to the winning solution that used TFIDF as one of its features (1st place Otto product classification). You will most likely get a stronger model if you combine the TFIDF and RNN into one ensemble. Other results from Kaggle:

2nd place: https://www.kaggle.com/c/stumbleupon/discussion/6184
4th place: https://www.kaggle.com/c/avito-demand-prediction/discussion/59881
3rd place: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52762

https://www.kaggle.com/c/avito-demand-prediction/discussion/56897:

A good number of kernels are going the traditional route with CountVectorizer/TF-IDF, and some brave souls (I say brave because training is slower and the results don't seem as spectacular so far) have been experimenting with embeddings, as per the previous competitions."
what is BIO Tags for creating custom NER Named entity recognization?,"It is easy. You need to tag a phrase using B (Begin), I (Interior), and E (End). For example, you want to tag ""United States of America"" as the name of a country. You will tag likes:

United(B_Country) States(I_Country) of(I_Country) America(E_Country)


In the same text if you find ""Islamic Republic of Iran"", you will tag likes:

Islamic(B_Country) Republic(I_Country) of(I_Country) Iran(E_Country)


Also, you will tag ""United Kingdom"" likes:

United(B_Country) Kingdom(E_Country)


Therefore, for every label that you have in the label set, you will have three labels in the tagging that is B_LabelName, I_LabelName, and E_LabelName.

Notice that In some tagging systems, also uses from I_LabelName instead of E_LabelName.

Now, what is ""O""? In some cases the all part of the phrase is not in the label but comming in the middle of the phrase and we need to say it is out (""O"") of the label. For example, in the text analysis we want to take out ""of"" from country names. Hence we will tag ""United States of America"" like the following:

United(B_Country) States(I_Country) of(O_Country) America(E_Country/I_Country)"
How to validate a chatbot?,"It is impossible to answer this question because you have not defined what precisely you want your chatbot to do (and not to do!). Once you have defined that, you have your metric.

So the answer to your second question is: no, there is no standard validation metric."
Is it a red flag that increasing the number of parameters makes the model less able to overfit small amounts of data?,"It is not necessarily a red flag. Of course, without seeing the code, it is impossible to say that for sure.

What is likely happening here is that adding parameters to your model, it makes it harder for it to converge to some minimum. More parameters roughly mean that your model is able to explain ""more complex stuff"". And since you have a small amount of data, the explanation should remain rather simple. Therefore, your model is trying to explain something simple in a complicated way, and it might not be easy to do so.

Also, are you using dropout or regularization? If yes, this might also be an issue as these are explicitly use to avoid overfitting."
How much text is enough to train a good embedding model?,"It is not the number of many articles that matter but the total number of words.

Enough ""meaningful/good"" is an empirical question that depends on the dataset. One way to test the results of a newly trained model is the Google analogy test set which compares a new model's predicted word to established embedding benchmarks.

As far as minimum number examples needed for each unique token in vocabulary, the general consensus is there should be at least 40 examples per token. If there are fewer than 40 examples for a token, the vector estimates can be unstable and the token should be dropped from training."
The meaning of random word dropout in NLP,"It is not uncommon that we can make sense of a sentence without reading it completely. Or when you are having a quick look at a document, you tend to oversee some words and still understand the main point. This is the intuition behind the word dropout.

Generally this is done by randomly dropping each word in a sequence following for example a Bernoulli distribution:

X‚ÜêX‚äô
e
‚Éó¬†
,
e
‚Éó¬†
‚àºB(n,p)
ùëã
‚Üê
ùëã
‚äô
ùëí
‚Üí
,
ùëí
‚Üí
‚àº
ùêµ
(
ùëõ
,
ùëù
)

where X is the index of the word token, n is the lenth of the sequence, and 
e
‚Éó¬†
ùëí
‚Üí
 is a vector with each word dropout state.

This is usually done after calculating the word embeddings, and the words selected to be left out are normally changed to the <UNK> equivalent embedding.

By doing this, we allow out model to learn more flexible ways of writing/convey meaning."
Entity Embeddings of email address,"It is possible but you would need lot of training data to reach a good result, because there is a wide variety of family and company names.

Fortunatelly, there could be an efficient solution to make a good classification.

My advice is to focus on human names recognition on one side, company name recognition on the other side, and then apply ML.

For human names, there are plenty of datasets available to recognize family names and first names that you can filter in the fields (ex: Gupta is recognized in ""guptamols"" => Name).

For company names, you can use dictionaries in english or any other language to detect lot of names (ex: textile is recognized in AgraTextile).

Once you do this safe classification, you would have lot of valuable labelled data, by which a NLP model (like Bert - I would recommend a byte per byte embedding as there could be special characters in companies) could learn patterns in order to classify the rest of the unknown data easily.

Note: Such models give a probability chance for each case that could be useful to limit the risk of wrong classification."
What is the typical accuracy of masked language models during BERT pretraining?,"It is very hard to tell and researchers usually don't measure it because it's not a comparable number such as GLUE score. It depends on size of your vocabulary and also the masking strategy (for example newer BERT successors use span-masking). BERT is a subword language model (uses WordPiece tokenizer) so if you use large vocab then sentences are usually tokenized to ""smaller"" subword units that are easier to predict. I trained a BERT-like model for Czech language, which is morphologically more complex than English, and got top@1 test accuracy ~0.55, top@3 test accuracy ~0.68 after 1.5M steps with 30K vocab size."
Why is T test reweighting on a word X word co-occurrence matrix so effective?,"IT is very similiar to PMI, here you just expand it to the whole dictionary matrix (matrix representation of the whole vocabulary), normalize it by subtracting quantitive representation of the sum of words found in row i column j and than standardize. (Like when using sklearn Standardize(), similiar atleast)

Intuition? Well why is tf-idf working (generally as text quantification technique), you are focusing on essential n-grams and minimising away the rest, with this re-weighting you are getting close to tf-idf representation in a sence."
Extract information using NLP and store it in csv file,"It looks like you try everything but didn't design the system so that it does what you need it to do. In this task I don't see any reason to use things like LDA for instance. In my opinion this is a typical case for training a custom NE system which extracts specifically the targets you want. The first step is to annotate a subset of your data, for example like this:

Please   _
book     _
a        _
cab      _
from     _
airport  FROM_B
to       _
hauz     TO_B
khaas    TO_I
at       _
3        TIME_B
PM       TIME_I


A NE model is trained from such annotated data. Here I proposed an option with labels by category plus B for Begin, I for Inside, but there can be many variants.

Once the model is trained, applying to any unlabelled text should directly give you the target information."
Finding repeating string patterns in thousands of files,"It looks like you want new word discover ?

Because thousand is not a big deal

Just build ngrams of filenames , count them would be fine .

You can use Trie to store string counts, reduce memory cost , I can provide a dict way(in python):

from collections import defaultdict, Counter

# for memory effcient, you would need trie here
# t = Trie()
t = defaultdict(int)



filenames = [""sjkghkjfs <data> skjfs <data> kjskdfjsfkjs <data> sahkj"", 
            ""tretyer erytewr fskjdf <data> trjk"",
            ""....""]

# some preprocess, tokenize to  sentence , to  words , filter useless one

def ngrams(s, start, end):
    for i in range(start, end+1):
        if len(s) > i:
            for j in range(i, len(s)):
                yield s[j:j+i]

# Suppose you need same string with length 4 ~ 6
for s in filenames:
    for word in ngrams(s, 4, 7):
        t[word]+=1

# Suppose you need most_common 5
print(Counter(t).most_common(5))

Further more

you can

tokenize the filenames at first.
Calculate tf-idf or word entropy to remove the useless words
then do something like above .
And

If you have a group of traget strings , word2vec may be a good tool, you can use it to search the strings in similar domain."
Building a graph out of a large text corpus,"It looks to me like topic modeling methods would be a good candidate for this problem. This option has several advantages: it's very standard with many libraries available, and it's very efficient (at least the standard LDA method) compared to calculating pairwise similarity between documents.

A topic model is made of:

a set of topics, represented as a probability distribution over the words. This is typically used to represent each topic as a list of top representative words.
for each document, a distribution over topics. This can be used to assign the most likely topic and consider the clusters of documents by topic, but it's also possible to use some subtle similarity between the distribution.

The typical difficulty with LDA is picking the number of topics. A better and less known alternative is HDP, which infers the number of topics itself. It's less standard but there are a few implementations (like this one) apparently. There are also more recent neural topic models using embeddings (for example ETM).

Update

Actually I'm not really convinced by the idea to convert the data into a graph: unless there is a specific goal to this, analyzing the graph version of a large amount of text data is not necessarily simpler. In particular it should be noted that any form of clustering on the graph is unlikely (in general) to produce better results than topic modelling: the latter produces a probabilistic clustering based on the words in the documents, and this usually offers a quite good way to summarize and group the documents.

In any case, it would possible to produce a graph based on the distribution over topics by document (this is the most natural way, there might be others). Calculating a pairwise similarity between these distributions would represent closely related pairs of documents with a high-weight edge and conversely. Naturally a threshold can be used to remove edges corresponding to low similarity edges."
Mining timelines in a long text,"It looks to me like what you propose makes sense, but there has been some research done around these questions of time representation already. I'd suggest you check the state of the art in this domain, if only not to reinvent the wheel or miss important cases.

I'm not very knowledgeable about it but I can at least point you to TimeML and the related publications. There are certainly other recent works building on TimeML, for example this one (disclaimer: I know the author)."
Does a precision score increasing with a higher number of folds mean the model will improve with more data?,"It may or may not indicate that more data will help.

The correct way to see if model performance increases with more data is to create a learning curve. Train your model on a small subset of your data with cross-validation, and then keep increasing the amount of data used. This is not just reducing/increasing the number of folds. It is actually limiting yourself to a small amount of data for your 10-fold cross-validation and increasing it, evaluating the performance at each increase. Here's more about a learning curve."
"What does it exactly mean by ""different representation subspaces"" in transformer?","It means that, because each attention head has its own projection, it can learn to capture different aspects of the sequences. For instance, one sentence may focus on negation, while another head may focus on coreference resolution.

While these examples serve as illustrations of the concept, they are probably not happening in an actual trained Transformer model, because they are basically bound to human interpretation of language and Transformers are probably not bound by them, especially taking into account that most Transformer models use sub-word tokens instead of word-level tokens, which limit the applicability of word-centered interpretation of language."
Types of averages when analyzing sentences,"It might depend on what you will then use the scores for. For instance, should one long sentence score higher than two shorter sentences even if all three sentences have the same density of technical words? If so, adding rather than averaging the scores? Or adding, then doing an adjustment for sentence length.

The other way to get the more technical words to have more weight, when you take the mean, is to raise their score to a power. The power becomes a hyperparameter you can tune for, but simply squaring scores would be enough to test the idea.

I'll also mention https://en.wikipedia.org/wiki/Tf%E2%80%93idf, in case you were not aware of it; the see also section might also bring up some ideas."
Grouping company information,"It seems like you want to want to first get the datasets together using as shared key which in this case is the company name. That does not sound like a modelling problem but a data wrangling problem.

Assuming that each datasets are in csv, then get them to pandas dataframe:

df_a = pd.read_csv('data_set_a.csv')


Then assuming you have the datasets in df_a, df_b, df_c and each have a column 'name':

df_a.name = df_a.name.str.lower()
df_a.name = [i.split()[0] for i in df_a.name]


Now you will have a dataset where all three are 'facebook'. Then the three dataframes can be merged in to one:

df = pd.merge(df_a, df_b, left_on='name', right_on='name')
df = pd.merge(df, df_c, left_on='name', right_on='name')


And now you have the three different datasets merged, and ready for analysis."
"""Change the features of a CNN into a grid to fed into RNN Encoder?"" What is meant by that?","It seems they use a shared RNN which process each row sequentially on the sequence of concatenated channels of individual pixels. From the paper

Implementation with channels last

Let the output of the ConvNet be of size (batch_size, height, width, channels). The RNN expects an input of size (batch_size, sequence_length, input_size)`. So you have to reshape it with the following correspondence.

batch_size*height -> batch_size
channels -> input_size
width -> sequence_length


And process each row (along height dimension) with the same RNN and concatenate the result.

To do that, we simply reshape to merge the batch and height axis into one dimension so that our RNN will process columns independantly.

rnn_input = keras.layers.Reshape((batch_size*height, width, channels))(convnet_output)
rnn_output = keras.layers.RNN(hidden_dim, return_sequences=True)(rnn_input)


rnn_output will have shape (batch_size*height, width, hidden_dim). You can then combine this tensor into a context vector using a dense layer with tanh activation, as it is written in the paper.

The paper also use trainable initial state for the RNN, you might be interested in this library to implement it.

Implementation with channels first

If you set your Conv2D layer with ""channels_first"", the output convnet_output will be of size (batch_size, channels, height, width). Therefore you need first to permute the dimensions before reshaping.

convnet_output = keras.layers.Permute((0, 2, 3, 1))(convnet_output)


After this step, convnet_output has dimension (batch_size, height, width, channels). You can then proceed as previously, reshaping and feeding to the RNN.

rnn_input = keras.layers.Reshape((batch_size*height, width, channels))(convnet_output)
rnn_output = keras.layers.RNN(hidden_dim, return_sequences=True)(rnn_input)"
How to cluster components of a graph containing text data?,"It seems to me that you're trying to use different methods at the same time: if your graph already contains a similarity value on the edges, then it would be redundant to use some form of topic modelling or text similarity again.

In my opinion, you either:

you assume that the text similarity values are reliable and exploit them. This is the simplest option, all you need is a generic graph clustering algorithm or a distance-based clustering algorithm.
you want to redo the processing of the text: in this case you might as well as restart from the set of titles, i.e. ignore the current graph. This leaves you with many options for topic modelling methods, from which the clusters can be derived."
How can I figure out section headings in a document?,It seems you are lucky and the section headings are XML-tagged. So you can use XSLT transformations or Python lxml.etree to get rid of them or to extract them.
How to collect info about unseen bugs given user's comments/feedbacks? [closed],"It sounds like you are looking for an unsupervised learning approach (meaning you don't need to manually label your data).

Something like k-means clustering could work well. This would allow you to group you comments into k distinct clusters. You could then view counts of comments in those clusters and explore the clusters to determine their meaning.

In order to perform the clustering, you need to transform your data from text to a numerical vector space. A common approach would be tf-idf, but you may find that something else works better.

Since you mentioned Python, both k-means and tf-idf can be accomplished using sklearn:

sklearn.cluster.KMeans
sklearn.feature_extraction.text.TfidfVectorizer

There's a pretty nice example of k-means clustering using tf-idf on Kaggle."
Pre-trained models,"It would be very convenient but I'm not aware of any such site.

Besides, it would be quite difficult to agree on what is the current best performing model in general, as this depends on the dataset, how it's been annotated and the evaluation method. Not to mention the multiplicity of languages, since a particular model is usually language specific. And of course it would be difficult to keep up with new methods and datasets being published constantly."
Build train data set for natural language text classification?,"It would help to do some analysis of the scripts to identify aspects that distinguish the various categories. Once you do this manually for some examples, you could consider writing some rules based on the observations. The rest of the examples can be labeled using the rules. For a model-based approach, if you label a small set of examples (~50), then a simple model (Naive Bayes, etc.) can potentially be trained on these."
Classification of a free text field to determine which product,"It's a quite complex problem, depending on the different possible types of constraints.

As far as I know the problem would usually be decomposed into two parts:

The extraction of the constraints from the text, which results in a set of constraints expressed in some predefined formal language.
The application of the constraints to the database. This is the simple part: it's essentially building an SQL query or similar.

Of course part 1 is the NLP complex part, it can itself be decomposed into several parts:

Locating a constraint in the text and detecting the type of constraint. This could be designed as sequence labeling (e.g. named entity recognition)
Extracting the variable elements, for example the weight and the relation ""over"". Then this is mapped to a formal expression in the predefined language. Note that this can get difficult if the text references a previous information like ""the upper weight"", this would involve coreference resolution (as far as I know this rarely work perfectly in general).

The whole problem is similar to relation extraction, so there might some similar problems or even systems which could help doing it, but it would certainly require adaptation to the specific case.

Also note that if the text descriptions don't have too much diversity, it's possible that some simple pattern matching would suffice to capture most of the cases. For example one can imagine designing this as an hybrid system which first tries a set of predefined simple patterns, then if it doesn't find a match attempts a more general method."
"How does ""A Neural Probabilistic Language Model"" learn good word vectors?","It's because during the training, it is not only the neural network with tanh that learns but also the words' representation in Matrix C.

There are 2 different parts in this model:

The classic neural network can make correlations between words sequentially.
The words' representation can make correlations between words in general. Without words' representation, the model couldn't make unknown correlations that haven't been learned previously. They've explained it in the paper with the words ""cat"" and ""dog"". This is obtained progressively by building a map of probability between words.

For instance: The sentences

""The cat is walking in the room."" and

""The dog is running in the bedroom.""

would increase the neighbor vectors (ex: ""walking"", ""running"", ""room"", ""bedroom"") in Matrix C of both ""cat"" and ""dog"", thanks to the phrases' similarity.

In other words, if we increase the vector of ""cat""/""walking"", it would also increase the vectors of ""cat""/""running"" and ""dog""/""walking"" in a similar context, thanks to an embedding lookup.

We could define the words' representation as a general probability space of all words, which is slightly modified at each learning iteration.

Explanation from Yoshua B (great thanks):

The reason why similar words end up having similar word embeddings is because of the smoothness of the neural net that takes these word embeddings in input. If ""The cat is walking in the --- "" can be completed by ""room"", it is also true when we replace ""cat"" by ""dog"", which puts pressure on both words to have similar word embeddings. Small change of the embeddings = small change in the output probabilities.

If you think about it, the architecture is very similar to a 1-D convolutional neural network:

the matrix C corresponds to a usual dot-product neural operation when the input is a one-hot vector for the word at each position (with a 1 at the position corresponding to the word symbol)
using the same matrix C at every position makes sense and proceeds of the same inductive bias as in 1-D convolutional neural networks: the meaning of a word is position invariant (i.e. if we only know that a word appeared at position 3 vs 4, the meaning does not change).

The idea of using such layers, with shared weights across different positions, is found not just in 1-D convnets and time-delay neural net (which pre-existed the NNLM, and which I worked on in my 1991 PhD thesis), but also in neural nets operating on symbols (which were explored among others by Geoff Hinton and his student Paccanaro a few years earlier, cited in the paper)."
mathematical accurate definition of the binary independence model,"It's better to talk about x, q and R as (random) events - set of outcomes of the random experiment. x and q will be one element sets but R is an event denoting x is relevant to q and thus it is a subset of cartesian product X x Q (pair of a document and a query).

Comma is then set conjunction and P(A,B) = P(A ^ B) which equals to P(A) * P(B) when A is independent of B.

The wiki statement 'by using Bayesian rule' is a bit of a shortcut since you need to apply it several times. To derive the above formula for P(R|x,q), I would start with conditional probability definition (which is the root of Bayesian rule):

P(A|B) = P(A ^ B) / P(B)

Then:

P(R|x,q) = P(R ^ x ^ q) / P(x,q) = P(x|R,q) * P(R,q) / [ P(x|q) * P(q) ] =

= P(x|R,q) * P(R|q) / P(q) / [ P(x|q) * P(q) ]

When you divide nominator and denominator by P(q), you obtain

P(R|x,q) = P(x|R,q) * P(R|q) / P(x|q)"
tf-idf for sentence level features,"It's common to see some confusion about TFIDF so thank you for asking this question :)

TFIDF is not a metric, it's a weighting scheme

This means that it's a way to represent a document, not to compare documents. TFIDF assumes a bag of words (BoW) representation, i.e. a document or sentence is represented as a set of words (their order doesn't matter). The basic BoW representation is to encode every token/word with its frequency (TF); In TFIDF the frequency of the word is multiplied by the IDF (actually the log of the IDF) in order to give more importance to words which appear rarely.

Two important points:

The TF part is specific to the document, whereas the IDF part is calculated across all the documents in the collection.
Each dimension in a TFIDF vector represents a word. The dimensions are the same for all the documents, they correspond to the full vocabulary across all the documents (this way index 
i
ùëñ
 always corresponds to the same word 
w
i
ùë§
ùëñ
).

Note that there are other weighting schemes which can be used to represent documents as vectors, for example Okapi BM25.

Cosine-TFIDF is a metric to compare TFIDF vectors

Once documents (or sentences) have been encoded as TFIDF vectors using the same vocabulary (same dimensions), these vectors can be used to calculate a similarity score between any pair of documents (or a document and a query encoded the same way).

The Cosine similarity measure is certainly the most common way to compare TFIDF vectors. It's so common that sometimes people omit to mention it or over-simplify the explanation by saying that they ""compare documents with TFIDF"" (this is technically incorrect).

Note that other similarity measures can be used as well with TFIDF vectors. Most other measures (e.g. Jaccard) tend to give similar results, they're not fundamentally different from Cosine."
What is the current state-of-the-art within aspect-based sentiment analysis?,"It's hard to say what is state-of-the-art in general without breaking aspect-level sentiment analysis down into its subtasks:

1) Aspect extraction

2) Sentiment classification

As you've probably read in Liu's book, aspect extraction can be done relatively well by extracting the most common noun phrases and adding some heuristics. This works particularly well when you are dealing with texts that revolve around a few topics. Topic-model based techniques (LDA etc) are better, but more complicated to implement.

As for classification, all current state-of-the-art approaches use neural networks (Recurrent NNs or Convolutional NNs). At sentence-level Kim (2014) is still soa on several datasets. There was a paper by Wang et al. about attention-based LSTMs for aspect-level sentiment analysis in EMNLP last year.

I'd suggest looking at the recent SemEval tasks (2014 task 4, 2015 task 12, 2016 task 5) on aspect-based sentiment analysis. There's a lot of good ideas that you could pick up on there."
How to extract assignment from natural language text?,"It's indeed a very specific type of relation extraction. Generally relation extraction is much more complex because it's not only about simple sentences Subject Verb Object and not only with the verb is.

It's not clear to me if your examples are representative of the real cases you're dealing with: if yes, you probably don't need full blown relation extraction, basic pattern matching rules will do the trick."
Explanation about i//2 in positional encoding in tensorflow tutorial about transformers,"It's not a bug, although they added some confusion with this trick. They should better call their argument 
j
ùëó
 instead of 
i
ùëñ
, cos what they actually do is they take all values 
0‚â§j‚â§
d
model
‚àí1
0
‚â§
ùëó
‚â§
ùëë
ùëö
ùëú
ùëë
ùëí
ùëô
‚àí
1
 and compute 
PE(pos,j)
ùëÉ
ùê∏
(
ùëù
ùëú
ùë†
,
ùëó
)
. 
j
ùëó
 —Åan be either even or odd, but in the right side of the equation it's even, that's why they compute i//2 and multiply back by 2."
What is a lower bound on the vocabulary size for generating word/sentence embedding using word2vec or skip thought vectors?,"It's not a straightforward question to answer as it is hard to compare the quality of two word2vec models with a meaningful metric. You could, of course, use the loss function, but that won't give much.

Another approach is more heuristic: take for example the frequency of each word, and remove those words that are repeated less than N times, where you can set N to for example 10 or 20. This is common practice as you need a certain number of repetitions of the same word to have some meaningful results."
Predicting Missing Word in Text,"It's not simple, but doable. I suggest you to create training data in the following way: take a text corpus, as large as possibile, and remove words sampled randomly. Then train an seq2seq RNN to map this ""deteriorated"" text with its original.

The RNN you need won't be too different from an NMT model, but it's goal is different of course.

It's the first time I encountered this kind of task, therefore I can't say what is the state of the art."
Why is it useful to use different word splitting with different tokenizers?,"It's rare to represent sentences as sequences of characters, since most NLP tasks are related to the the semantics of the sentence, which is expressed by the sequence of words. A notable exception: stylometry tasks, i.e. tasks where the style of the text/author matters more than the topic/meaning, sometimes rely on sequences of characters.
Yes, the question of tokenization can indeed have an impact of the performance of the target task. But modern methods use good word tokenizers trained on large corpora, not simplifed whitespace-based tokenizers. There can still be differences between tokenizers though.
There are even more text representations methods than listed here (embeddings are an important one). And yes, these also have a huge impact on performance.

For all these different options (and others), the reason why it's often worth testing different variants is clear: it affects performance and it's not always clear which one is the best without trying, so one must evaluate the different options. Btw it's crucial to precisely define how the target task is evaluated first, otherwise one just subjectively interprets results.

Basically imho this is a matter of proper data-driven methodology. Of course experience and intuition also play a role, especially if there are time or resources constrains."
Question about BERT embeddings with high cosine similarity,"It's really hard to say if the BERT assigns similar embeddings to the same words in the similar context. Most likely it will not. This is because, the embeddings, is not just function of context (other words in the sentence), but the position as well. Hence, even if the words occurs in similar context, with different positions, they can have different embeddings."
CRFSuite/Wapiti: How to create intermediary data for running a training?,"It's true that it's a bit of a complex process but it's worth understanding it in order to get the best out of the model.

""Feature"" and ""attribute"" (and probably observation but I'm not 100% sure) are the same thing. The features are the ones directly used by the model (as opposed to the raw input data). For every input word a vector of binary features is generated based on the input data following the custom ""patterns"" defined in the configuration file. Note that I'm using the word ""data"" because the input data doesn't have to be only the text, it can optionally include additional information as columns, for example POS tags (as obtained by a POS tagger) and syntactic dependencies (as obtained by a dependency parser).

This kind of information is often very useful for the model: if the model can only use the text then the default binary features are made of a basic one-hot-encoding of the words. This means that the model can only use conditions based on whether word == x or word != x. To see why this is not enough: the word ""12345"" is different from ""12346"" in the same way that the word "";"" is different from ""paleontology"", i.e. in this example the model can not capture the fact that ""12345"" and ""12346"" are both numbers.

Additionally the patterns allow the model to use other ""neighbour features"", which is why the notation is a bit complex. The idea is that the label may depend not only on the features of the current word but also on the features of the previous word, or the one before that. In other words, this allows the model to take into account the context in the sequence.

Finally it's usually also possible to define the dependencies between labels. For example there might some sequences of labels which cannot happen, and this information can help the model to determine the correct label for the current word by taking into account the previous/next label in the sequence.

Ok that's a very short summary, now how to decide which patterns to use? Well, the most common option is to try a few configurations, then test and tune them manually. It's also possible to automatize this process but it's rarely worth the effort imho."
What does Prec@1 in fastText mean?,"Its ""Precision at 1"", or how often the highest ranked document is relevant:

http://ir-ratio.blogspot.co.uk/2012/03/precision-at-1-and-reciprocal-rank.html

Suppose you are looking for items about monkeys. Your query engine queries documents for ""monkeys"" and ranks by relevance. If the highest ranked document is indeed about monkeys, then that's a win for your query algorithm. But if the highest ranked document is ranked 1 because it has the text ""Enough of your monkey business"" then its a loss, because that's not really about monkeys.

Repeat over a bunch of search terms. The Precision-at-one is then the number of wins over the total number of search terms tried."
Why I get a very low accuracy with LSTM and pretrained word2vec?,"Its huge discrepancy, I suspect a lie.

While +33% can be achieved, you said that you tried very different architectures and you did not get even close. Dont expect that one tweak, one layer, one xyz can give you all of a suddenly such a huge increase. If you did not get closer using suggestions from the paper there is also a possibility (not saying they did, but it has been done before in ML papers- lying about accuracy achieved without providing code) that they lied."
Extending a trained neural network for a larger input,"Its okay as long as the nerwork you are planning to create has the same number of layers and units i.e the dimensions of your network must be compatible with the weights that you are borrowing from the trained model. Also it would be better if you follow the second blog post of suriyadeepan practical seq2seq where he trains a conversation model on twitter chat. The code is much simpler and easier to understand, also it is on a smaller dataset, also he mentioned that the bot trained on cornell movie dialog corpus wasnt performing so well. Mainly to use the pre-trained weights all you have to do is load the model, create placeholders for the weights, assign thr weights from the loaded model to the placeholders and run a forward pass. This blog and this question might help you with this task"
Features for POS tagging,JJ is short for adjective and NN stands for noun. Those can be easily confused because you are using a windowing method. Another option would be use a dependency parser to construct a parse tree which provides more information about the structure of language to the model.
Using Vowpal Wabbit for NER,John Langford's documentation on GitHub could help. You can find something on the Learning to Search Sub System page.
Text extraction from documents using NLP or Deep Learning,"Jurafsky and Martin's NLP textbook has a chapter about information extraction that should be a good starting point. For example, if you want to extract company names it will tell you how to do that.

A paralegal would go through the entire document and highlight important points from the document.

What you need to do depends heavily on what your definition of ""important"" is here. It would help if you can give some specific examples."
A simple attention based text prediction model from scratch using pytorch,"Just for fun, run this for long generative output. Here is some code to put at the end. Also, you may want to change it to n-tuples or ngrams. This is a nice toy language model!

output_str = []
with torch.no_grad():
  context = ngrams[0][0][:]

  # Getting context and target index's
  context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)
  output_txt = context

  for i in range(30):
    context = context[1:]+[vocab[ixp]]
    output_txt+=[vocab[ixp]]
    #print(vocab[ixp],end=' ')

    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)
    log_preds = model(context_idxs)
    ixp=torch.multinomial(np.exp(log_preds),1)
    #ixp=torch.argmax(log_preds)

"" "".join(output_txt)"
What is the structure and dimension of input passed to neural network when training CBOW and SKIP GRAM word embedding,"Just think of Skip-Gram (with negative sampling) as a simple binary logistic classifier.

The data is a collection of nearby word pairs 
(w,c)
(
ùë§
,
ùëê
)
 extracted from a large corpus. For each of those 
k
ùëò
 negative samples are formed by drawing a context word 
c
‚Ä≤
ùëê
‚Ä≤
 from a noise distribution.

The model has two layers of parameters without a non-linear function between them (equivalent to matrix multiplication of layer parameters) and a sigmoid function on the output (not softmax). Input and output layers have one node per word and the middle layer has the dimension of embeddings 
d
ùëë
 (e.g. 500).

For each word pair 
(w,c)
(
ùë§
,
ùëê
)
, feed a one-hot vector representing 
w
ùë§
 and predict 1 at the output node representing 
c
ùëê
 and 0 at each of the negative output nodes 
c
‚Ä≤
ùëê
‚Ä≤
.

Each input word 
w
ùë§
 corresponds to 
d
ùëë
 parameters in the first layer and each context word 
c
ùëê
 has 
d
ùëë
 parameters in the second layer. These are embeddings. Since each word is considered as 
w
ùë§
 or 
c
ùëê
 at different times, each dictionary word has two 
d
ùëë
-dimensional embeddings (one in each layer), typically only one is used."
Topic modelling on long documents: intra document clustering first,"K means has certain assumption which leads to high bias due to hard assignments. In case of topic modelling documents may have overlapping classes i.e. a document may have two topics in it.

I would suggest you to try Non Negative Matrix factorisation over K means to check the results."
How cluster a twitter data-set?,"k-means is very sensitive to noise

because it is designed as a least-squares approach. Noise deviations, when squared, become even larger.

Twitter is mostly noise

Twitter is full of spam and nonsense tweets. These will be entirely unlike any other and thus have the largest deviations.

Chances are you get one ""cluster"" that contains almost everything, and the other k-1 clusters consist of a few tweets with their duplicates. Clusters are not topics. They are more likely to be duplicates than topics.

An appropriate clustering algorithm for tweets should probably discard 90% of tweets and produce thousands of clusters. But it will rarely be better than finding all tweets in common - most tweets only have 2-3 usable words."
How to use pre-trained word2vec model generated by Gensim with Convolutional neural networks (CNN),"Keras provides a good example how to load pretrained word embeddings and train a model on it. The tricky part is to load the pretrained embeddings, but this is well explained in the code and can be adopted easily. Also note that you need to load the embeddings in the embedding layer, which must be ""frozen"" (should not be trainable). This can be achieved by seting trainable=False:

from tensorflow.keras.layers import Embedding

embedding_layer = Embedding(
    num_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
)


There are a number of useful resources provided by Keras related to natural language processing (NLP), including examples on semantic similarity (BERT), NER transformers, and sequence-to-sequence learning."
Extract sentences from beginning of news in single document summarization,"Keselman, Schubert

Computational models for text summarization

The paper deals with methods (models) for text summarization. The reference (base) model was ""first sentence model"":

As a baseline for our models we used a trivial model that repeats the first sentence of the input document.

Then, various experiments and results are presented, like this one: (notice that ""first sentence model"" is always present as ""baseline"")

Moreover, one of the datasets for training and evaluation of models in this paper is DUC, which may be interesting to you.

Steinberger (doctoral Thesis, 2005)

Text Summarization within the LSA Framework

In section 2.1, the author discusses document summarization approaches based on sentence extraction. He identifies five approaches:

Surface Level Approaches
Corpus-based Approaches
Cohesion-based Approaches
Rhetoric-based Approaches
Graph-based Approaches

(The ""First Sentence Approach"" belongs to the *Surface Level Approaches"") The author further describes these approaches and compares them.

Khodra, Widyantoro, Aziz, Trilaksono (Journal of ICT Research and Applications, 2011)

Free Model of Sentence Classifier for Automatic Extraction of Topic Sentences

The author identifies and tests methods for identifying the most important sentences in a text (see list of 58 items below, called features). Surprisingly, in the conclusion, it is said that position of the sentence is a dominant feature, meaning that including all other features into consideration leads only to small improvement.

position
sentence length
number of words before a main verb
adjective incidence
existential there incidence
incidence of 3rd person singular grammatical form
anaphora incidence
coordinators incidence
cardinal number incidence
incidence of past tense endings
Hypernymy
Polysemy
concreteness index
affect_formulai
bad_formulaic
comparison_formulaic
continue_formulaic
contrast_formulaic
detail_formulaic
future_formulaic
gap_formulaic
good_formulaic
here_formulaic
in_order_to_formulaic
method_formulaic
no_textstructure_formulaic
similarity_formulaic
them_formulaic
textstructure_formulaic
tradition_formulaic
us_previous_formulaic
affect
argumentation
better_solution
change
comparison
continue
contrast
interest
need
presentation
problem
research
solution
textstructure
use
copula
aim_ref_agent
gap_agent
general_agent
problem_agent
ref_agent
ref_us_agent
solution_agent
textstructure_agent
them_agent
them_pronoun_agent
us_agent

For you, the most important part of the paper may be table 5:

Read carefully the explanation of the table in the paper, and the whole Section 4.3.

Other papers worth examining:

Luhn (1958)

The Automatic Creation of Literature Abstracts

Kupiec, Pedersen, Chen (1995)

A Trainable Document Summarizer

Yang, Pedersen (1997)

A Comparative Study on Feature Selection in Text Categorization

Sebastiani (2002)

Machine Learning in Automated Text Categorization"
"Do large pretrained language models already ""know"" about NLP tasks?","Large pretrained language models are empirically useful. They are empirically useful at prediction for established NLP benchmarks and novel tasks. Since this class of models is the best currently available at prediction across this spectrum, there is no need for ""prefiltering steps"" and does not matter if the models have memorized previously seen answers."
Eigenvectors and eigenvalues for natural language processing,"Latent Semantic Analysis (LSA) relies on linear-algebraic decompositions (e.g. SVD), which in turn involve eigenvectors/values (see here).

Not sure if this is quite what your question is driving at, but in general, eigenvectors are useful in data analysis because they define some ""natural"" direction in the data. For example, the eigenvectors of the covariance matrix of some data are all perpendicular to one another, and the i'th one points in the direction of i'th greatest variance in the data. In other words, the first eigenvector points in the direction where the data's variance is greatest, the second one in (perpendicular) direction of second-greatest variance, and so on. This is why eigenvector-based decompositions can be so useful in feature selection and dimensionality reduction. In informal terms, they transform the data into perpendicular features that are aligned with some ""natural"" axes in the data."
Calculating optimal number of topics for topic modeling (LDA),"LDA being a probabilistic model, the results depend on the type of data and problem statement. There is nothing like a valid range for coherence score but having more than 0.4 makes sense. By fixing the number of topics, you can experiment by tuning hyper parameters like alpha and beta which will give you better distribution of topics.

The alpha controls the mixture of topics for any given document. Turn it down and the documents will likely have less of a mixture of topics. Turn it up and the documents will likely have more of a mixture of topics.

The beta controls the distribution of words per topic. Turn it down and the topics will likely have less words. Turn it up and the topics will likely have more words.

The main purpose of lda is to find hidden meaning of corpus and find words which best describe that corpus.

To know more about coherence score you can refer this"
Attributes extraction from unstructured product descriptions,"Left you a quick response on SO. The gist is that you can collect a lot of information from electronics shops and manufacturers' web sites, and lots you can annotate manually. If your goal is to only get training data, that's all you need:

My answer form the cross-post: ""Having developed a commercial analyzer of this kind, I can tell you that there is no easy solution for this problem. But there are multiple shortcuts, especially if your domain is limited to cameras/electronics.

Firstly, you should look at more sites. Many have product brand annotated in the page (proper html annotations, bold font, all caps in the beginning of the name). Some sites have entire pages with brand selectors for search purposes. This way you can create a pretty good starter dictionary of brand names. Same with product line names and even with models. Alphanumeric models can be extracted in bulk by regular expressions and filtered pretty quickly.

There are plenty of other tricks, but I'll try to be brief. Just a piece of advice here: there is always a trade-off between manual work and algorithms. Always keep in mind that both approaches can be mixed and both have return-on-invested-time curves, which people tend to forget. If your goal is not to create an automatic algorithm to extract product brands and models, this problem should have limited time budget in your plan. You can realistically create a dictionary of 1000 brands in a day, and for decent performance on known data source of electronic goods (we are not talking Amazon here or are we?) a dictionary of 4000 brands may be all you need for your work. So do the math before you invest weeks into the latest neural network named entity recognizer."""
Why use cosine similarity instead of scaling the vectors when calculating the similarity of vectors?,"Let 
u,v
ùë¢
,
ùë£
 be vectors. The ""cosine distance"" between them is given by

d
cos
(u,v)=1‚àí
u
‚à•u‚à•
‚ãÖ
v
‚à•v‚à•
=1‚àícos
Œ∏
u,v
,
ùëë
cos
(
ùë¢
,
ùë£
)
=
1
‚àí
ùë¢
‚Äñ
ùë¢
‚Äñ
‚ãÖ
ùë£
‚Äñ
ùë£
‚Äñ
=
1
‚àí
cos
‚Å°
ùúÉ
ùë¢
,
ùë£
,

and the proposed ""normalized Euclidean distance"" is given by

d
NE
(u,v)=
‚à•
‚à•
‚à•
u
‚à•u‚à•
‚àí
v
‚à•v‚à•
‚à•
‚à•
‚à•
=
d
E
(
u
‚à•u‚à•
,
v
‚à•v‚à•
).
ùëë
ùëÅ
ùê∏
(
ùë¢
,
ùë£
)
=
‚Äñ
ùë¢
‚Äñ
ùë¢
‚Äñ
‚àí
ùë£
‚Äñ
ùë£
‚Äñ
‚Äñ
=
ùëë
ùê∏
(
ùë¢
‚Äñ
ùë¢
‚Äñ
,
ùë£
‚Äñ
ùë£
‚Äñ
)
.

By various symmetries, both distance measures may be written as a univariate function of the angle 
Œ∏
u,v
ùúÉ
ùë¢
,
ùë£
 between 
u
ùë¢
 and 
v
ùë£
. [1] Let's then compare the distances as a function of radian angle deviation 
Œ∏
u,v
ùúÉ
ùë¢
,
ùë£
.

Evidently, they both have the same fundamental properties that we desire -- strictly increasing monotonicity for 
Œ∏
u,v
‚àà[0,œÄ]
ùúÉ
ùë¢
,
ùë£
‚àà
[
0
,
ùúã
]
 and appropriate symmetry and periodicity across 
Œ∏
u,v
ùúÉ
ùë¢
,
ùë£
.

Their shapes are different, however. Euclidean distance disproportionately punishes small deviations in the angles larger than is arguably necessary. Why is this important? Consider that the training algorithm is attempting to reduce the total error across the dataset. With Euclidean distance, law-abiding vectors are unfairly punished (
1
2
d
NE
(
Œ∏
u,v
=œÄ/12)=0.125
1
2
ùëë
ùëÅ
ùê∏
(
ùúÉ
ùë¢
,
ùë£
=
ùúã
/
12
)
=
0.125
), making it easier for the training algorithm to get away with much more serious crimes (
1
2
d
NE
(
Œ∏
u,v
=œÄ)=1.000
1
2
ùëë
ùëÅ
ùê∏
(
ùúÉ
ùë¢
,
ùë£
=
ùúã
)
=
1.000
). That is, under Euclidean distance, 8 law-abiding vectors are just as bad as maximally opposite-facing vectors.

Under cosine distance, justice is meted out with more proportionate fairness so that society (the sum of error across the dataset) as a whole can get better.

[1] In fact, 
d
cos
(u,v)=
1
2
(
d
NE
(u,v)
)
2
ùëë
cos
(
ùë¢
,
ùë£
)
=
1
2
(
ùëë
ùëÅ
ùê∏
(
ùë¢
,
ùë£
)
)
2
."
Maximum Entropy modelling - likelihood equation,"Let 
X
1
,...,
X
n
ùëã
1
,
.
.
.
,
ùëã
ùëõ
 have a discrete join probability distribution 
P(x‚à£Œ∏)
ùëÉ
(
ùë•
‚à£
ùúÉ
)
 where 
x
‚Éó¬†
=(
x
1
,...,
x
n
)
ùë•
‚Üí
=
(
ùë•
1
,
.
.
.
,
ùë•
ùëõ
)
 is a vector in the sample and 
Œ∏
ùúÉ
 is a parameter from parameter space. We will call 
P(x‚à£Œ∏)
ùëÉ
(
ùë•
‚à£
ùúÉ
)
 the likelihood function when it is a function of 
Œ∏
ùúÉ
. By the ML principle we must maximize the likelihood function over some training set. Assuming that the samples are i.i.d then ML is maximize by:

thet
a
‚àó
=argmax
‚àë
m
i=1
log(P(
x
i
‚à£Œ∏))
ùë°
‚Ñé
ùëí
ùë°
ùëé
‚àó
=
ùëé
ùëü
ùëî
ùëö
ùëé
ùë•
‚àë
ùëñ
=
1
ùëö
ùëô
ùëú
ùëî
(
ùëÉ
(
ùë•
ùëñ
‚à£
ùúÉ
)
)

The above part should be review.

Let 
œá
ùúí
 be the set 
{
a
1
,...,
a
n
}
{
ùëé
1
,
.
.
.
,
ùëé
ùëõ
}
 and let 
P(a‚à£œï)
ùëÉ
(
ùëé
‚à£
ùúô
)
 be the probability of drawing 
a
i
ùëé
ùëñ
. Finally let 
x
1
,...,
x
n
ùë•
1
,
.
.
.
,
ùë•
ùëõ
 be the sequence of the drawn and be i.i.d based on probability P. 
f(a)
ùëì
(
ùëé
)
 is the measurement of the draws i.e. 
f(a)=|{i:
x
i
=a}|
ùëì
(
ùëé
)
=
|
{
ùëñ
:
ùë•
ùëñ
=
ùëé
}
|
.

The empirical distribution is defined by 
P
^
(a)=
f(a)
‚àë
a‚ààœá
f(Œ±)
=
f(a)
m
ùëÉ
^
(
ùëé
)
=
ùëì
(
ùëé
)
‚àë
ùëé
‚àà
ùúí
ùëì
(
ùõº
)
=
ùëì
(
ùëé
)
ùëö
.

The joint probability 
P(
x
1
,..
x
m
|Œ∏)=
‚àè
m
i=1
p(
x
i
|œï
)
f(a)
ùëÉ
(
ùë•
1
,
.
.
ùë•
ùëö
|
ùúÉ
)
=
‚àè
ùëñ
=
1
ùëö
ùëù
(
ùë•
ùëñ
|
ùúô
)
ùëì
(
ùëé
)

As you can see here Training data, your tutorial and I have defined our emperical distribution similiary and obtained the same result.

TLDR, it's just a matter of notation."
How to preprocess with NLP a big dataset for text classification,"Let me first clarify the general principle of classification with text data. Note that I'm assuming that you're using a ""traditional"" method (like decision trees), as opposed to Deep Learning (DL) method.

As you correctly understand, each individual text document (instance) has to be represented as a vector of features, each feature representing a word. But there is a crucial constraint: every feature/word must be at the same position in the vector for all the documents. This is because that's how the learning algorithm can find patterns across instances. For example the decision tree algorithm might create a condition corresponding to ""does the document contains the word 'cat'?"", and the only way for the model to correctly detect if this condition is satisfied is if the word 'cat' is consistently represented at index 
i
ùëñ
 in the vector for every instance.

For the record this is very similar to one-hot-encoding: the variable ""word"" has many possible values, each of them must be represented as a different feature.

This means that you cannot use a different index representation for every instance, as you currently do.

Vectors generated from those texts needs to have the same dimension Does padding them with zeroes make any sense?

As you probably understood now, no it doesn't.

Vectors for prediction needs also to have the same dimension as those from the training

Yes, they must not only have the same dimension but also have the same exact features/words in the same order.

At prediction phase, those words that hasn't been added to the corpus are ignored

Absolutely, any out of vocabulary word (word which doesn't appear in the training data) has to be ignored. It would be unusable anyway since the model has no idea which class it is related to.

Also, the vectorization doesn't make much sense since they are like [0, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] and this is different to [1, 0, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] even though they both contain the same information

Indeed, you had the right intuition that there was a problem there, it's the same issue as above.

Now of course you go back to solving the problem of fitting these very long vectors in memory. So in theory the vector length is the full vocabulary size, but in practice there are several good reasons not to keep all the words, more precisely to remove the least frequent words:

The least frequent words are difficult to use by the model. A word which appears only once (btw it's called a hapax legomenon, in case you want to impress people with fancy terms ;) ) doesn't help at all, because it might appear by chance with a particular class. Worse, it can cause overfitting: if the model creates a rule that classifies any document containing this word as class C (because in the training 100% of the documents with this word are class C, even though there's only one) and it turns out that the word has nothing specific to class C, the model will make errors. Statistically it's very risky to draw conclusions from a small sample, so the least frequent words are often ""bad features"".
You're going to like this one: texts in natural language follow a Zipf distribution. This means that in any text there's a small number of distinct words which appear frequently and a high number of distinct words which appear rarely. As a result removing the least frequent words reduces the size of the vocabulary very quickly (because there are many rare words) but it doesn't remove a large proportion of the text (because the most frequent occurrences are frequent words). For example removing the words which appear only once might reduce the vocabulary size by half, while reducing the text size by only 3%.

So practically what you need to do is this:

Calculate the word frequency for every distinct word across all the documents in the training data (only in the training data). Note that you need to store only one dict in memory so it's doable. Sort it by frequency and store it somewhere in a file.
Decide a minimum frequency 
N
ùëÅ
 in order to obtain your reduced vocabulary by removing all the words which have frequency lower than 
N
ùëÅ
.
Represent every document as a vector using only this predefined vocabulary (and fixed indexes, of course). Now you can train a model and evaluate it on a test set.

Note that you could try different values of 
N
ùëÅ
 (2,3,4,...) and observe which one gives the best performance (it's not necessarily the lowest one, for the reasons mentioned above). If you do that you should normally use a validation set distinct from the final test set, because evaluating several times on the test set is like ""cheating"" (this is called data leakage)."
NLP varying amount of features and BoW as feature concatenating to feedforward NN,"Let me take a crack at your questions:

The article specifies the features are concatenated. How does a concatenation layer work internally? Does it concatenate all the values in a single variable, in a very literal sense? how does that work computationally?


The concantenation of information in this context is, concatenation of vector represententations of the text. You could concatenate using a concatenation layer as described here. This is a very common approach followed where you want to feed your network information by taking various contexts depending on the problem you need to solve.

How can a Bag of Words be a feature, when its a key-value pair? Or is it also just all concatenated into one variable. Which again, how can that work computationally?


Bag of words, is typically a vector representation of the context. The above answer should help you. You could take a look at how to combine embeddings here as well

The text specifies multiple words are used as a single feature; e.g. Left context: five words before the entity. Is this again concatenating the embedding / vectors?


Yes, you are spot on. You combine the embedding vectors that you generate using skipgram or cbow, or you could even use one-hot encoded vectors.

Entity end: last three words of the entity (they can be duplicated with the previous feature if they overlap, or padded if there are not that many) does this mean a variable amount of features as input to the NN (or concatenation layer) or is this more intended as a configuration? Less context available so fewer amount of 'hard' coded input features?


Its always good to pad and use well defined dimensional vectors. Helps you structure your architecture better. You could use masking layer to ensure that the network ignores the paddings.

I hope this helps."
What is generative and discriminative model? How are they used in Natural Language Processing?,"Let's say you are predicting the topic of a document given its words.

A generative model describes how likely each topic is, and how likely words are given the topic. This is how it says documents are actually ""generated"" by the world -- a topic arises according to some distribution, words arise because of the topic, you have a document. Classifying documents of words W into topic T is a matter of maximizing the joint likelihood: P(T,W) = P(W|T)P(T)

A discriminative model operates by only describing how likely a topic is given the words. It says nothing about how likely the words or topic are by themselves. The task is to model P(T|W) directly and find the T that maximizes this. These approaches do not care about P(T) or P(W) directly."
What is the feedforward network in a transformer trained on?,"Let's take the common translation task which transformers can be used for as an example: If you would like to translate English to German one example of your training data could be

(""the cat is black"", ""die Katze ist schwarz"").

In this case your target is simply the German sentence ""die Katze ist schwarz"" (which is of course not processed as a string but using embeddings incl. positional information). This is what you calculate your loss on, run backprop on, and derive the gradients as well as weight updates from.

Accordingly, you can think of the light blue feed forward layers of a transformer

as a hidden layer in regular feed forward network. Just as for a regular hidden layer its parameters are updated by running backprop based on transformer 
loss(output,target)
ùëô
ùëú
ùë†
ùë†
(
ùëú
ùë¢
ùë°
ùëù
ùë¢
ùë°
,
ùë°
ùëé
ùëü
ùëî
ùëí
ùë°
)
 with target being the translated sentence."
How to generate a sentence with exactly N words?,"Limit outputs od decoder to N. Not sure how easy it would be, probably a bit digging into official implementation but after that the main ""skeleton"" of the GPT2 is usable, meaning that all of the pre-training can be reused to produce meaningful sentences."
List of NLP challenges,"List of NLP competitions on Kaggle by popularity [number of teams]:

Two Sigma Connect: Rental Listing Inquiries [2709 teams] https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries
Home Depot Product Search Relevance [2125 teams] https://www.kaggle.com/c/home-depot-product-search-relevance
Quora Question Pairs [2123 teams] https://www.kaggle.com/c/quora-question-pairs
What's Cooking? [1388 teams] https://www.kaggle.com/c/whats-cooking
Crowdflower Search Results Relevance [1326 teams] https://www.kaggle.com/c/crowdflower-search-relevance
Bag of Words Meets Bags of Popcorn [578 teams] https://www.kaggle.com/c/word2vec-nlp-tutorial
Transfer Learning on Stack Exchange Tags [380 teams] https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags/data
Facebook Recruiting III - Keyword Extraction [367 teams] https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction
New entrants are prohibited, you won't be able to download data for this competition
Dato: Truly Native? [274 teams] https://www.kaggle.com/c/dato-native
The Allen AI Science Challenge [170 teams] https://www.kaggle.com/c/the-allen-ai-science-challenge"
Document embedding vs locality sensitive hashing for document clustering,"Locality sensitive hashing (LSH) is a search technique. With it, similar documents get the same hash with higher probability than dissimilar documents do. LSH is designed to allow you to build lookup tables to efficiently search large data sets for items similar to a given item. It is also a probabilistic method in that it can generate false positives and false negatives. While there are ways to train LSH, most LSH is untrained. That's because LSH has been studied more in the search setting than the machine learning setting.

Embeddings are a machine learning technique to capture semantic information for use in some downstream task, such as clustering or classification. Typically semantically similar items get similar (but not the same) embeddings. Embeddings are trained from data. There are many unsupervised algorithms (word2vec, glove) and there are supervised methods too (auto-encoders, hidden layer output from deep models).

Embeddings can be used to map items into a space where near neighbor search would find semantically similar items. However, on large data sets you would still need to index the data to search efficiently, which raises the possibility of doing LSH on embeddings. That way you get the benefit of a trained model that learns the distribution of your data set and the benefit of a fast lookup table."
Using the whole GloVe pre-trained embedding matrix or minimize the matrix based on the number of words in vocabulary,"Long Short Term Memory (LSTM) can take a long time to train because of the complexity of the architecture.

If you think the size of the embedding space is also slowing down training, you can reduce the size of the vocabulary by only taking the most frequently occurring words. Since it appears you are using PyTorch, you can do this within the torchtext.vocab.GloVe class with the max_vectors argument which limits the size of the loaded set."
Tokenize sentence based on a dictionary,"Look at the NLTK library for Python, there are functions to facilitate tokenizing sentences."
Inferring Relational Hierarchies of Words,"Look up taxonomy/ontology construction/induction. Relevant papers:

Automatic Taxonomy Construction from Keywords via Scalable Bayesian Rose Trees
Topic Models for Taxonomies
OntoLearn Reloaded. A Graph-Based Algorithm for Taxonomy Induction
Ontology Population and Enrichment: State of the Art
Probabilistic Topic Models for Learning Terminological Ontologies"
Multi-label text classification with minimum confidence threshold,"Looks like the common approaches to multi-class classification actually solve this challenge.

Building individual Naive Bayes Classifiers with only the training data for a single label is insufficient - we must also the include data from other labels as ""everything else"".

See Text-Classification-Problem, what is the right approach?"
How prevalent is `C/C++` in machine learning development?,"Machine learning is inherently data intensive, and typical ML algorithms are massively data-parallel. Therefore, even when developing new algorithms, high-level mathy languages (like Python, R, Octave) can be reasonably fast if you are willing to describe your algorithm in terms of standard operations on matrices and vectors.

On the other hand, for deeper exploration of fundamental concepts it can be more interesting to treat individual components as objects for which you want to conceptualize and visualize their internal state and interactions. This is a case where C++ may shine. Using C++, of course, means that a compiler will attempt to optimize your execution speed. Additionally, it opens the door to straightforward multi-core execution with OpenMP (or other available threading approaches).

C++ is a high level language -- not inherently more verbose or tedious than Python for algorithm development. The biggest challenges for working with C++ are:

A more anarchic library ecosystem means a bigger effort for choosing and integrating existing components.
Less stable language rules (or the interpretation thereof) means that something you create today might not compile a few years down the road (due to compiler upgrades).

Consider, also, that TensorFlow documentation identifies some benefits of using C++ over Python for certain low-level cases. See TensorFlow: Create an op.

Low-level coding for GPU acceleration is an entirely different can of worms with very limited language options. This is not something to be concerned about until after you have a well-defined custom algorithm that you want to super-optimize. More likely, you would be better off using a framework (like TensorFlow) to handle GPU interactions for you.

For exploratory visualization purposes, don't discount the interactive power of JavaScript, which is also comparatively fast:

A Neural Network Playground
TensorFlow.js demos
ConvNetJS: Deep Learning in your browser"
Would Topic Modelling be classified as NLP or NLU?,"Maybe I'm having trouble formulating the inherent difference between NLP and NLU, when do we draw the line between the two?

There is a confusion here: NLP is the whole domain of AI which deals with natural language. It includes virtually any task related to processing language data (usually mostly written data, but that's not the point). Topic modeling is one of these tasks.

NLU is the problem of Natural Language Understanding, which is usually considered as one of the main goals of NLP. If anything, NLU is a problem that NLP tries to solve, i.e. a sub-topic in the large area of NLP.

Also notice that using words embeddings can improve things, but it doesn't solve all the difficulties related to semantics, far from it.

[edit] The scope of NLU is not strictly defined: in the broadest possible definition, it would include anything vaguely related to extracting meaning from text, and in this very generous sense topic modeling would have a connection to it with or without embeddings (and so would a lot of other NLP tasks). Wikipedia says:

The umbrella term ""natural-language understanding"" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text.

But the most commonly accepted definition of NLU is stricter, it would only consider tasks which directly involve the interpretation of text in a quite complex setting. The typical example is the ""virtual assistant"" such as Amazon Alexa, OK Google, Apple's Siri. In this sense topic modeling is simply a completely different task, no matter the ""degree of understanding""."
Differentiate between positive and negative clusters,"Maybe you don't have a positive and a negative class. Your input are word vectors. Unless you trained your word vectors before with explicit positive and negative labels, it is very unlikely that your KMeans learned that difference.

If you used pre-trained word vectors, your KMeans could have learned an arbitrary difference between cluster 0 and cluster 1. Maybe it learned which reviews are from males and which from females, maybe which have the word ""parachute"" and which don't have the word ""parachute"", the options are endless.

What you can do, is access which labels your KMeans learned (model.labels_) and filter your input X per cluster. Then, count the occurence of each word in each cluster and order which words happen the most in each of them. This might help you understand the difference between cluster 0 and cluster 1.

Note: if the top words you get are words like: a, the, of, if, etc. Use a stop-word list, or filter those word with a max document frequency threshold."
Does it make sense to concatenate datasets to improve accuracy of model?,"More data is always better. It makes 100% sense to concatenate all datasets in to a larger one, assuming that they are in the same format. This will most likely improve the performance of your model."
How should labeled data from multiple annotators be prepared for ML text classification?,"Most machine learning algorithms are designed with complete trust in the labels. There is no standard way to model uncertainty in data labels. Thus, create a project-specific threshold for uncertainty to omit data or labelers. For example, a trusted classification data label would require n of m ensemble voting.

One major issue is re-labeling. Systems tend to evolve over time and the definition of labels is refined. Mature data labeling system have a notion of data lineage - ""Who labeled what data when with what criteria"".

The book ""Human-in-the-Loop Machine Learning"" by Robert Munro goes into greater detail."
What is word embedding and character embedding ? Why words are represented in vector with huge size?,"Most problems in NLP require the system to understand the semantic meaning of the text and not just the arrangement of specific words.

Semantic understanding enables a system to say that, ""I am happy"" and ""It's joyful"", have the same meaning.

To incorporate this feature to a system, we present words of a particular language in form of vectors. Often called as embeddings, they help in establishing similarities between words and phrases.

For instance, a vector representing the word ""happy"" will lie in the vicinity of the vectors representing the words ""joy"", ""pleasure"", ""sad"" etc. These vectors are high dimensional but using PCA or other dimensionality reduction techniques they are brought down to 3 dimensions where they could be visualized.

That's why we encode words in the form of vectors. We often use cosine similarity to determine the closest vector to a given vector in analysing sematic similarity.

For an intuition, the 3D space which contains vectors for all possible English words could be thought of our knowledge base. We tend to keep similar words together in our mind. If we are talking about fast food, for instance, our brain would capture the region of the knowledge base to retreive words related with fast food like ""burgers"", ""chicken"" etc."
Xgboost multiple class predictive performance beats one versus rest,"Multiclass models in XGBoost consist of n_classes separate forests, one for each one-vs-rest binary problem. At each iteration, an extra tree is added to each forest. But it isn't actually a one-vs-rest approach (as I thought in the first version of this answer), because these trees are built to minimize a single loss function, the cross-entropy of the softmax probabilities. https://discuss.xgboost.ai/t/multiclassification-training-process/29
https://github.com/dmlc/xgboost/issues/806
https://github.com/dmlc/xgboost/issues/3655

In general, the one-vs-rest models are very good at identifying the single class, whereas the multiclass model has to balance performance on all of them. More specifically, I think that the softmax may be responsible for the phenomenon you're displaying. (I'm still thinking about it, but I thought I should post the above for now.)

Suppose one of your documents is reasonably likely to be in either of two topics: the probability scores given by the forests are 0.9, 0.85, then <0.1 in all the rest. In your topic-1 model, you make a fairly confident judgement that this document is of topic 1 (score of 0.9). But in the multiclass ensemble, you see things as much more uncertain; maybe the model applies softmax, so that the probability of topic 1 is only ~0.5.

More extreme, suppose the individual topic model scores are all 0.9. Now the multiclass ensemble applies softmax and produces equal 1/17 probabilities for each topic!

In the other direction, suppose one of your documents is judged unlikely to fit any of the topics: all the individual topic model probability scores are 0.01. In the multiclass ensemble, that gets scaled up to 1/17 (OK, 17 topics makes this a harder sell).

Hrm, except how likely is it to get the 0.9 and 0.85, since a training sample in one of these two topics will be pushed toward 0 by the other model... ? Especially when your scores are fairly high, so it's not like the models have huge blind spots.
(This part still causes a problem with the correct understanding of how XGBoost works; the log-loss of the softmax probabilities still penalizes being confident about belonging to two different classes...)"
Information Extraction from Free-form text to create Transactions,"My answer is based on couple of assumptions:

user input is more or less standard, so there won't be ""Ex 20000""
you have at least majority of forms of input covered

In every representative example of transaction description you would need to mark words of interest, be it name of holder and account number. You can start small with 10-20 examples for start, and then, when you have all required fields marked, you can train a sequence labelling model, or, say it, custom Named Entity Recognition model, which will parse new text and extract required data for you.

How to train actual model is answered in that question: Help regarding NER in NLTK , and how to mark up the data - is more of a question you should answer, since only you do know, what should be marked as account number, account holder etc.

If you would be digging into NER training, I would advise you not to rely only on current word features, but add some regexp-alike features and dictionaries as features, since task seems very limited in terms of context variation."
"Which type of models generalize better, generative or discriminative models?","My answer is not limited to NLP and I think NLP is no different in this aspect than other types of learning.

An interesting technical look is offered by: On Discriminative vs. Generative Classifiers - Andrew Ng, Michael Jordan.

Now a more informal opinion:

Discriminative classifiers attack the problem of learning directly. In the end, you build classifiers for prediction, which means you build an estimation of 
p(y|x)
ùëù
(
ùë¶
|
ùë•
)
. Generative models arrive through Bayes theorem to the same estimation, but it does that estimating the joint probability and the conditional is obtained as a consequence.

Intuitively, generative classifiers require more data since the space modeled is usually larger than that for a discriminative model. More parameters mean there is a need for more data. Sometimes not only the parameters but even the form of a joint distribution is harder to be modeled rather than a conditional.

But if you have enough data available it is also to be expected that a generative model should give a more robust model. Those are intuitions. Vapnik asked once why to go for joint distribution when what we have to solve is the conditional? He seems to be right if you are interested only in prediction.

My opinion is that there many factors that influence building a generative model of a conditional one which includes the complexity of formalism, the complexity of input data, flexibility to extend results beyond prediction and the model themselves. If there is a superiority of discriminant models as a function of available data, that is perhaps a small margin."
How do you train an ML algorithm to achieve a desirable clustering?,"My first reaction was that this is clearly not clustering, in the sense that clustering is an unsupervised task. Imho it's a complex combination of several tasks, My idea, possibly not the only one:

Fist a custom NER model would detect the two kinds of indicators, mainly person names and intent verbs. Possibly there are other words which contribute to knowing the intent or the person.
Then have a method (probably heuristic) for deciding the intent and the person in case of ambiguity. For example, ""Mary gives a flower to John"" would generate two names. Also the NER will likely generate errors sometimes.

Note that this method should in theory work even with new actions and/or new people, because NER classifies based on context words. But it will require a good amount of training data."
Mathematically rigorous NLP,"My personal recommendation would be Introduction to Natural Language Processing by Jacob Eisenstein.

In this book you should find sufficient mathematical formalization/rigor. This books is also, in my opinion, a touchstone of many introductory NLP books."
Class token in ViT and BERT,"My question is ‚Äî why does this token exist as input in all the transformer blocks and is treated the same as the word / patches tokens?

The transformers, by default are sequence to sequence networks. As there is no decoder layer in ViT, then the length of input sequence (number of patches) equals the length of output sequence. So If the goal is classification, there is two choices:

Either apply a fully connected layer on top of the transformer (which is not a good idea because then we have to fix the number of patches--which translates to input image resolution)
Or apply the classification layer on one items of the output sequence, but which one?! The best answer here is none of them! We don't want to be biased toward any of the patches. So the best solution here is to add a dummy input, call it class token and apply the classification layer on the corresponding output item!

Treating the class token like the rest of the tokens means other tokens can attend to it. I'd expect that the class token will be able to attend other tokens while they could not attend it.

Not sure, but I think if other tokens can attend to class token, then they can use some intermediate information about image class in lower layers! Just a guess and it worth to test different scenarios!

Also, specifically in ViT, why does the class token receive positional encodings? It represents the entire class and thus doesn't have any specific location.

I think the main reason is that, this way the network can distinguish a class embedding from patch embedding and treat them differently!"
Does the transformer decoder reuse previous tokens' intermediate states like GPT2?,"My understanding is that transformer decoders and transformer encoder-decoder models typically operate in the way that the GPT-2 does, i.e., representations in the generated sequence are computed once and then reused for future steps. But you are correct that this is not the only way things can be done. One could recompute the representations for all tokens in the partially-generated sequence using full self-attention over the tokens in the sequence generated so far (there's no mathematical hindrance to doing this -- it's akin to running a typical transformer encoder over the sequence of words in the partially-generated sequence).

But this additional computation is not commonly done as far as I can tell from the literature. I think there are at least two reasons. First, as noted by others, it's cheaper computationally to use previously-computed representations from earlier time steps (though it leads to different results, and I have not seen an empirical comparison in any papers). Second, it matches how training is done. During training, a consequence of masking in self-attention is that the representation at output position i is computed using representations at output positions <= i. That means that during training, there is only a single representation computed for output position i for each layer. That matches what happens at inference time using the standard approach that we've been discussing and which is used in the GPT-2.

If we wanted to train a model in which the representation for an output position was computed based on all available output representations (always excluding the ones that have not yet been ""generated"" of course), then we would need to compute multiple representations for each output position during training, one for each possible uncovered partial right context. For example, if we train a language model on windows of size 512, we would have to compute (about) 512 representations for the first word, one corresponding to the loss for generating each subsequent word in the window. This would lead to a very large computation graph and slow down training. However, it may work better because it leads to richer output representations, so please try it and let us know. :)"
Name Tagger in Stanford NLP,"Name parsing does not appear to built-in to Stanford CoreNLP. .

One option is writing a series of Regular Expression using Stanford TokensRegex to parse and label name tokens.

Another option is using a third party package, such as nameparser in Python."
Chunking Sentences with Spacy,"Named Entity Recognition (NER) would extract names of people, organizations and such. Example:

""Penalty missed! Bad penalty by <person>Felipe Brisola</person>  - <organization>Riga FC</organization> -  shot with right foot is very close to the goal. <person>Felipe Brisola</person> should be disappointed.""


So it could be helpful for the ""person"" field, but probably not for the rest. Note that you could also train a system similar to NER in order to predict other fields, but it would require a good amount of annotated data and it's not sure to work well."
Extracting Part of Speech (Source and Destinations) using text mining/NLP?,"Named Entity Recognition is technique which can be used here. Location is one of the 3 most studied classes (with Person and Organization). Stanford NLP has an open source Java implementation that is extremely powerful.

For Example let say sentence is ""i will be going to Sweden from Boston."" 

Now here you can use regular expression to detect these LOCATION tags."
Calculating confidence score in NER,Named Entity Recognition is traditionally evaluated using precision/recall and F1 score - the medium article gives a low down on how to achieve this I recently happened to read this article on a new approach for the same. Please see the details in the attached medium link but havent tried this out yet though
How to process natural language queries?,"Natural language querying poses very many intricacies which can be very difficult to generalize. From a high level, I would start with trying to think of things in terms of nouns and verbs.

So for the sentence: How many books were sold last month?

You would start by breaking the sentence down with a parser which will return a tree format similar to this:

You can see that there is a subject books, a compound verbal phrase indicating the past action of sell, and then a noun phrase where you have the time focus of a month.

We can further break down the subject for modifiers: ""how many"" for books, and ""last"" for month.

Once you have broken the sentence down you need to map those elements to sql language e.g.: how many => count, books => book, sold => sales, month => sales_date (interval), and so on.

Finally, once you have the elements of the language you just need to come up with a set of rules for how different entities interact with each other, which leaves you with:

Select count(*) from sales where item_type='book' and sales_date >= '5/1/2014' and sales_date <= '5/31/2014'

This is at a high level how I would begin, while almost every step I have mentioned is non-trivial and really the rabbit hole can be endless, this should give you many of the dots to connect."
How to find entity names in non-grammaratical text?,"NER requires indications in the context of the text to detect entities, so it's not surprising that it doesn't work here.

I am amazed how humans (even non-Pharmacists) can label most of the data correctly

The key certainly lies here: you should try to find out how humans do it, which indications they use, so that you can encode theses indications as features in order to predict the category.

My guess would be that you can rely on the pattern and identify only shop names: once the shop names are found, the rest are meds."
Smart sentence segmentation not splitting on abbreviations,"Neural tools trained on Universal Dependencies corpora use learned models for tokenization and sentence-spliting. Two I know of are:

UDPipe ‚Äì developed at Charles University in Prague. Gets very good results (at least for parsing), but has a little unintuitive API.

Stanza ‚Äì developed at Stanford University. The API is quite similar to Spacy.

However, they are quite slow compared to regex-based sentence-spliting."
How to work with different Encoding for Foreign Languages,"Nevermind, the solution was trivial. Since I had the .bin file I could just open it in binary form. If somebody doesn't really have the .bin file, they could consider converting the .txt file to .bin and solve further."
How do you distinguish between conversational text and possible news article?,"News sentences will have more unique tokens than normal conversations.
Conversations have more stop words than news articles.

I think you can use bert or normal wordvect classification to train a baseline model here. I would play aroud the pipeline of fake news classifier and news-conversation classifier. like passing the text to news classifier first and then passing it to news-conversation classifier. Try to mix and match to get the best results. Set some thresholds."
Possible reasons for word2vec learning context words as most similar rather than words in similar contexts,"Nice example of using embeddings with Keras.

If I interpret it correctly, there is a big difference between your implementation and the ""original word2vec"". The original framework operates not with one 'embeddings' vector_size x vector_dim weight matrix as in your case, but with two matrices (or layers): the ""projection layer"" which maps the input to a vector of dimension vector_dim and the ""hidden layer"" which maps this vector to probabilities over all vocabulary words.

The hidden, or prediction layer, is often discarded after the training, although it can be useful.

If your code indeed forces those layers to share weights, then we get something also interesting, but different, it is plausible the embeddings reflect co-ocurrences - see this question which asks specifically what happens if the two layers are shared.

Either way 'plant' should be most similar to 'plant' for cosine similarity - or there are some vectors that are exactly the same (normalized).

According to the original word2vec paper, hierarchical softmax is used to speed up the training. So if you don't use it and have two weight layers, your version might become too slow. I can recommend for example the gensim library instead that implements the soft-max and should run on a modern laptop in a manageable time with your setup."
What tools are available for programming language parsing for ML?,"NLP stands for Natural Language Processing. Programming language source code are synthetic (or unnatural) languages. Thus, NLP tools are not useful for processing programming language source code.

Understanding programming language source code is done by the compiler or interpreter. Compilers and interpreters perform many functions, including lexical analysis, parsing, and semantic analysis.

For Python language, language analysis is relatively straightforward since the main Python implementation, named CPython, compiles Python programs into intermediate bytecode, which is executed by the virtual machine. The dis module supports the analysis of CPython bytecode by disassembling it."
Under what circumstance is lemmatization not an advisble step when working with text data?,"NLP tasks that would be harmed by lemmatization:

1) Tense classification

      sentence        |  tense
------------------------------------
He cooked a nice meal |  past
He cooks a nice meal  |  present


The sequence of characters at the end of verbs can help in this task. The verbs cooked and cooks differ at the last characters ed and s repectively.

With lemmatization, this information is lost. Both verbs become cook, making both sentences seem (in this case) in the present tense.

2) Author identification

Given

a set of documents 
P
ùëÉ
 written by author 
a
ùëé
,
a set of documents 
Q
ùëÑ
 written by author 
b
ùëè
,
a set of documents 
S
ùëÜ
 written by either author 
a
ùëé
 or 
b
ùëè
,

classify if a document 
s‚ààS
ùë†
‚àà
ùëÜ
 is written by author 
a
ùëé
 or 
b
ùëè
.

One way to achieve this is by looking at the histogram of words present in 
s
ùë†
 and compare it to documents from 
P
ùëÉ
 and 
Q
ùëÑ
 and select the most similar one.

This works because different authors use certain words with different frequencies. However, by using lemmatization, you distort these frequencies impairing the performance of your model."
Data scraping & NLP?,"NLTK

A great starting point for keyword extraction is the NLTK (natural language toolkit) library. To extract keywords, you probably need to tokenize your data, breaking each word out into a token, and ignore the most common or unimportant words known as ""stopwords"". Assuming you're searching for keywords across a large number of query results, identify the most important terms in each document using TF-IDF (term frequency‚Äìinverse document frequency). There are tools and tutorials for this in the NLTK documentation. Sort the resulting token-scores, choose the highest scoring tokens, and these are a good start at your keywords."
Python: validating the existence of NLTK data with database search,"NLTK has a built-in NER model that would extract potential Organizations from text, you can read about it here (and see examples) NLTK book (look for section ""5 Named Entity Recognition"").

However, if your input text has organizations in a very specific context that wasn't seen by NLTK NER model, performance might be quite low. In that case you should be looking into training your own NER model, what would extract company names. For that you would require to manually markup a small amount of your dataset."
GPT-3 API Documentation?,"No - GPT-3 API is not currently public.

However once you get access, the documentation can be found at https://beta.openai.com/api-ref."
Character-level embeddings in python,"No - There is no way to get character level embeddings for SpaCy.

One option for character level embeddings is the flair package which implements Contextual String Embeddings for Sequence Labeling."
Difference between using BERT as a 'feature extractor' and fine tuning BERT with its layers fixed,"No, approaches 1 and 2 are not the same:

In approach 1 (feature extraction), you not only take BERT's output, but normally take the internal representation of all or some of BERT's layers.

In approach 2, you train not only the classification layers but all BERT's layers also. Normally, you choose a very low learning rate and a triangular learning rate schedule to avoid catastrophic forgetting.

There are many scientific articles studying how to best use BERT in transfer learning scenarios. This one may be a good starting point: https://www.aclweb.org/anthology/W19-4302/"
Are LDA clusters identical across different runs?,"No, as there is randomness in the method implementation, for example here (in LdaModel of the gensim library). Hence, it can affect your final result in each run. Therefore, if you want to keep the result reproducible, you can set the random_state property of the model to a constant seed (see the documentation for more details)."
Does the output of the Sequence-to-Sequence encoder model exist in the same semantic space as the inputs (Word2vec)? [closed],"No, assuming your input vectors are one-hot encodings. These input one-hot encodings are in an 
n
ùëõ
-dimensional Euclidean vector space. The last hidden layer of an LSTM is not due to the non-linear activation functions across the encoder. Therefore, an average of the inputs will not necessarily align well in a vector space with the model output, nor are you guaranteed any similarity in cosine/Euclidean distance."
Is BERT a language model?,"No, BERT is not a traditional language model. It is a model trained on a masked language model loss, and it cannot be used to compute the probability of a sentence like a normal LM.

A normal LM takes an autoregressive factorization of the probability of the sentence:

p(s)=
‚àè
t
P(
w
t
|
w
<t
)
ùëù
(
ùë†
)
=
‚àè
ùë°
ùëÉ
(
ùë§
ùë°
|
ùë§
<
ùë°
)

On the other hand, BERT's masked LM loss focuses on the probability of the (masked) token at a specific position given the rest of the unmasked tokens in the sentence.

Therefore, it makes no sense to use the token probabilities generated by BERT and multiply them to obtain a sentence level probability.

A secondary issue is the BERT's tokenization is subword-level so, even if it would make sense to compute a sentence-level probability with BERT, such a probability would not be comparable with a word-level LM, as we would not be taking into account all possible word segmentations into subwords.

UPDATE: there is a new technique called Masked Language Model Scoring (ACL'20) that allows precisely what the OP asked for. From the article:

To score a sentence, one creates copies with each token masked out. The log probability for each missing token is summed over copies to give the pseudo-log-likelihood score (PLL).

So the answer is now YES. It is possible to score a sentence using BERT, by means of the described pseudo-log-likelihood score."
Training data for multi-category classification algorithm,"No, it is perfectly possible to train on multiple categories. What you need, though, is an exhaustive list of these categories (in supervised learning, that is).

Suppose you are trying to associate sentences with topics, and you have a list of possible topics topics = ['sports', 'soccer', 'politics']. It sounds like your data look something like this:

sentence                       | topics
-------------------------------|----------------------------------
'Barack Obama loves soccer'    | ['politics', 'sports', 'soccer']
'The parliament is important'  | ['politics']
'Soccer is fun'                | ['sports', 'soccer']


Then you need to one-hot encode the topics:

X = [['Barack Obama loves soccer'], ['The parliament is important'], ['Soccer is fun']]

Y = [[1, 1, 1], [1, 0, 0], [0, 1, 1]]


And then you train a neural network to predict not one but three (= number of topics) values."
In sklearn tfidf what is the difference between term frequecy and document frequency,"No, regarding the TF-IDF:

Term frequency (TF): means the count of a term in a specific document
Document frecuency (DF): means the count of the documents that contains a specific term

At first, I was also confused as I expected a TF-IDF per term, but in reality, you'll have a TF-IDF per term per document. Not all the documents has the same TF-IDF for the same word (i.e. the word line could have different TF-IDF for different documents).

In your example:

the TF of ""line"" in doc1 is 1 but in doc2 is 2
the DF of ""line"" is 4 (for all corpus)

Based on the following formula (
N
ùëÅ
 is number of documents in the corpus):
TF-IDF=TF‚ãÖ
N
DF
,
ùëá
ùêπ
-
ùêº
ùê∑
ùêπ
=
ùëá
ùêπ
‚ãÖ
ùëÅ
ùê∑
ùêπ
,

the TF-IDF for the term ""line"" for these documents are:

for doc1:
TF-IDF=1‚ãÖ
4
4
=1,
ùëá
ùêπ
-
ùêº
ùê∑
ùêπ
=
1
¬∑
4
4
=
1
,
for doc2:
TF-IDF=2‚ãÖ
4
4
=2,
ùëá
ùêπ
-
ùêº
ùê∑
ùêπ
=
2
¬∑
4
4
=
2
,

As you can see, the DF is a global metric (is the same for ""line"" across the corpus), but the TF is specific (could vary for ""line"" across the corpus)."
word2vec - log in the objective softmax function,"No, the logartihm doesn't disappear. From the equation

 ,

When you want to calculate

 , it essentially means calculating , 

Now , 

So ,

as ."
"Naive bayes, all of the elements in predict_proba output matrix are less than 0.5","No, your classifier can label text. It doesn't do it well but it is still almost 2 times better than random (for 7 classes, random will get you ~0.15 accuracy).

Looking at the test set is not enough. You need to create the same confusion matrix for you training set.

If the results you will get for the test set are similar in magnitude than maybe your model is too simple for the task or maybe you haven't trained it long enough.

If the results of the test set are good, than you might have a generalization problem (overfitting), which means that you need to increase the regularization during training. It also might mean that your training set comes from a different distribution than your test set."
Is there an alternative to nltk in golang?,"No. Not yet

There is no single package in Golang, which acts as versatile as nltk for NLP. However, there are several packages which aim to do it.

Here is a compiled list of such packages: https://github.com/gopherds/resources/blob/master/tooling/README.md#nlp"
What is a good explanation of Non Negative Matrix Factorization?,"Non-Negative Matrix Factorization (NMF) is described well in the paper by Lee and Seung, 1999.

Simply Put

NMF takes as an input a term-document matrix and generates a set of topics that represent weighted sets of co-occurring terms. The discovered topics form a basis that provides an efficient representation of the original documents.

About NMF

NMF is used for feature extraction and is generally seen to be useful when there are many attributes, particularly when the attributes are ambiguous or are not strong predictors. By combining attributes NMF can display patterns, topics, or themes which have importance.

In practice, one encounters NMF typically where text is involved. Consider an example, where the same word (love) in a document could different meanings:

I love lettuce wraps.
I love the way I feel when I'm on vacation in Mexico.
I love my dog, Euclid.
I love being a Data Scientist.

In all 4 cases, the word 'love' is used, but it has a different meaning to the reader. By combining attributes, NMF introduces context which creates additional predictive power.

""love""+""lettuce¬†wraps""¬†‚áí¬†""pleasure¬†by¬†food""
""
ùëô
ùëú
ùë£
ùëí
""
+
""
ùëô
ùëí
ùë°
ùë°
ùë¢
ùëê
ùëí
¬†
ùë§
ùëü
ùëé
ùëù
ùë†
""
¬†
‚áí
¬†
""
ùëù
ùëô
ùëí
ùëé
ùë†
ùë¢
ùëü
ùëí
¬†
ùëè
ùë¶
¬†
ùëì
ùëú
ùëú
ùëë
""
 
""love""+""vacation¬†in¬†Mexico""¬†‚áí¬†
""pleasure¬†by¬†relaxation""
""
ùëô
ùëú
ùë£
ùëí
""
+
""
ùë£
ùëé
ùëê
ùëé
ùë°
ùëñ
ùëú
ùëõ
¬†
ùëñ
ùëõ
¬†
ùëÄ
ùëí
ùë•
ùëñ
ùëê
ùëú
""
¬†
‚áí
¬†
""
ùëù
ùëô
ùëí
ùëé
ùë†
ùë¢
ùëü
ùëí
¬†
ùëè
ùë¶
¬†
ùëü
ùëí
ùëô
ùëé
ùë•
ùëé
ùë°
ùëñ
ùëú
ùëõ
""
 
""love""+""dog""¬†‚áí¬†""pleasure¬†by¬†companionship
""
""
ùëô
ùëú
ùë£
ùëí
""
+
""
ùëë
ùëú
ùëî
""
¬†
‚áí
¬†
""
ùëù
ùëô
ùëí
ùëé
ùë†
ùë¢
ùëü
ùëí
¬†
ùëè
ùë¶
¬†
ùëê
ùëú
ùëö
ùëù
ùëé
ùëõ
ùëñ
ùëú
ùëõ
ùë†
‚Ñé
ùëñ
ùëù
""
 
""love""+""Data¬†Scientist""¬†‚áí¬†
""pleasure¬†by¬†occupation""
""
ùëô
ùëú
ùë£
ùëí
""
+
""
ùê∑
ùëé
ùë°
ùëé
¬†
ùëÜ
ùëê
ùëñ
ùëí
ùëõ
ùë°
ùëñ
ùë†
ùë°
""
¬†
‚áí
¬†
""
ùëù
ùëô
ùëí
ùëé
ùë†
ùë¢
ùëü
ùëí
¬†
ùëè
ùë¶
¬†
ùëú
ùëê
ùëê
ùë¢
ùëù
ùëé
ùë°
ùëñ
ùëú
ùëõ
""

How Does It Happen

NMF breaks down the multivariate data by creating a user-defined number of features. Each one of these features is a combination of the original attribute set. It is also key to remember these coefficients of these linear combinations are non-negative.

Another way to think about it is that NMF breaks your original data features (let's call it V) into the product of two lower ranked matrices (let's call it W and H). NMF uses an iterative approach to modify the initial values of W and H so that the product approaches V. When the approximation error converges or the user-defined number of iterations is reached, NMF terminates.

NMF data preparation

Numeric attributes are normalized.
Missing numerical values are replaced with the mean.
Missing categorical values are replaced with the mode.

It is important to note that outliers can impact NMF significantly. In practice, most Data Scientist use a clipping transformation before binning or normalizing. In addition, NMF in many cases will benefit from normalization.

As in many other algorithmic cases, to improve matrix factorization, one needs to decrease the error tolerance (which will increase compute time)."
"Using different tokens for padding, end-of-sentence, and start-of-sentence in autoregressive sequence modeling?","Normally start-of-sequence is the same as end-of-sequence, that is, usually you use the end-of-sequence token to mark the start of the sequence.

The padding token is usually different, because that way you can easily compute the masks to use to mark which tokens should be ignored. end of sequence and start of sequence positions should not be ignored, so it's useful to have different padding than start/end of sequence."
Are there any objections to using the same (unlabelled) data for pre-training of a BERT-Based model and the downstream task?,"Not at all. A recent ACL paper by AllenAI even says this is the best way. They recommend continuing pre-training on the task data and claim that it reduces the problems caused by domain mismatch. So, if you train the model on the in-domain data from the very beginning, it is probably a good thing given you have enough data for that."
Is it domain adaptation?,"Not sure if it is domain adaptation as it has been described in literature, because the performance of your classifier in the target domain depends more in the alignment between the two domains due to the mistaken spelling etc. instead of measuring the divergence between the distribution of the two domains. If the distribution of the data between source and target is different then after the alignment you should use any appropriate method in order to tackle this difference and use the available information from the source domain.

P.S. Have you used the ""levinstein distance"" in order to align the two domains? (Match together the most similar ones etc...)"
Extract relevant vocabulary from a document,"Not sure if this is what you are looking for. Couldn't understand your question properly do you want 4-grams then this is the way you could do it. Assuming you have cleaned your text file by eliminating articles.

from nltk import ngrams
file=open('abc.txt','r')
txt = file.read()
n = 4
fourgrams = ngrams(txt.split(), n)
for grams in fourgrams:
  print (grams)


In case your file abc.txt contains ""This is random text to demonstrate the use of n-grams""

OUTPUT:
('This', 'is', 'random', 'text')
('is', 'random', 'text', 'to')
('random', 'text', 'to', 'demonstrate')
('text', 'to', 'demonstrate', 'the')
('to', 'demonstrate', 'the', 'use')
('demonstrate', 'the', 'use', 'of')
('the', 'use', 'of', 'n-grams')


If you do not want repetition then

words = 'This is random text we‚Äôre going to split apart'
x=[]
for word in words.split():
    x.append(word)
    if len(x) == 4:
        print(x)
        x=[]
print(x)

OUTPUT:
['This', 'is', 'random', 'text']
['we‚Äôre', 'going', 'to', 'split']
['apart']"
How to detect if one tweet is agreeing with another,"Not sure there is anything for that, you could check

is it a verbatim retweet
does it have the same sentiment
is the edit distance low

Or you can train your own model, where you label (agree) by hand and then build features."
Interpretation of the loss function for word2vec,"Not sure what the video said, but 
T
ùëá
 should not be the vocabulary size, but the training corpus size (number of all words).

For example, if your training corpus is

deep learning is popular . i love deep learning . i want to learn more about it.


Then when you sume up over 
T
ùëá
, you will sum up all the word pairs in the corpus including duplicates. The word pair (deep learning) is indeed calculated twice.

For details please refer to the original skip-gram paper, and notice the definitoin (1) on page 2."
Identifying most informative (sub)words/vectors that help classify a sample,"Note 1: I believe the technical term for what you are looking for is '(sub)word salience'.

First thought (which is not the best method, but could be worth a quick try if your examples are smallish) - Run the text with all words through fastText, and get the baseline most likely labels with probabilities (predict-prob function). Then remove each word that you are interested in testing, (maybe removing common words like 'a' 'the' etc), and compare the predicted probabilities of each obtained set of predicted_probs. The ones that give the greatest difference between baseline and missing wordX can be interpreted as the most informative.

A permutation of this idea is to get the embedding for the interesting words, and then get a distance measure (I assume cosine distance) to the labels you have. Then you have a problem, as they may be distributed evenly across labels in terms of distance, but if there is a group of words that are close to a particular label, that can be interpreted as informative.

I am sure there is a better technique to do this, so I am curious as well :). You may want to check these out (but they use additional modeling to get salience)

Learning Neural Word Salience Scores.

A NEW METHOD OF REGION EMBEDDING FOR TEXT CLASSIFICATION - this deals with the exact problems of how to identify the useful bits of an input text"
What is the difference between Okapi bm25 and NMSLIB?,"Note that I don't know nmslib and I'm not familiar with search optimization in general. However I know Okapi BM25 weighting.

How do they both (bm25, nmslib) differ?

These are two completely different things:

Okapi BM25 is a weighting scheme which has a better theoretical basis than the well known TFIDF weighting scheme. Both methods are intended to score words according to how ""important"" they are in the context of a document collection, mostly by giving more weight to words which appear rarely. As a weighting scheme, Okapi BM25 only provides a representation of the documents/queries, what you do with it is up to you.
nmslib is an optimized similarity search library. I assume that it takes as input any set of vectors for the documents and query. So one could provide them with vectors made of raw frequencies, TFIDF or anything else. What it does is just computing (as fast as possible) the most similar documents to a query, using whatever representation of documents is provided.

How can I pass bm25 weights to nmslib to create a better and faster search engine?

Since you mention that the results based on BM25 are satisfying, it means that the loss of quality is due to the nmslib search optimizations. There's no magic, the only way to make things fast is to do less comparisons, and sometimes that means discarding a potentially good candidate by mistake. So the problem is not about passing the BM25 weights, it's about understanding and tuning the parameters of nmslib: there are certainly parameters which allow the user to select an appropriate trade off between speed and quality."
NLP - Simple approach to identify commonalities in text comments between people,"Note: The questioner requests for the response to be simple. Although this approach below is not simple, it is an attempt to provide a perspective that could help.

Understanding the problem stated:
At a conceptual level, there are arguably 3 concepts in this question as given below.

Semantic similarity i.e. how similar the responses (in meaning) are to one another. In a way, this similarity could be loosely inferred as similar proposition.
Syntactic similarity i.e which aspects (tokens or entities or chunks) overlap between the responses.
Text classification i.e. ""is one feedback in agreement with another or neutral or against?"".

Evaluating approaches:
There could be many potential solutions. Following steps are one such attempt.

Step 1 Deploy a cosine similarity algorithm to measure the similarity between responses. In order to bring it a step closer to semantic similarity, use WORDNET to build the features for computing cosine similarity. This will ensure that tokens such as ""path"" are treated closer to token ""road"".

Step 2 Group responses (for the same question) beyond a threshold cosine value (example: 0.75) as similar responses. This can be loosely considered as a set of different responses for a question.

Step 3 Train a model to identify agreement or disagreement between responses. This training can be based on a classification algorithm or at the least based on a bag of words approach(hard coded tokens such as ""i dont agree"", ""it is incorrect"" etc). This step is perhaps the least scientific one but the only pragmatic one that the author can think of."
Natural language processing [closed],"Ok, something like this should work:

import json
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

with open('data_full.json','r') as f:
    data0 = f.read()
rawdata = json.loads(data0)

for dataset,instances in rawdata.items():
    for instance in instances:
        sentence = instance[0]
        label = instance[1]
        tokens = word_tokenize(sentence)
        print('in ',dataset,': ', '|'.join(tokens),'; label:', instance[1])
        lemmas = [ lemmatizer.lemmatize(token) for token in tokens ]
        print('          lemmas = ','|'.join(lemmas))



Note: you will probably need to install a few resources for nltk, follow the instructions given in the error messages."
Searching for a dataset that targets difficult words,"On the basis that difficult words are difficult because they are not commonly used, I think something as simple as TF-IDF would work well."
Add Custom Labels to NLTK Information Extractor,"Once you have your own lists of named entities, and you're only interested in extracting the relations, I believe there are simpler solutions (although I never tried relation extraction in NLTK, so I might be wrong):

ReVerb - a tool written in Java. Once it produces the results, you can simply keep the rows, where your labels are present as objects of the relation.

OpenIE - the successor of ReVerb (also written in Java). The authors claim better perfomance, and the output might be more informative.

IEPY - a relation extraction tool in Python. You should be able to provide your own labels/named entities using gazetees.

MITIE - this library has bindings in Python, and it offers relation extraction functionality."
Combine multiple vector fields for approximate nearest neighbor search,"One alternative is to re-encode the sentences and context together into the same vector space. This can be done with doc2vec or StarSpace.

If the sentences and contexts are in the same vector space, any approximate nearest neighbor (ann) search libraries could work."
What's the best way to detect bible verse mentions in a text?,"One approach is using Word Mover‚Äôs Distance (WMD). WMD is an algorithm for finding the distance between texts of different lengths, where each word is represented as a word embedding vector.

The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document.

For example:

 Source: ""From Word Embeddings To Document Distances"" Paper

Each sentence in the text could be compared to the 10 Bible verses. The bible verses could be ranked based on similarity."
Semantic similarity on a large dataset,"One approach would be to profile the code to empirically find the slowest parts. A quick visual scan of the code you referenced relieved inefficiencies.

For example, there are several list comprehensions:

labels = [headline[:20] for headline in headlines]

docs = [nlp(headline) for headline in headlines]


One straightforward way to speed up the code is converting those into generator expressions.

Additionally, there are nested for-loops:

similarity = []
for i in range(len(docs)):
    row = []
    for j in range(len(docs)):
        row.append(docs[i].similarity(docs[j]))
similarity.append(row)


You may not need to do a doc-by-doc comparison."
How to create a good list of stopwords,One approach would be to use tf-idf score. The words which occur in most of the queries will be of little help in differentiating the good search queries from bad ones. But ones which occur very frequently (high tf or term-frequency) in only few queries (high idf or inverse document frequency) as likely to be more important in distinguishing the good queries from the bad ones.
Software/Library Suggestion: Is there a usable open-source sequence tagger around?,"One can find sequence labelling libraries by searching for the term conditional random fields, the state of the art method. Probably one could also find libraries and tutorial by searching the term Named Entity Recognition, which is certainly the most standard NLP application of sequence labelling.

Here are a few libraries that I know of:

CRF++
crfsuite (there is a python wrapper)
Wapiti is a particularly efficient library (also with python wrapper)

See also this question."
Model to implement Question Answering System over structured data,"One efficient way is to use the roberta base squad 2 model, using your text as context and then ask questions. It should work well and the model can be downloaded directly.

git lfs install
git clone https://huggingface.co/deepset/roberta-base-squad2


Here is an extract of code to use it:

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

model_name = ""deepset/roberta-base-squad2""

# a) Get predictions
nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
QA_input = {
    'question': 'Why is model conversion important?',
    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'
}
res = nlp(QA_input)

# b) Load model & tokenizer
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)


You can also test the Robert squad 2 model using this link:

https://huggingface.co/deepset/roberta-base-squad2?context=Francisco+is+in+the+west+USA.&question=Where+is+San+Francisco%3F

You can also fine-tune your model on your data: https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial2_Finetune_a_model_on_your_data.ipynb"
Student answer evaluation,"One idea that could be leveraged - use all the model answers and learning material, find some sort of semantic similarity between all that text and the student answer. The higher the similarity with a model answer with a specific grade tag, the higher the probability of assigning that answer that particular grade. You can couple this with a metric of similarity between the student answer and learning material, more similarity between the two, higher probability of a better grade.

This seems like a very hard problem considering it is still a very active area of research in NLP, Some tools or concepts that might be interesting to you - Latent Semantic Analysis, word embeddings, text/document similarity, semantic coherence."
What model should I choose for Jobs recommender system?,"One method is using locality-sensitive hashing (LSH), an approximate nearest neighbor search method.

Each document, both resumes and job descriptions, is hashed into the same space. There are several ways to perform the hashing. An older method is shingling. The entire process is outlined in Chapter 3: Finding Similar Items in Mining of Massive Datasets. A newer method is doc2vec.

Once all the documents are in the same space, LSH maps similar items to the same ‚Äúbuckets‚Äù with high probability. It is a type of hashing where collisions are features, not bugs. The collisions are a variation of clustering. Given a new document, all similar documents are retrieved also. Given a resume, retrieve similar job descriptions. Given a job description, retrieve similar resumes. (After retrieval, filter out documents in the same document class)

Since LSH is an approximate algorithm, it scales well to millions of documents and handles noisy data. Resumes and job descriptions are noisy descriptions so it is appropriate to have an algorithm that can handle noise."
Distribution plot of word embeddings,"One method to visualize high-dimensional data in lower dimensions is t-sne, which has a python implementation in scikit-learn (here)

To calculate similarity between two sentences, having their embeddings, its common to use the cosine similarity"
Sentiment analysis using sources other than the IMDB data,"One of the most fundamental assumptions in machine learning is that the training data is 'similar' to the test data. Training makes no sense otherwise.

So the question is: How similar are reviews of, say, movies and stocks? Maybe somewhat, but not too much. Your movie-trained algorithm would certainly be able to deal with statements like 'This company is awesome' or 'Warning, do not buy this stock'.

But what about 'The stock price will explode/implode' or 'Concerning this stock, I am bearish/bullish'? The words explode, implode, bearish and bullish have probably never been expressions of sentiment in movie reviews."
How to identify sentiment of a given word from a sentence [closed],"One of the ways to do it is to build bag of words and apply a linear model for sentimental analysis. Then you can look at model's coefficients - positive coefficient means word is positive, negative coefficient - negative word."
How to extract values from unstructured text,One option is a data labeling system. The goal of a data labeling system is to allow less technical users to map raw data to established features. An example package is snorkel.
How to cluster sentences based on company names from a post(s) containing several company names using similarity metric.,"One option is Anchored CorEx which performs clustering with anchor words. For your problem, the anchor words would be company names."
Entity linking vs aliasing,"One option is to call them synonyms, a word or phrase that means exactly or nearly the same as another word or phrase.

Entity linking is mapping words of interest to corresponding unique entities in a knowledge base. It is useful to think of the unique entities in knowledge base as hashes and all surface forms as synonyms."
How to add extra word features other then word Embedding in Recurrent Neural Network model,"One option is to concatenate them, the second is to treat them as separate inputs. For example Keras offers such neural model: https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models"
How to use multiple text features for NLP classifier?,"One option is to embed all the information in a single space. The embedding space would contain the tokens and feature names.

Often times the tokens are changed to track the provenance. For example, science__DOMAIN and professor__COMMENT_BY.

An example of a package that does that is StarSpace."
How to train millions of doc2vec embeddings using GPU?,One option is to switch to a deep learning framework that supports distributed training.
What are the best methods to reduce the bag of words dimensionality?,One option is to use a pre-trained embedding space. The pre-trained embedding space will have much lower dimensionality and most likely all of the words in your corpus will be in it.
Text extraction from large pool of documents of different formats,"One option is to use Apache SOLR + Apache TIKA.

Apache TIKA has support for most common file formats, it extracts test content from files. Extracted text can be stored in SOLR. SOLR supports various kinds of text + aggregation queries.

Tutorials :

https://blog.webnersolutions.com/apache-solr-indexing-all-files-in-a-folder-recursively

https://lucene.apache.org/solr/guide/6_6/introduction-to-solr-indexing.html

https://lucene.apache.org/solr/guide/6_6/uploading-data-with-solr-cell-using-apache-tika.html"
How to find similar phrases,"One option is Word Mover‚Äôs Distance (WMD). WMD is an algorithm for finding the distance between phrases. WMD is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors.

The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document.

For example:

 Source: ""From Word Embeddings To Document Distances"" Paper

For your problem, you would have to generate candidates then use WMD to find the best matches for a given phrase."
Implementing back translation as a data augmentation for text classification,"One option, which I have discovered, is back-translation via the Unsupervised Data Augmentation repository made public by Google Research. This is based on this paper."
How does amazon's reviews that mention extracts topics from reviews?,"One possible approach I can see is as follows:

Amazon considers (until now and based on its historic data, and checked every X time) a possible number of frequent categories (i.e. labels in a classification context)
In the product you send, you can see the considered categories:

and the most frequent terms users have writen on their reviews, used as filters:

by applying some techniques like word embeddings, you can build a classifier to find which categories those terms belong to, based on some predefined category labels

new ones categories could be found with unsupervised clustering techniques"
Why ConLL is not in XML format,"One reason for sure is, that you can easialy open it in spreadsheet viewer."
Why gaussian assumption in GMM-HMM ASR?,"One reason Gaussian distributions are assumed is that it simplifies modeling since a Gaussian distribution only has 2 parameters to estimate (i.e., mu and sigma) and the properties of the distribution are well understood.

Gaussian distributions are often a reasonable assumption for acoustic data since acoustic data can modeled as a sum of independent random variables. According to the central limit theorem, the normalized sum of independent random variables tends toward a Gaussian distribution."
Can you make Q&A language model stay on topic?,"One solution could be to influence the answer using a context starter so that it stays in a specific field.

For instance, if chapter one is about ""car engines"", you can request the model starting with the related keywords, with a sentence such as ""About car engines,"" followed by the user's request.

In this way, you will get the model on track, and the following question will be strongly connected to the main topic you want.

You can hide this trick from the user and answer as if the context starter doesn't exist.

One piece of advice, keep the context starter short in order to get a good result for the users."
Word labeling with Tensorflow,"One solution to this problem can be found in Grammar as a Foreign Language. The paper outlines a system that ""translates"" English sentences into syntactic constituency parse trees using a sequence-to-sequence LSTM (Long Short-Term Memory model). A TensorFlow implementation can be found here."
Treating Word Embeddings as Multivariate Gaussian Random Variables,"One way to approach is to separate the steps.

Learn an embedding space of either words and/or documents. Learning an embedding makes no assumptions about the distributional form of the data. The data (e.g., words or documents) could be uniform, normal, or another distribution. The result is an embedding space.

Gaussian Mixture Model (GMM) clustering of entities in the embedding space. GMM assumes a multivariate normal distribution best fits the data since it only estimates the parameters related to a multivariate normal distribution. The more the underly features are not a multi-variate normal distribution, the worse a GMM model will fit.

It is up to you as the modeler to decide if the features are normally distributed enough that a GMM is a useful model. If GMM is not a useful model, then choose a non-parametric clustering algorithm (e.g., kernel density estimation) that has fewer assumptions than a GMM."
Degree of Profanity in a Sentence [closed],"One way to approach it is to split the sentence into tokens and count the number of tokens that are profanities.

import re

def tokenize(text): 
    return re.findall(r'\w+', text.lower())

profane_tokens = {""nerfherder""}

sentence = ""Why you stuck-up, half-witted, scruffy-looking nerfherder!""

tokens = tokenize(sentence)

# Rate: number of occurrences normalized by total number
degree_of_profanity = sum(1 for t in tokens if t in profane) / len(tokens)


This code will not handle multiple tokens and many profanities are multiple tokens."
Can we use doc2vec to detect outlier documents?,"One way to approach it:

Define a center tendency of the documents, a location in vector space.

Then, define a distance metric (e.g., cosine, Minkowski, or Mahalanobis).

Lastly, set a threshold in the distance metric that would define an outlier."
How word2vec can handle unseen / new words to bypass this for new classifications?,"One way to do this is to use context information to represent each word along with the w2v vector. You can choose to represent this information in any way you like: add another 600 dimensions (100D w2v vectors for 3 left and 3 right context words), just another 100D as the sum of context vectors or any other fixed length representation for your context.

When you're training, you can use a version of 'word dropout' that will utilize this information. 20% of the time, set your w2v vectors to zero, forcing your classifier to use the context dimensions to represent the word.

When you encounter a new word, the hope is, the classifier learned to use the context information as well as it learned to use the w2v information."
Using NLP to automate the categorization of user description,"One way to handle this is to use 'supervised classification'. In this model, you manually classify a subset of the data and use it to train your algorithm. Then, you feed the remaining data into your software to classify it.

This is accomplished with NLTK for Python (nltk.org).

If you are simply looking for strings like ""hardware"" and ""software"", this is a simple use case, and you will likely get decent results using a 'feature extractor', which informs your classifier which phrases in the document are relevant.

While it's possible to implement an automated method for finding the keywords, it sounds like you have a list in mind already, so you can skip that step and just use the tags you are aware of. (If your results aren't satisfactory the first time, this is something you might try later on).

That's an overview for getting started. If you are unhappy with the initial results, you can refine your classifier by introducing more complex methods, such as sentence segmentation, identification of dialogue act types, and decision trees. The sky is the limit (or more likely, your time is the limit)!

More info here."
How does the character convolution work in ELMo?,"One way to understand how ELMo's character convolutions work is by directly inspecting the source code.

There, in the forward method, you can see that the input to the network is a tensor of dimensions (batch_size, sequence_length, 50), where 50 is the maximum number of characters per word. Therefore, before passing the text to the network, it is segmented in words, and each character is encoded as an integer value.

This is what happens in the forward method before the highway layers:

The tensor gets prepended and appended sentence boundary tokens (beginning-of-sentence (BOS), end-of-sentence (EOS)).
The tensor goes through an embedding layer (this is somewhat similar to one-hot encoding and a matrix multiplication, see this answer). This gets us a vector for each character.
The tensor goes through different 1D convolutions of configurable kernel sizes.
The resulting activation maps are concatenated and passed as input to the highway networks

This architecture was proposed by kim et al. (2015), and is summarized well in one of the figures of the paper:"
"How to access GPT-3, BERT or alike?","OpenAI has not released the weights of GPT-3, so you have to access it through their API. However, all other popular models have been released and are easily accessible. This includes GPT-2, BERT, RoBERTa, Electra, etc.

The easiest way to access them all in a unified way is by means of the Transformers Python library by Huggingface. This library supports using the models from either Tensorflow or Pytorch, so it is very flexible.

The library has a repository with all the mentioned models, which are downloaded automatically the first time you use them from the code. This repository is called the ""model hub"", and you can browse its contents here."
What is the best approach for specified optical character recognition?,"Optical character recognition is a well-studied problem with many possible solutions (ressources). CNNs have proven to work extremely well even for hand-written character recognition. Take a look at this two papers:

Backpropagation Applied to Handwritten zip code
Comparaison of Classifier Methods: A Case Study in Handwritten Digit Recognition

Here is a beginner tutorial to do just that with Tensorflow.

If you need extra data to train your model, take a look at the MNIST dataset."
How to deal with spelling errors NLP,"Other options would be to...

Compare similar text sequences
Compare similar string sequences
Use fuzzy matching

Fuzzy Matching:

library(fuzzyjoin)
# https://stackoverflow.com/questions/26405895/how-can-i-match-fuzzy-match-strings-from-two-datasets

a <- data.frame(name = c('Ace Co', 'Bayes', 'asd', 'Bcy', 'Baes', 'Bays'),
                price = c(10, 13, 2, 1, 15, 1))
b <- data.frame(name = c('Ace Co.', 'Bayes Inc.', 'asdf'),
                qty = c(9, 99, 10))

# Find matches
stringdist_join(a, b, 
                by = ""name"",
                mode = ""left"",
                ignore_case = FALSE, 
                method = ""jw"", 
                max_dist = 99, 
                distance_col = ""dist""
) %>%
  group_by(name.x) %>%
  top_n(1, -dist)


This gives a ""distance"" between one and another word. So if you know the real product name, you might be able to find the erroneous names via the distance.

# A tibble: 6 x 5
# Groups:   name.x [6]
  name.x price name.y       qty   dist
  <fct>  <dbl> <fct>      <dbl>  <dbl>
1 Ace Co    10 Ace Co.        9 0.0476
2 Bayes     13 Bayes Inc.    99 0.167 
3 asd        2 asdf          10 0.0833
4 Bcy        1 Bayes Inc.    99 0.378 
5 Baes      15 Bayes Inc.    99 0.2   
6 Bays       1 Bayes Inc.    99 0.2  


Alternatively, you could look into similarity in string sequences:

If your product names are not to common from their string sequences and the erroneous names get only part of the name wrong, you could try something like:

library(dplyr)
library(tidytext)
library(fuzzyjoin)
library(tokenizers)

##############################
# Compare text sequences

text1=as.character(""Hi my name is Bixi and I like cycling a lot. It is just great!"")
mytext1=data_frame(text1)

text2=as.character(""Hi my name is Lissi and I'm good in swimming. It is just great!"")
mytext2=data_frame(text2)

ngram1 = unnest_tokens(mytext1, ngram, text1, token = ""ngrams"", n = 4)
ngram2 = unnest_tokens(mytext2, ngram, text2, token = ""ngrams"", n = 4)

# Find matching sequence(s)
semi_join(ngram1,ngram2)

##############################
# Compare sequences of single letters

ngram3=tokenize_character_shingles(mytext1$text1, n = 10, n_min = 10, strip_non_alphanum = FALSE)
ngram4=tokenize_character_shingles(mytext2$text2, n = 10, n_min = 10, strip_non_alphanum = FALSE)

ngram3=as.data.frame(ngram3)
ngram4=as.data.frame(ngram4)

# Find matching sequences of single letters
semi_join(ngram3,ngram4)


You may have a look at my Github. There are some more related options."
semantic relation or semantic relatedness between terms or phrases,"Other-than Ontologies do check word2vec, this tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words and can be used to find the closest words for a user-specified word"
Can natural language generation algorithms generate valid words too?,"Outside of context of NLG (thus not a direct answer to your whole question, but an answer to your question's title): Generating words from a character-level model has been done using RNNs exposed to large corpora of text, such as Wikipedia content, and trained to predict text character-by-character.

Used to generate content, the model is normally fed a few starting characters and asked to predict the next one. A choice is made from its most-likely predictions and fed back to it to continue the sequence.

Here is a blog showing some examples trained on some Shakespear and Wikipedia.

Such a network can and does generate nonsense words, although they are often fitting and might read like e.g. a noun or verb as you could expect depending on context. The sentence structure and grammar can come out sort of right, but the semantic content is usually complete gibberish."
Do I need a multilabel classification machine learning methodology or is it unnecessary?,"Overall I think your reasoning makes sense. Between the two options I agree that the multi-label setting is more appropriate, because it doesn't artificially force a single label by document.

While this is not exactly your question I'd like to suggest a few other ideas. Keep in mind that they are not necessarily better, just potentially interesting to consider as well.

Topic modelling could be an option. Advantages:

It's unsupervised: it just ""follows the text"", so there's no risk of any bias introduced in the training data or labeling method.
LDA is the standard method, it's parameterized which means that you choose the number of clusters/topics. You could generate 2 topics and manually check the top words associated with each topic in order to know which one corresponds to which label.
It's probabilistic, so for every document you would obtain a probability that the document belongs to topic A or topic B. This could be considered equivalent to multi-label, in the sense that a probability around 50% means that the document has the two labels.

However there's a risk that the topics would simply not correspond to your expected labels, it depends on the data. In this case it might be interesting to split into more than 2 topics, and then analyze the resulting topics and possibly label them.

Other idea: the design that you propose made me think of semi-supervised learning: given a small initial set of labeled data (initial training set) and a large set of unlabelled data, train and predict iteratively and add the newly labeled instances for which the model is most confident to the training set. There is also active learning, which aims to minimize the number of instances which need to be manually annotated. I'm not sure if these ideas are suitable for your case, mentioning just in case.

Finally in order to study the importance of words with respect to topics or documents TFIDF is too coarse in my opinion. Instead you could consider:

Conditional probabilities: for instance 
p(w|t)
ùëù
(
ùë§
|
ùë°
)
 is the probability that a document of topic 
t
ùë°
 contains the word 
w
ùë§
, while 
p(t|w)
ùëù
(
ùë°
|
ùë§
)
 is the probability among documents which contain 
w
ùë§
 of topic 
t
ùë°
. Advantages: simple to calculate, intuitive and informative.
For measuring the association between for instance a word 
w
ùë§
 and a topic 
t
ùë°
 (or two words), pointwise mutual information is quite standard."
NLP - Paraphrase extraction in Python,"Paraphrase detection is still a very active and very challenging research area, so it's unlikely that there are full-fledged standard libraries for this task since there is still no clear ""best solution"" to this problem.

In order to build a corpus you might want to look at how shared tasks/competitions have done it before. I know at least of SemEval which often proposes tasks related to paraphrases (there might be others). I haven't checked but usually the overview paper of the task (e.g. here) explains how the corpus was built and presents the main approaches submitted by participants."
What is ChunkParserI in nltk.chunk ? What exactly it has been called for?,"Parsing is the process of decomposing a string into it's constituent symbols (if the string is a word or a sequence of characters) or syntactic components (if the string is a meaningful textual entity like a short story, a scientific abstract or a sentence). In an NLP context, when one talks about parsing, he/she usually refers to the latter interpretation.

Chunking (in an NLP context) is a specific form of parsing in that it extracts groups of words in so-called 'chunks'. These groups of words or chunks are 'meaningful short phrases from the sentence (tagged with Part-of-Speech). Chunks are thus made up of words and the kinds of words are defined using the part-of-speech tags. One can even define a pattern or words that can't be a part of chunk and such words are known as 'chinks''1. The latter can be defined with chunking rules.

I assume the code you posted comes from ""Natural Language Processing: Python and NLTK"" by Hardeniya et al.2? From there i can find that the LocationChunker class 'starts by constructing a set of all locations in the gazetteers corpus. Then, it finds the maximum number of words in a single location string so it knows how many words it must look ahead when parsing a tagged sentence.' (cf. Chapter 5, p. 319)"
Handling data imbalance and class number for classification,"People talk a lot about data imbalance, but in general I think you don't need to worry about it unless your data is really imbalanced (like <1% of one label). 50/200 is fine. If you build a logistic regression model on that dataset, the model will be biased towards the majority class - but if you gave me no information about an input to classify, the prior probability is that the new input is a member of the majority class anyway.

The question you want to be able to answer is whether you are differentiating classes fine - so if you do have a minority class, do NOT use 'accuracy' as a metric. Use something like area under the ROC curve (commonly called AUC) instead.

If your data is really super imbalanced, you can either over-sample the minority class or use something called 'SMOTE', for ""Synthetic Minority Over-Sampling Technique"", which is a more advanced version of the same thing. Some algorithms also let you set higher weights on minority classes, which essentially incentivizes the model to pay attention to the minority class by making minority-class errors cost more.

To learn to differentiate between lots of classes, I think (a) you will need to have a ton of examples to learn from and (b) a model that's expressive enough to capture class differences (like deep neural network, or boosted decision tree), and (c) use softmax output. If those still don't work, you might try a 'model-free' approach like K-nearest-neighbors, which matches each input to the most similar labeled data. For kNN to work however, you need to have a very reasonable distance metric."
Alternatives to TF-IDF and Cosine Similarity when comparing documents of differing formats,"Perhaps you could use word embeddings to better represent the distance between certain skills. For instance, ""Python"" and ""R"" should be closer together than ""Python"" and ""Time management"" since they are both programming languages.

The whole idea is that words that appear in the same context should be closer.

Once you have these embeddings, you would have a set of skills for the candidate, and sets of skills of various size for the jobs. You could then use Earth Mover's Distance to calculate the distance between the sets. This distance measure is rather slow (quadratic time) so it might not scale well if you have many jobs to go through.

To deal with the scalability issue, you could perhaps rank the jobs based on how many skills the candidate has in common in the first place, and favor these jobs."
How to annotate text documents with meta-data?,"Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you've started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)

It might seem strange, but I would honestly suggest JSON. It's extremely well supported, supports a lot of structure, and is flexible enough that you shouldn't have to move from it for not being powerful enough. For your example, something like this:

{'text': 'I saw the company's manager last day."", {'Person': [{'name': 'John'}, {'indices': [0:1]}, etc...]}


The one big advantage you've got over any NLP-specific formats here is that JSON can be parsed in any environment, and since you'll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.

You can also implicitly store tokenization information if you want:

{""text"": [""I"", ""saw"", ""the"", ""company's"", ""manager"", ""last"", ""day.""]}


EDIT: To clarify the mapping of metadata is pretty open, but here's an example:

{'body': '<some_text>',
 'metadata': 
  {'<entity>':
    {'<attribute>': '<value>',
     'location': [<start_index>, <end_index>]
    }
  }
}


Hope that helps, let me know if you've got any more questions."
What is whole word masking in the recent BERT model?,"phil ##am #mon is a subword encoding of the single word ‚Äúphilammon‚Äù into 3 tokens. The comment just means that they mask words as opposed to tokens by taking into account subword encoding.

For more on subword encodings take a look at the slides from cs224, especially Byte Pair Encoding, from the Feb 14 subwords lecture at http://web.stanford.edu/class/cs224n/index.html#schedule."
Address parsing using spaCy,"Please look at my comment to add more information to your post. Based on the information you provided, here are my remarks:

SpaCy is trained to find locations, not addresses per se

If you use a ""common"" language, SpaCy is trained using WikiNER data, where locations aren't addresses but more like geographical places like city names, country names etc. So it's quite normal to not be able to detect full addresses.

You likely need to train your own entity recognizer. They detail how to do this on their website, including code samples: https://spacy.io/usage/training#ner

Don't underestimate SpaCy's rule-based matching

Is it a fancy neural network? No. Does it matter? Also no. SpaCy allows you to create rules to find entities and in cases like addresses which are generally following a pattern across entities."
Tagging Unix/Non-Unix logs using NLP,"PoS tagging works for natural language only and identifies grammatical parts of the sentence, nothing more. LSTM is an algorithm that can be used to predict series. Named Entity Recognition (NER) and Terminology Extraction could work if you have already data to engage in Information Extraction (IE). However, In order to use a these techniques you need to have a trained model and in order to train one you need data. In your case that would involve identifying and tagging parts of the sentence by hand and later training a model with that data.

The best approach, in my opinion, is just use regex to identify parts of the sentence as one of the approaches of Information Extraction and use hard coded rules to best identify what you are trying to replace later.

For instance, if you want to search the version in the example:

import re

s = 'Releasing version. 0.0.1 for Stackoverflow, on 01/01/2019. The coverage is 99% and build is passed.'

re.search(r'Releasing version(.*?)for', s).group(1)

' 0.0.1 '

Check out these resources that will help you parse a log in Python using Regex.

https://pythonicways.wordpress.com/2016/12/20/log-file-parsing-in-python/

https://medium.com/devops-challenge/apache-log-parser-using-python-8080fbc41dda

But if you prefer to use Named Entity Recognition or Terminology Extraction techniques you could hack a NER model and train it yourself with your data. Keep in mind though, that according to Poibeau, Thierry; Kosseim, Leila (2001). ""Proper Name Extraction from Non-Journalistic Texts"". Language and Computers. 37 (1): 144‚Äì157.:

Research indicates that even state-of-the-art NER systems are brittle, meaning that NER systems developed for one domain do not typically perform well on other domains.

And according to Wikipedia: Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.

So even if you pull that off, accuracy will be less than if you just extracted data using regex. An approximate of a reduction of 97% to 93% only for named entities (companies, names, etc.) Accuracy reduction will be much less in you case.

Check this link for more information about Information Extraction: https://web.stanford.edu/~jurafsky/slp3/17.pdf"
any efficient way to find surrounding adjective/verbs with respect to the target phrase in python [updated]?,"POS-tagging consist of qualifying words by attaching a Part-Of-Speech to it. Part-Of-Speech is a tag that indicates the role of a word in a sentence (e.g. a noun, a transitive verb, a comparative adjective, etc.). You need this to know if a word is an adjective, and it is easily done with the nltk package you are using [source]:

>> nltk.pos_tag(""The grand jury"")
>> ('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN')


Here, JJ means ""Adjective"" and ""NN"" means ""Common Noun"".

In your case, you are interested in neighbors adjectives. Does that mean ""the closest adjective"" in the sentence ? Or adjectives within a radius of the target, if any? Depending on the definition, the way to do it differs.

For adjectives within a radius, as you have already selected words within a radius using the snippet you mentioned, you can POS-tag them and then select only those with a tag that indicates an adjective tag.

>> adjective_tags = [""JJ"", ""JJR"", ""JJS""]
>> close_adjectives_list = [a[0] for a in nltk.pos_tag("" "".join(close_words_list)) if a[1] in adjective_tags ]


You can look at this answer that list most of existing POS-tags."
Measuring precision and recall,"Precision and recall are ""hard"" metrics. They are measure if the model's prediction is exactly the same as the target label.

Often times systems like yours can use a more flexible metric such as top-5 error rate, the model is considered to have generated the correct response if the target label is one of the model‚Äôs top 5 predictions."
Convert natural language text to structured data,"prepare an excel sheet with columns as - sentence, color,material,fit ,style,sleeve_lenth, etc. Treat it as training data.

https://spacy.io/usage/spacy-101#training, use spacy to train a model where you feed the model - sentence ,tags and spantokens , something like - [{'red color shirt':[['color',(0,3)],['type',(10,14)]].

learn how to create a train the model given in spacy tutorial and train a model.

you can also use a fast,rule based method - https://spacy.io/usage/rule-based-matching of spacy on a simple english model and adding patterns to it.

I would prefer point 4 at the beginning, once the rule based model is prepared, you can run it on a bunch of data to form the excel data mentioned in point 1 for training."
Ranking skills depending on similarity,"Pretty late but I'm surprised this wasn't answered more. ""Cosine similarity"" is a great technique to try, though simply letting users search with a hard string and then ranking by popularity isn't so bad (e.g. ""dutch"" brings up everything with ""dutch"" in it, though I would discard mid-word matches, so ""ball"" wouldn't return ""football"", but would return ""ball room dancing"").

I'd say in any approach a main issue will be deduplicating previous (non-standardized) skills input by users that weren't quite standardized. You could also try replacing the candidate skills with versions that have different synonyms substituted at search time, e.g. ""soccer coaching"" might be stored also as ""football coaching"" if most of your content is from Europeans.

Sometimes extreme accuracy might not be the best goal, though... You may want to encourage users to explore new skills that they never knew existed! Not sure what your needs are...

Whatever you settle on, it might be worth building a semi-hand-crafted test set of queries and relevant results so that you can see if the performance is terrible (Google precision and recall in the context of search results)."
Does it make sense to use TF-IDF to extract most important tokens from a corpus?,"Provided limited information & context you have provided, I would suggest you to look for feature selection when each dimension belongs to a word. Feature selection will give you most important words. Most important words in the sense, words deciding the decision surface of the model."
Who is supposed to label my sentiment analysis? Linguistics or psychologist?,"Psychology was mentioned because psychology has a long history of assigning numeric scores to subjective topics. One of the most important concepts is inter-rater reliability, how much do different people agree on an interpretation.

Other concepts that are useful are the degree of subjectivity and the degree of polarity (vs. assigning binary polarity labels). This is how Python's TextBlob package models sentiment."
Plagiarism detection with Python,"Python's fuzzywuzzy uses Levenshtein Distance which looks at character level differences.

You have to explore other approaches to text similarity. Find algorithms that nonlinearly weight n-grams differences, such as Q-gram.

python-string-similarity repo has implementations of many text similarity algorithms."
Predicting the missing word using fasttext pretrained word embedding models (CBOW vs skipgram),"Question 1:

To do so, I would use the Gensim wrapper of FastText because Gensim has a predict_output_word which does exactly what you want. Given a list of context words, it provides the most fitting words.

Question 2:

It is up to the user. FastText isn't inherently CBOW or Skipgram. See this

Question 3:

Yes, even though CBOW and SkipGram are different training procedures, they share a common goal. Both will generate word embeddings where (hopefully) words that are semantically close also have embeddings that are close. The main difference between SkipGram and CBOW is the inherent heuristic used for semantic closeness."
Answer to Question,"Question answering (QA) is a complex problem and an active field of research. There are probably some academic prototypes around, but I doubt there's any general-purpose ready-to-use QA library. However there are probably state of the art implementations for closed QA, i.e. QA restricted to a specific domain (I'm not aware of any specific library though).

Paraphrasing is a related but different problem, and also an active research question.

Extracting keywords is a much more standard task and is an important part of traditional Information Retrieval methods."
Why is hard for neural machine translation model to learn rare words?,"Rare words are not a problem only for NMT, they are a problem for MT in general. The reason is simple: in order to accurately translate a word in any particular context, the model needs to see as many examples as possible during the training stage. By definition the training data contains very few occurrences of rare words (especially hapax words which occur only once), so the model doesn't have enough information to learn their translation properly."
How to classify support call texts?,"Rather than use an external dictionary of keywords that are indicative of target class, you may want to take your raw data (or rather a random subset of it), then hand-label your instances (assign each row a label, BEST_EFFORT or URGENT). This becomes your training data - each row of data can be transformed into a bag-of-words vector indicating the presence/absence of the word in that particular text. You can train a classifier on this data, for example a naive bayes classifier, which can then be tested on the held out unseen test data. The advantages of the proposed approach are: (1) automated computation of features vs. hand created dictionary; (2) probabilistic/weighted indicators of class vs. binary dictionary indicators."
ngram and RNN prediction rate wrt word index,"Recurrent Neural Network (RNN) create a single state vector over time. Thus that curve is to be expected. Initially, the state vector does have enough information to make a quality prediction. Then quickly reaches asymptotic performances. Overall, the predictions are between 15% and 22% correct.

The shape of the graph might be a function of sentence length in the training corpus. Possibly sentences could be between 3 and 7 words long. The drop could be because there is less training data for longer sentences."
What Preprocessing is Needed for Semantic Search Using Pre-trained Hugging Face Transformers?,"Resumes are quite different from classic text because there are many proper nouns (names, companies, places, etc.) and other data difficult to classify (phone numbers, marks, age, etc.).

That's why you can use lighter versions like DistilBert to train your data on resumes and get good results.

Therefore, you should first separate every paragraph and label them to classify resumes correctly.

You can also use pre-trained models like this one and fine-tune them with your data.

However, this is not a semantic search yet. After classifying resumes content correctly, you can use a semantic transformer to look for field similarity among the same resumes category.

Note: the computing power might be very high if you have thousands of CVs to compare with, even if you detect the search category and process the comparisons in one category only."
Multilingual Bert sentence vector captures language used more than meaning - working as interned?,"Sadly, I don't think that Multilingual BERT is the magic bullet that you hoped for. As you can see in Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT (Wu and Dredze, 2019), the mBERT was not trained with any explicit cross-lingual task (for example, predicting a sentence from one language given a sentence from another language).

Rather, it was trained using sentences from Wikipedia in multiple languages, forcing the network to account for multiple languages but not to make the connections between them.

In other words, the model is trained with predicting a masked instance of 'cat' as 'cat' given the rest of the (unmasked) sentence, and predicting a foreign word meaning 'cat' in a masked space given a sentence in that language. This setup is does not push the model towards making the connection.

You might want to have a look in Facebook's LASER, which was explicitly trained to match sentences from different languages.

P.S

The fact that the sentences do not have the same representation does not mean that the mBERT cannot be used for zero-shot transfer learning across languages. Again, please see Wu and Dredze"
What is the difference between batch_encode_plus() and encode_plus(),"See also the huggingface documentation, but as the name suggests batch_encode_plus tokenizes a batch of (pairs of) sequences whereas encode_plus tokenizes just a single sequence. Looking at the documentation both of these methods are deprecated and you use __call__ instead, which checks by itself if the inputs are batched or not and calls the correct method (see the source code with the is_batched variable and if statement)."
Efficient database model for storing data indexed by n-grams,"See Lucene NGramTokenizer

Are you sure you can't just use lucene or similar indexing techniques?

Inverted indexes will store the n-gram only once, then just the document ids that contain the ngram; they don't store this as highly redundant raw text.

As for finding ngrams that contain your query sub-n-gram, I would build an index on the observed ngrams, e.g. using a second lucene index, or any other substring index such as a trie or suffix tree. If your data is dynamic, probably lucene is a reasonable choice, using phrase queries to find your n-grams."
Classify event announcement,"Semi-supervised learning and in particular active learning could be considered in cases like this:

The general semi-supervised setting consists in training a model from an initially small training set by applying it iteratively to unlabelled instances. There are various methods to minimize the risk of training the model on wrongly classified instances.
Active learning is a variant of semi-supervised learning where the model queries the human expert for annotations, but the instances are carefully selected in order to minimize the amount of human labor.
There is also bootstrapping, where one would focus on the positive instances: apply the original model to the unlabelled data, the manually annotate only the instances which are predicted as positive (useful only in cases where the positive class is much smaller than the negative one)."
What the differences between self-supervised/semi-supervised in NLP?,"Semi-supervised learning is having label for a fraction of data, but in self-supervised there is no label available. Imagine a huge question/answer dataset. No one labels that data but you can learn question answering right? Because you are able to retrieve relation between question and answer from data.

Or in modeling documents you need sentences which are similar and sentences which are dissimilar in order to learn document embedding but these detailed labels are usually not available. In this case you count sentences from same document as similar and sentences from two different documents as dissimilar and train your model (example idea: you can run a topic modeling on data and make similar/dissimilar labels more accurate). It is called self training."
On a multi lingual sentiment corpus,"Several questions and thoughts come to mind.

What languages are in the corpus? This may impact what services you can leverage.
I like the ""Sentiment Idea"" for languages that are supported natively by the services you mentioned.
I would keep the ""Language Idea"" as the last resort as it is possible that the translation engine may not capture the sentiment of the original language.
Mechanical Turk would be a good option if you can limit the number of samples sent for classification. For each language, you could try clustering the passages by, for example, word count into 30 (you pick) clusters and then perform sampling within the clusters to identify candidate passages to send to Mechanical Turk. I have used this technique to try to sample across the vector space more uniformly.

Don't dismiss oW_'s comment. You should seriously consider breaking the articles into paragraphs. You can always aggregate the paragraph scores to the article, but it's hard to get one representative score as the text gets longer.

HTH"
General approach to extract key text from sentence (nlp),"Shallow Natural Language Processing technique can be used to extract concepts from sentence.

-------------------------------------------

Shallow NLP technique steps:

Convert the sentence to lowercase

Remove stopwords (these are common words found in a language. Words like for, very, and, of, are, etc, are common stop words)

Extract n-gram i.e., a contiguous sequence of¬†n¬†items from a given sequence¬†of text (simply increasing n, model can be used to store more context)

Assign a syntactic label (noun, verb etc.)

Knowledge extraction from text through semantic/syntactic analysis approach i.e., try to retain words that hold higher weight in a sentence like Noun/Verb

-------------------------------------------

Lets examine the results of applying the above steps to your given sentence Complimentary gym access for two for the length of stay ($12 value per person per day).

1-gram Results: gym, access, length, stay, value, person, day

Summary of step 1 through 4 of shallow NLP:

1-gram          PoS_Tag   Stopword (Yes/No)?    PoS Tag Description
-------------------------------------------------------------------    
Complimentary   NNP                             Proper noun, singular
gym             NN                              Noun, singular or mass
access          NN                              Noun, singular or mass
for             IN         Yes                  Preposition or subordinating conjunction
two             CD                              Cardinal number
for             IN         Yes                  Preposition or subordinating conjunction
the             DT         Yes                  Determiner
length          NN                              Noun, singular or mass
of              IN         Yes                  Preposition or subordinating conjunction
stay            NN                              Noun, singular or mass
($12            CD                              Cardinal number
value           NN                              Noun, singular or mass
per             IN                              Preposition or subordinating conjunction
person          NN                              Noun, singular or mass
per             IN                              Preposition or subordinating conjunction
day)            NN                              Noun, singular or mass

Step 4: Retaining only the Noun/Verbs we end up with gym, access, length, stay, value, person, day


Lets increase n to store more context and remove stopwords.

2-gram Results: complimentary gym, gym access, length stay, stay value

Summary of step 1 through 4 of shallow NLP:

2-gram              Pos Tag
---------------------------
access two          NN CD
complimentary gym   NNP NN
gym access          NN NN
length stay         NN NN
per day             IN NN
per person          IN NN
person per          NN IN
stay value          NN NN
two length          CD NN
value per           NN IN

Step 5: Retaining only the Noun/Verb combination we end up with complimentary gym, gym access, length stay, stay value


3-gram Results: complimentary gym access, length stay value, person per day

Summary of step 1 through 4 of shallow NLP:

3-gram                      Pos Tag
-------------------------------------
access two length           NN CD NN
complimentary gym access    NNP NN NN
gym access two              NN NN CD
length stay value           NN NN NN
per person per              IN NN IN
person per day              NN IN NN
stay value per              NN NN IN
two length stay             CD NN NN
value per person            NN IN NN


Step 5: Retaining only the Noun/Verb combination we end up with complimentary gym access, length stay value, person per day


Things to remember:

Refer the Penn tree bank to understand PoS tag description
Depending on your data and the business context you can decide the n value to extract n-grams from sentence
Adding domain specific stop words would increase the quality of concept/theme extraction
Deep NLP technique will give better results i.e., rather than n-gram, detect relationships within the sentences and represent/express as complex construction to retain the context. For additional info, see this

Tools:

You can consider using OpenNLP / StanfordNLP for Part of Speech tagging. Most of the programming language have supporting library for OpenNLP/StanfordNLP. You can choose the language based on your comfort. Below is the sample R code I used for PoS tagging.

Sample R code:

Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7') # for 32-bit version
library(rJava)
require(""openNLP"")
require(""NLP"")

s <- paste(""Complimentary gym access for two for the length of stay $12 value per person per day"")

tagPOS <-  function(x, ...) {
  s <- as.String(x)
    word_token_annotator <- Maxent_Word_Token_Annotator()
    a2 <- Annotation(1L, ""sentence"", 1L, nchar(s))
    a2 <- annotate(s, word_token_annotator, a2)
    a3 <- annotate(s, Maxent_POS_Tag_Annotator(), a2)
    a3w <- a3[a3$type == ""word""]
    POStags <- unlist(lapply(a3w$features, `[[`, ""POS""))
    POStagged <- paste(sprintf(""%s/%s"", s[a3w], POStags), collapse = "" "")
    list(POStagged = POStagged, POStags = POStags)
  }
  
  tagged_str <-  tagPOS(s)
  tagged_str

#$POStagged
#[1] ""Complimentary/NNP gym/NN access/NN for/IN two/CD for/IN the/DT length/NN of/IN stay/NN $/$ 12/CD value/NN per/IN     person/NN per/IN day/NN""
#
#$POStags
#[1] ""NNP"" ""NN""  ""NN""  ""IN""  ""CD""  ""IN""  ""DT""  ""NN""  ""IN""  ""NN""  ""$""   ""CD"" 
#[13] ""NN""  ""IN""  ""NN""  ""IN""  ""NN"" 


Additional readings on Shallow & Deep NLP:

Integrating Shallow and Deep NLP for Information Extraction"
Determine document novelty/similarity with the aid of Latent Dirichlet allocation (LDA) or Named Entities,"Short answer: Topic Models and LDA aren't suited at all for novelty detection. I did some experiments with data that show a succession of topics in temporal order, and at the boundaries of the time interval one topic gets really high percentages in the document-topic matrix, i.e., the upcoming new trend (or the outrunning old trend) is not seen by LDA.

Topic Models are also not really suited for ""online"" processing‚Äîyou read the whole corpus of documents once and than infer the topics."
What are key dataset requirements for topic models and word embeddings?,"Similarity of documents can be done with varied approaches.

As your documents are based on domain based words, you could employ a tfidf representation for each document and compute similarity based on this.

Previously, I have used word2vec representations of words and constructed document vectors by taking an average of all the vectors ( as one of the approaches ) and did a cosine similarity between these vectors. Basically an n*n similarity computation.

When you do topic modelling, even though the idea is to find documents which have similar topic distribution, there is a prior to the approach i.e identifying number of topics. Just using randomly 100 topics without looking at the distribution of the words might lead you on a wrong path.

A very interesting approach I have recently come across is combining topic modelling and word vectors, here is the link to a blog by stitchfix : http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/"
Next sentence prediction in RoBERTa,"Similarly to BERT, they sample negative (i.e., non-adjacent) examples and train a classifier telling whether the sentence are consecutive or not."
How to choose similarity measurement between sentences and paragraphs,"Simphile NLP package creator here. Choosing the text similarity method that works best for your application can be difficult. Ideally you have a large sample set with many known positives (i.e. text that are sufficiently related to the reference). You would choose the method that orders the sample set such that the positives are concentrated at the head; this could be measured with the area under the precision-recall curve.

The Simphile package makes it easy to try several different methods:

Install:

pip install simphile


Choose your favorite method. This example shows three:

from simphile import jaccard_similarity, euclidian_similarity, compression_similarity

text_a = ""I love dogs""
text_b = ""I love cats""

print(f""Jaccard Similarity: {jaccard_similarity(text_a, text_b)}"")
print(f""Euclidian Similarity: {euclidian_similarity(text_a, text_b)}"")
print(f""Compression Similarity: {compression_similarity(text_a, text_b)}"")

Compression Similairty ‚Äì leverages the pattern recognition of compression algorithms
Euclidian Similarity ‚Äì Treats text like points in multi-dimensional space and calculates their closeness
Jaccard Similairy ‚Äì Texts are more similar the more their words overlap

More docs and examples at the repo"
does entity recognition comes under classification problem?,"Simply put, Named Entity Recognition (NER) is a multi-class structured prediction (classification) problem, so you have a sequence of words and you want to label each one most of the time with these labels ( start-of-a-person-name, continue-of-a-person-name, start-of-an-org-name, continue-of-an-org-name, start-of-a-location-name, continue-of-a-location-name, other). Note that these are not the only classes, classes with different granularity can also be used.

If you want to solve this problem with SVM you can use StructSVM or other variations of SVM for structure prediction. Though the common baseline for this task uses Maximum Entropy (Maxent)(log-linear) models."
Representation options of strings (keywords/topics) in models,"Since (word-based) one-hot encoding and real-valued vector representations are already mentioned in the question, I would only add the n-gram representation, especially the character-based n-gram representation.

For word-based n-gram representations you consider not individual words, but their ordered combinations in the text and use the one-hot encoding for the combinations. E.g. for n=2 you might end up with the bigrams [""John likes"", ""likes to"", ""to watch"", ""watch movies""] and each of them would be assigned to some dimension using a static index.

This also works with characters, so you can represent the word ""encoding"" e.g. with those 3-grams: [""enc"", ""nco"", ""cod"", ""odi"", ""din"", ""ing""]. The one-hot encodings of n-grams are typically added, so multiple occurances of the same n-gram are recognizable in the resulting Bag-of-n-grams representation. This kind of representation is especially useful for languages with rich morphology and/or compound words. In a one-hot representation each single word form would be encoded in its own dimension whereas a character n-gram approach helps preserve similarity between different forms. An example in the English language would be the similarity between ""encode"", ""encoded"" and ""encoding"" which would stay preserved this way. Similar techniques are also used by some word embedding algorithms which consider subword information like e.g. FastText.

Also, although it's not directly an encoding, but depending on your use case and language it might be worth looking at different preprocessing options like lemmatization and stemming where you reduce different word forms to their base form. This would also affect the choice of representation, e.g. the word-based one-hot encoding might make more sense if you choose to use these preprocessing techniques."
Clause type classification,"Since all the sentences length are not highly varying, you can use sentence embeddings and do the clustering on top of that.

For example,

Text => USE => vector[1024] => KMeans


USE - Universal sentence encoders

Kmeans - SKlearn Module

You can adjust the number of clusters using these techniques."
How to choose threshold for gensim Phrases when generating bigrams?,"Since min_count and threshold are hyperparameters, better values could be found through cross validation. Evaluate a range of values to empirically find the values that have the highest performance on a validation set."
"How can I create a ""trained"" dataset for categorizing news articles?","Since news article categorization is a relatively common task, it would be fastest and easiest to use a already labeled training data.

Options include:

scikit-learn's 20 newsgroups dataset
Kaggle's BBC News Classification
Kaggle's India News Headlines Dataset

There is nothing preventing you from training a model on those datasets and then using that model for commercial purposes."
Classify tweets by topic [closed],"Since there are no predefined topics, the task is unsupervised: the goal is to group tweets which are semantically similar together (as opposed to classification, which requires training a model to predict among specific classes).

The standard unsupervised approach is topic modelling. In the traditional LDA approach, a topic model groups the documents into clusters and also provides the probability of a word given a topic, so a list of ""top words"" by topic can be extracted from the model. LDA requires the number of topics as input parameter but Hierarchical Dirichlet Processes can be used to avoid this issue (it's less common however)."
Classification of scanned documents in pdf files using deep learning or NLP,"Since this is a unsupervized problem, you need to try to extract ""topics"" using topic modeling. There are a number of tools available in Python, e.g. from sklearn or spacy.

Basic workflow:

Extract text from PDF
Text preprocessing (lowercase, stemming etc)
Topic modeling
Return ""topic"" per page"
Text processing,"Since you are going to use TF-IDF representations, you already have a feature matrix. To calculate cosine similairty between all vectors, you can use:

from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(tfidfmat)
#tfidfmat is your TF-IDF matrix


#Use numpy arrays

To begin clustering, you can use K-means algorithm to begin with, and use cosine similairty as the distance metric. Here's an example from scikit-learn itself on clustering documents.

Further things to try: If you find the above methods not working to your expectations, look into word2vec and doc2vec, and instead of using tfidf, which isa Bag of Words approach, use word vector representations. Here is a good blog explaining the concept."
One-Class Text Classification,"Since you mention deep learning, one option is to embedded the documents and then cluster the documents.

Each cluster could be labeled as ""Good"" or ""Not Good"". The labeling could be done by hand or automatically by voting with existing labels (e.g., if a majority of the documents are ""Good"" then the entire cluster is ""Good"").

The trained regions could also be used for prediction."
"I have 2 Columns of text, Should I use different vectorizer and Embeddings for each or just one?","Since you mention that the two texts are related, choose the same text vectorizer.

The average length of each text instance has almost no impact on choosing a text vectorizer."
regex to remove repeating words in a sentence,"Since you were working with RegEx, I will ofer a RegEx solution.

I will also show that you need to also take care to first remove punctuation. (I will not go down the rabbit-hole of re-sinserting the punctuation back where it was!)

A RegEx solution:
import re
sentence = 'I need need to learn regex... regex from scratch!'

# remove punctuation
# the unicode flag makes it work for more letter types (non-ascii)
no_punc = re.sub(r'[^\w\s]', '', sentence, re.UNICODE)
print('No punctuation:', no_punc)

# remove duplicates
re_output = re.sub(r'\b(\w+)( \1\b)+', r'\1', no_punc)
print('No duplicates:', re_output)


Returns:

No punctuation: I need need to learn regex regex from scratch
No duplicates: I need to learn regex from scratch

\b : matches word boundaries
\w : any word character
\1 : replaces the matches with the second word found - the group in the second set of parentheses

The parts in parentheses are referred to as groups, and you can do things like name them and refer to them later in a regex. This pattern should recursively catch repeating words, so if there were 10 in a row, they get replaced with just the final occurence.

Have a look here for more detailed definitions of the regex patterns.

The more pythonic (looking) way

It has to be said that the groupby method has a certain python-zen feel about it! Simple, easy to read, beautiful.

Here I just show another way of removing the punctuation, making use of the string module, translating any punctuation characters into None (which removes them):

from itertools import groupby
import string

sentence = 'I need need to learn regex... regex from scratch!'

# Remove punctuation
sent_map = sentence.maketrans(dict.fromkeys(string.punctuation))
sent_clean = sentence.translate(sent_map)
print('Clean sentence:', sent_clean)

no_dupes = ([k for k, v in groupby(sent_clean.split())])
print('No duplicates:', no_dupes)

# Put the list back together into a sentence
groupby_output = ' '.join(no_dupes)
print('Final output:', groupby_output)

# At least for this toy example, the outputs are identical:
print('Identical output:', re_output == groupby_output)


Returns:

Clean sentence: I need need to learn regex regex from scratch
No duplicates: ['I', 'need', 'to', 'learn', 'regex', 'from', 'scratch']
Final output: I need to learn regex from scratch
Identical output: True

Benchmarks

Out of curiosity, I dumped the lines above into functions and ran a simple benchmark:

RegEx:

In [1]: %timeit remove_regex(sentence)
8.17 ¬µs ¬± 88.6 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)


groupby:

In [2]: %timeit remove_groupby(sentence)
5.89 ¬µs ¬± 527 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)


I had read that regex would be faster these days (using Python3.6) - but it seems that sticking to beautiful code pays off in this case!

Disclaimer: the example sentence was very short. This result might not scale to sentences with more/less repeated words and punctuation!"
Why the label is not explicitly involved in the loss function of skip-gram?,"Skip-gram is self supervised, the model uses the current word to predict the surrounding window of context words.

The skip-gram loss function is the negative log likelihood of the observed context words given the target word. The goal of training is minimize that loss so the model makes better predictions of context words."
Understanding Transfer Learning of Word Embeddings,"So Named Entity Recognition is a mechanism where you ask your network to learn about how to detect entities given word vectors as the input.

The theoretical aspect of word embeddings is that based on your construction of sentences, the word embeddings for Orange and Apple are very similar i.e their cosine angle is very small.

In Named entity recognition you use these word embeddings and feed them into a network where the data you are training one has tags for each of the word embeddings i.e entities or normal words. So your network is actually understanding the relationship of the word embeddings and how to tag them. This makes it incredible for us to see that Apple gets detected even though its not in the training set, precisely where word embeddings help us quite well because the word embeddings are usually trained on a large corpus of data containing the words apple, orange and other tokens. This is where transfer learning helps because you are using the word embeddings trained in an unsupervised manner and then used to learn about entities.

Hope that helps. I can elaborate if required."
How to get columns from unsorted rows in Pandas? (MALLET),"So one way to do it is to use python slice operators to grab every other value in the line and zip them (along with the filename) into 3-tuples, e.g.:

data = []
malletOutput = open('doc-topics','r').readlines()

for line in malletOutput:
    line = line.split('\t')[1:-1] # slicing out useless leading index and trailing \n
    _id = line[0]

    tIndicies = map(int,line[1::2])
    tVals = map(float, line[2::2])
    topics = sorted(zip(tIndicies, tVals))
    topics = [t + tuple([_id]) for t in topics] # add _id
    for t in topics:
        data.append(t)


Now you have a list of 3-tuples which you can add to a dataframe. A simple pivot from there results in the desired shape:

df = pd.DataFrame(data)
df = df.pivot(index=2,columns=0,values=1)"
First two principal components explain 100% variance of data set with 300 features,"So the question is asking why the first two principal components of your encoded text data is encapsulating all of the variation in the data.

One potential issue could be the averaging over word vectors.

Suppose for a particular feature across word vectors for a particular post f, there could be an array of positive and negative values. When we then apply an average over f we could zero out the dimension and thus cause greater data sparsity, which could explain for what you are seeing (this zero value will exist regardless of whether you multiply this average with the td-idf or not). It could be the case that this sort of thing is happening across multiple dimensions in your text embeddings / feature vectors.

With this, you might need to think of another way of deriving a text embedding, maybe used Doc2Vec instead, which follows the same principles as Word2Vec, but instead derives document embeddings, which encapsulates the meaning of a section of text instead of word embeddings, which encapsulates the meaning of an individual word within a section of text."
Detecting if a sentence contains a numeric series,"So there are many ways to denote a series. How are you going to parse the series down to determine the values if you don't know the format?

Determining if the label has a series does not get you to the specific numbers in the series.

2,3,5,7 parses out to 4 numbers

Is 6 in 1996? I assume that is one number and 1996 != 6

""55,56,57"" is series with a 6 but but not the number 6

Does 7-9 parse out to 2 numbers or 3 number
Is 6 in 7-9?
If 6 is in 7-9 identifying that as a series does not answer that question.

How many ways can there be represent a series that regex got out of hand? For each format of series you also need to parse the values. You need to know the format of the series to parse out the numbers. I would have a set of regex mapped to set of parsers.

Maybe use machine learning to identity new series formats but you are still going to need to parse out the series."
Machine Learning - Input Prepocessing - NLP email classification model,"So, from what I have gathered, you are asking how to preprocess the new (I suppose unobserved) emails, which do not appear in the training set. In that case, you should convert your email text into 1000-dimensional vector, where each value corresponds to a particular feature value.

I am going to go with the basis that you are simply counting the number of times any of the most frequent 1000 words occur in a new email (let's call it 
x
(1)
test
ùë•
ùë°
ùëí
ùë†
ùë°
(
1
)
).

To convert the email into a vector form, here is one way of doing it:

import pandas as pd
import numpy as np

words = [""blah"", ""tea"", ""tetra"", ""pak""]

def vectorise_email(words, e_mail):

    """"""Vectorise email to a vector of word counts based on a list of words.

    :param words: (List of Strings) List of frequent words in training set
    :param e_mail: (String) E-mail string

    :return word_counts: (Numpy Array) containing counts of words based on words list.
    """"""

    e_mail = pd.DataFrame(e_mail.split())

    e_mail = e_mail[e_mail[0].isin(words)][0].value_counts()
    word_counts = np.zeros(len(words))

    for w_idx, word in enumerate(words):
        word_counts[w_idx] = e_mail.at[word]

    return word_counts

print(vectorise_email(words, ""this is a blah tetra pak tea tea blah blahh""))


Here we firstly tokenise sentences into words (I used the standard string split method, but you could use nltk's tokenise methods [https://www.nltk.org/api/nltk.tokenize.html]). Then we convert this list into a pandas DataFrame to use the value_counts method (Ref: https://stackoverflow.com/questions/22391433/count-the-frequency-that-a-value-occurs-in-a-dataframe-column) to get the word counts for those words which appear in the word list. We then complete the vectorisation process by mapping these counts into a Numpy Array where each element in the array corresponds to a particular word count in the input e-mail.

Hope that helps"
How does BERT work for Aspect-Based sentiment analysis?,"So, from what I have understood there are two ways to perform ABSA:

Aspect category detection + Aspect category sentiment classification

Aspect target extraction + Aspect target sentiment classification

ABSA with BERT involves using pre-trained BERT language model as a feature extractor for a supervised learning task. The model is then fine-tuned on a labeled set. For example, let's consider a set of reviews where each review is annotated with the aspect being discussed and the sentiment associated with the aspect.

During fine tuning, BERT takes the review text as input and produces a sequence of contextualized embeddings for each token in the input. These embeddings are often fed into a classification head, which predicts the aspect and sentiment of the review.

To perform the ABSA, the best model is typically fine-tuned using a multi-task learning approach, where the model is trained to perform multiple related tasks simultaneously such as aspect extraction and sentiment classification. This approach allows the model to learn shared representations that can capture both aspects and sentiment information in the input text.

In conclusion, ABSA with BERT involves fine tuning the pre-trained model on a labelled dataset to learn to identify aspects and sentiments in reviews. The contextualized embeddings generated by BERT are used to capture the meaning of the review text, while the classification head predicts the aspect and sentiment associated with each aspect."
How to do give input to CNN when doing a text processing?,"So, here the question is asking how you use text in a CNN architecture.

Just like when we process images with a CNN, the text data will also be a 2D matrix, where the rows will represent the text features and the columns the sequence of characters (which make up a name) for example.

Now, of course, if we simply use one-hot encoding, we get data sparsity, which is not not particularly computational efficient, especially when using CNNs. So, preferably, we would use word embeddings, which you have described in your post. Here, word embeddings aim to collapse the high dimensionality of the input so that reduces the chance of data sparsity.

However, there is a potential problem with using word embeddings in your problem. The principal behind word embeddings is that it will assign similar word embeddings to words which can be easily replaced in similar contexts (e.g. Sam is kind & Simon is kind) and that don't affect the coherence of a sentence. As you can see, when it comes to proper nouns, we might end up with similar vectors for all names since they are very interchangeable in any context and do not affect the overall coherence of the sentence.

Therefore, although this is contradictory, I would advise using one-hot encoded sequence of characters as a start point for this problem and then when you implement this, you might come up with a more effective way to represent the names."
How to use fine tuning of BERT when i have unlabelled dataset of text documents?,"So, how should I refine the word/sentence embeddings vector given by the BERT model in the case when I have a set completely unlabelled set of documents?

What are you looking to achieve with these unlabelled documents? If you are looking to classify them, then there is no way of getting around getting labels and fine-tuning on them.

I'm aware that the BERT model is originally trained on unlabelled data, so there must be some way.

If you don't need to perform any specific end task (like classification) with the data, and instead is just looking to train BERT using your own data, then there is a way. BERT is self-supervised, meaning that it takes unlabelled data and automatically generates labels that it can train on. The same methods can be used to train on any large language corpus. However, this will only train the embeddings and fine-tuning on labels will still always be needed for classification.

I think huggingfaces' blog post is a good starting point if you want train a general BERT on your own dataset.

You can read about how BERT is trained on generated labels from text here."
Sentiment Analysis models trained on articles / alternative data,"So, I think the interpretation of the model is part of the problem.

For the first: ""How Your Family Can Volunteer During the Pandemic"" them model is not 99% sad. It is 99% confident based on your training, that the context of this sentence is sad.

The same holds true for the second sentence: ""There was a massacre in Bosnia where many were slaughtered"" the model is 96% confident that sad is the sentiment of this observation.

Those probabilities are not intensities of the sentiment. So, if you are finding that globally your model is performing poorly on many or most test cases, it suggests one of a few things:

your model is not robust enough or trained on text of different length with different contextually complexities or there is too little training data

your labels are not appropriate to the sentiment you expect. Remember what is considered sad is determined by your input labels.

that massacre and slaughter might not be in the training set, or fully represented or used together such that when combined you get a high probability of sadness

In my experience this happens often when people cherry pick sentences and say, ""look how awful this NLP model was on this: X."" It happens frequently at my job. But remember, a neural network is designed to emulate a human neural network with appropriate training. Human beings are constantly taking in input and augmenting our training. Subtlety and consistency of our assessment of sentiment is the result of billions of exposures. Your model only has what you have trained it on.

Expecting it to always be right is not reasonable. Also it is not the case that the probability of a sentiment will scale relative to our sense of intensity. The goal of the model is to pick a class. That is done by identifying a sentiment and giving you a probability.

As the creator of the model, you decide where the cutoff value is.

As for the model itself, you are showing what look like titles.

Are you training on article titles or the article itself?

Titles and articles are not the same thing linguistically. Using a model that was designed for tweets to assess long-form copy also might not provide the best generalization capabilities depending on your methods.

In my personal experiences, the labeling itself and the pre-work (wrangling, grooming and such) which you do on language models (including the relative similarity in size and shape of training copy) has a lot more to do with results than small changes in the model structure.

I would start by looking at the global accuracy and then look at all the cases where if fails and see if there is something missing in the training data that you can augment to correct the erroneous predictions."
how to use word embedding to do document classification etc?,"So, one document is a matrix. If I want to use some traditional method like random forest to classify documents, how to use such data?

You can't, at least not directly because traditional methods require a fixed number of features for every instance. In the case of document classification the instance must represent the document, so unless all the documents have exactly the same length (unrealistic) it's impossible to use a set of vectors as features.

The traditional approach would consist in representing a document with a vector where each cell represents a word in the vocabulary, and the value is for instance the TFIDF weight of the word in the document."
approach for multi label text classification,"So, you are asking about how to develop this system / model, which can classify text. Yes, it is a great idea to instantiate a ""baseline"" or dummy model, which can be rule-based or randomly assigns a label to a certain piece of text. From this dummy model, yes you can then use RNN/LSTMs that does multiple-inputs (e.g. words in text) to single output probability over classes as a more sophisticated model and yes you would then compare the validation and test accuracy, F1-score, etc. to see if that improvement to the model is warranted by the change in the model's functionality to classify the texts."
NLP Text Summarization - which metrics to use in evaluation?,"So, your question is talking about whether human reference summaries are required to evaluate summarisation models.

The short answer is yes at the moment. The most important things about an output summary that we need to assess are the following:

The fluency of the output text itself (related to the language model aspect of a summarisation model)
The coherence of the summary and how it reflects the longer input text.

The problem with have an automatic evaluation system for a text summarisation model is that, although we can assess fluency from a language model, we can't really assess whether the model has pulled ""the most salient"" pieces of information from the original, longer text (and this can subjective from person to person). Hence why we need multiple human reference summaries to compute ROUGE and BLEU. However, as you are aware, these metrics have their limitations.

ROUGE is essentially a further development of BLEU, which have been commonly used a dubious proxy for output text fluency in research to compare summarisation and translation models. These metrics are dubious because they simply look at how much they overlap with reference texts from humans (https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xt1ewy-ZOi4)."
Detect passive voice in headlines,"So, your task is to detect the passive voice from sentences. Currently, you have defined some rules to detecting the passive voice and you have noticed that there some exceptions to your defined rules.

Therefore, it would be a good idea to develop a model to predict the probability of a sentence being passive (or active).

You can do this by encoding the sentence as a sequence of words (converted into word embeddings) using a Recurrent Neural Network (RNN) or LSTM. This encode will encode the words into a ""hidden representation"". This hidden representation can then be decoded using a neural network with a final softmax output layer which then outputs the probability that the sentence is written in the passive voice and the active voice respectively.

As this would be a supervised learning problem, you would need to label examples of passive- and active-voice sentences."
Preprocessing advice for large text corpus in natural language generation (NLG),"Some comments:

With Transformers and subword vocabularies (e.g. byte-pair encoding (BPE)), usually there is no need to remove named entities because the model learns to handle them just fine. For instance, in machine translation models learn to copy them verbatim or to translate them without much problem. My advice would be not to overcomplicate things unless proven necessary.
Again, with Transformers and BPE usually there is no need for much preprocessing. If any, I would ensure there is no garbage in your data. What has worked for me in the past is to sort the sentences and eyeball the first and last sentences, where you can usually find the garbage, and remove them manually."
Generate paragraphs from given words,"Some ideas are:

a) Generate many many random sentences until you find your words (will be extremely slow, but the samples will be unbiased).

b) For very short sentences you can iterate over all positions and all possibilities to fill unknown positions like (today, neural networks,?), (?, today, neural networks), (neural networks, ?, today), etc. and use the language model to compute the probability of each sentence, then select the one with highest probability or one at random with weights equal to probabilities. Actually if you can generate all possible positions then you should be able to use a pretrained BERT model directly since it is trained on a similar Masked Language Model task.

c) Train a new language model from scratch, or fine tune a pretrained model (like BERT), with inputs (or sentence pairs) like

today, freenode, neural networks # Today I joined the freenode channel for neural networks.


You can sample the words or n-grams from existing sentences. This might require a large corpus, some work and some cost.

d) If you have access, try existing large models like gpt-3. For example, I tried transformer.huggingface.co, with the last sentence autocompleted by the transformer:

Please make a sentence containing the given words.

Words: horse, tree, walked.

Sentence: The horse walked around the tree.

Words: freenode, today, neural network.

Sentence: The freenode sat on the tree.

Update

Same experiment on https://6b.eleuther.ai/

Sentence: The freenode today uses a neural network to classify images."
Named entity disambiguation contests,"Some of the GREC shared task challenges included a named entity recognition & coreference resolution component (i.e., disambiguation), but I don't think they've run GREC since 2010...?

https://sites.google.com/site/genchalrepository/reg-in-context/grec-ner"
What are the elements in a BERT word embedding?,"Some points first:

BERT is a word embedding: BERT is both word and sentence embedding. It needs to be taken into account that BERT is taking the sequence of words in a sentence into account which gives you a richer embedding of words in a context but in classic embeddings (yes, after BERT we can call others ""classic""!) you mostly deal with neighborhood i.e. the semantic of the word vector is kind of the average of all semantics it had in the training set.
it did not take homonyms into account: Either a typo from you or I did not understand homonym very well. To be honest had to search it! Google says ""two or more words having the same spelling or pronunciation but different meanings like ""right"" and ""write"". That is not a problem for word embedding. Maybe you meant something else?!
BERT tackles this problem by taking context into consideration: All embeddings take context into consideration. The difference is in capturing context when the sequence of the words are taken into account e.g. modeling sentence.

About questions:

Is there still one vector for each word token?: It's one vector per token per layer. Means that for finding a single vector for a word you take n layers and sum the values up. As you know, more you go towards later layers (i.e. toward the output layer) information (features) encoded in layers gets richer. One could also concatenate all after each other and get a higher dimensional representation. Be aware that it is completely task dependent. BERT authors tried different things and came up with summing up last four layers for NER task.

The interesting thing in comparison with classic embeddings is that you can now encode the sentence-dependent semantic of a token! Means that in word2vec (trained on general corpus text like Wikipedia) you have 1 vector for the word apple and if you inspect it you probably see that it is in a relation with both iphone and fruit (never tried it. Just made up an example to make my point for you. Let me know if you tried and something else came out!) but with BERT, you can encode the sentences containing similar words in different context and check the encoding of those words in the sentence. You surprisingly see how it captures the semantics!

The last but not the least is this blog post which is the base of my answer.

Hope it helped!"
Twitter POS and NER: What is state-of-the-art?,"SOTA is changing so rapidly in NLP that even Data Science professionists struggle to cope with it. I have two main sources that I constantly check to gain some insights on SOTA:

NLP Progress from Sebastian Ruder. It contains updates on NLP on a whole lot of subfields, NER and POST included.

Paper with code contains a section on NLP. That's a great website for ML in general.

I know these links do not tackle the problem of Twitter specifically, however I don't think that domain is qualitatively different from others. IMO, of course.

About your other question:

Are industrial-strength programs like Spacy and SparkNLP accurate for such texts? How about FlairNLP and Stanford's CoreNLP accuracy measures?

As I wrote above, it's mostly a matter of personal preference and/or contingent project needs. There's no right or wrong tool. Personally, I found Stanford tools to be the best, for either the quality of their predictions and the amount of models available from a single pipeline. But as I said it's very subjective."
How to extract important phrases (which may contain company name) from resume?,"Sounds like you want named entity recognition. There are a variety of approaches to NER, and plenty of implementations, like the Stanford NER package.

After you find named entities, determining what the named entity refers to is called concept normalization."
NLP: What are some popular packages for phrase tokenization?,"Spacy can do this. Spacy's semantic parser is based on Language models trained on large corpus of text.

This parser can break sentence into lower level components such as words / phrases.

More details and examples :

https://spacy.io/usage/linguistic-features

Example with the first sentence from questions: https://explosion.ai/demos/displacy?text=I%20think%20you%27re%20cute%20and%20I%20want%20to%20know%20more%20about%20you&model=en_core_web_sm&cpu=0&cph=0"
SpaCy vs AllenNLP?,"spaCy used to recommended (archive link) that you use spaCy when you want production-grade performance but don't need to customize your architecture. They recommended that you use allenNLP when you want to explore different architectures or use the state-of-the-art models. They recommended against using allenNLP for production, though.

Since spaCy 3.0, they now recommend (live link) that you can also use spaCy for customized and state-of-the-art models. They still don't recommend spaCy for language generation models or for active research."
SpaCy string store,"Spacy uses a hash function that assigns an integer to any Unicode string, it is not an index in vocabulary it just a random integer that is used internally for better efficiency. It is a hash function, so it means conflicts are indeed possible, but very unlikely and too rare to have a negative influence on the accuracy of the library, so the efficiency gains outweigh it."
Converting paragraphs into sentences,"Spacy's Sentencizer is very simple. However, Spacy 3.0 includes Sentencerecognizer which basically is a trainable sentence tagger and should behave better. Here is the issue with the details of its inception. You can train it if you have segmented sentence data.

Another option is using NLTK's sent_tokenize, which should give better results than Spacy's Sentencizer. I have tested it with your example and it works well.

from nltk.tokenize import sent_tokenize
sent_tokenize(""A total...."")


Finally, if for some abbreviations sent_tokenize does not work well and you have a list of abbreviations to be supported (like ""spp."" in your examples), you could use NLTK's PunktSentenceTokenizer:

from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters
punkt_param = PunktParameters()
abbreviation = ['spp.']
punkt_param.abbrev_types = set(abbreviation)
tokenizer = PunktSentenceTokenizer(punkt_param)
tokenizer.tokenize(""A total ...."")"
What GPU size do I need to fine tune BERT base cased?,"Speaking about vanilla BERT.

It is currently not possible to fine-tune BERT-Large using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small (even with batch size = 1).

The fine-tuning examples which use BERT-Base should be able to run on a GPU that has at least 12GB of RAM using the hyperparameters given on this page.

However, GPU training is single-GPU only."
how to work with NLP with other features,"Sport_Type and City features are categorical features so they need to be encoded into a numeric format (e.g., one-hot encoding or feature hashing). Those numerical features can be added to any machine learning model, including Long Short Term Memory (LSTM)."
NER and context mapping,"Standard NER is going to extract individual entities, which in this case is time (3 months, 1 month) and currency ($3000, etc). You'll also want to think about Relationship Extraction, which identifies how two pieces of text relate to one another. For example, from a Euclidean distance measure, ""contract"" is related to ""valid"" and ""executed"", and ""valid"" is related to ""3 months"" while ""executed"" is related to ""1 month"". Based on what you said your desired output should be, you'll want to train your model to calculate the shortest distance between ""contract"" and ""3 months"", which in this case means teaching it to look for ""valid"" while ignoring ""executed"". There are different ways of doing this which you'll want to think about in terms of what works best for your corpus of text.

Here's a link to get you started (also includes links to other resources)."
What are CRF (Conditional Random Field),"Stanford CoreNLP is a very good implementation of CRF (In Natural Language Processing domain).

https://nlp.stanford.edu/software/CRF-NER.html . CRF specific implementation is : https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/crf/CRFClassifier.java

Few other resources :

https://www.analyticsvidhya.com/blog/2018/08/nlp-guide-conditional-random-fields-text-classification/

https://github.com/lancifollia/crf

https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463"
Machine Learning and Natural Language Processing : Project Initiation,"Starting with a small chunk of documents (~50) as a training set with a set of rules implements would be a good starting point. Train using a few algorithms and see if your training accuracy is coming out as acceptable or better than the human accuracy.

Then slowly increase your training batch size and see the impact on the results.

Once you believe that the training set is a partially good representation of the dataset, go ahead do a test run. Then keep growing from there."
What is the state of the art method for synonym detection?,"State of the art in synonym detection tend to be ensembles of embeddings and knowledge graphs. Embeddings model tokens as dense, semantic vector representations. Knowledge graphs model the relationships between entities and are useful for adding constraints."
"General approach to work with text of phone calls (topics, promises, sentiments, etc.) [closed]","Step by step.

Since you say its seperated into C-customer and A-agent I am going to assume its labeled (some of the questions could be solved unsupervised also)

Who is the customer and who is the agent? Simple binary classification. Find suitable numerical model representation of text and perform classification. Starting reference

Customer Name Named entity recognition - NER example how to use it:

import nltk

from nltk.tag.stanford import NERTagger

st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')
text = ""blablabla""

for sent in nltk.sent_tokenize(text):
    tokens = nltk.tokenize.word_tokenize(sent)
    tags = st.tag(tokens)
    for tag in tags:
        if tag[1]=='PERSON': print tag`


The topic of conversation Start with these two approaches: LDA and LSA

Promises made by the operator to the customer (for example, ""I call back tomorrow"") General approach (its plausible, needs a bit of work by you) But lets say you use word embeddings, all of the sentences that have promises will be vectors that are close to each other in this high-dimensional space. You can limit your system to return all sentences that fall into this cluster in the space.

Negative Sentiment (if there is something in the conversation that the subscriber is not happy with) Simply just look at this introduction to Emotion AI Nutshell: You already have pre-trained models that can model positive-neutral-negative sentiment in the given text. Use the recources."
"Stopwords for programming languages (for, while, print,...)","Stop words are common words in a language and they are usually removed when there appearance is not indicative to the analysis goal.

Suppose our goal is to text mine and find out if a given text is about sports or politics. If we use a bag of words, ""not"" is probably a stop word we should remove since it is probably not indicative of either categories.

On the other hand, if we would like to differ between ""This article will not discuss politics"" and ""this article will discuss politics"", we must not remove ""not"".

The examples you gave look like programming languages reserved words. In most languages you can find this documentation and skip the learning phase. Looking at the frequent tokens will probably also give you the reserved words (and probably plenty of English stop words that will appear in the remarks).

However, before utilising such a list, be sure that neglecting them will serve your goal. In case that typing them doesn't differ from typing other words, you might better leave them. This way you will have more data the represent better the user behaviour."
"NLP - why is ""not"" a stop word?","Stop words are usually thought of as ""the most common words in a language"". However, other definitions based on different tasks are possible.

It clearly makes sense to consider 'not' as a stop word if your task is based on word frequencies (e.g. tf‚Äìidf analysis for document classification).

If you're concerned with the context (e.g. sentiment analysis) of the text it might make sense to treat negation words differently. Negation changes the so-called valence of a text. This needs to be treated carefully and is usually not trivial. One example would be the Twitter negation corpus. An explanation of the approach is given in this paper."
NLP: what are the advantages of using a subword tokenizer as opposed to the standard word tokenizer?,"Subword tokenization is the norm nowadays in NLP models because:

It mostly avoids the out-of-vocabulary (OOV) word problem. Word vocabularies cannot handle words that are not in the training data. This is a problem for morphologically-rich languages, proper nouns, etc. Subword vocabularies allow representing these words. By having subword tokens (and ensuring the individual characters are part of the subword vocabulary), makes it possible to encode words that were not even in the training data. There's still the problem with characters not present in the training data, but that's tolerable in most of the cases.

It gives manageable vocabulary sizes. Current neural networks need a pre-defined closed discrete token vocabulary. The vocabulary size that a neural network can handle is far smaller than the number of different words (surface forms) in most normal languages, especially morphologically-rich ones (and especially agglutinative ones).

Mitigates data sparsity. In a word-based vocabulary, low-frequency words may appear very few times in the training data. This is especially troublesome for agglutinative languages, where a surface form may be the result of concatenating multiple affixes. Using subword tokenization allows token reusing, and increases the frequency of their appearance.

Neural networks perform very well with them. In all sorts of tasks, they excel: neural machine translation, NER, etc, you name it, the state of the art models are subword-based: BERT, GPT-3, Electra,..."
ChatGPT's Architecture - Decoder Only? Or Encoder-Decoder?,"Summary

ChatGPT is the fine-tuning of GPT-3.5, which is a language model based on a Transformer decoder with some modifications with respect to the original Transformer architecture. Therefore it is a decoder-only model.

Complete information with references

The origin of ChatGPT was GPT (Generative pre-Trained Transformer). The evolution from GPT to ChatGPT was as follows:

GPT (see the OpenAI announcement) was a normal Transformer decoder. From the GPT paper:

In our experiments, we use a multi-layer Transformer decoder [34] for the language model [...]

GPT-2 (see the OpenAI announcement and the source code) is also a Transformer decoder, but with some modifications. It is also bigger and trained on more data. From the GPT-2 paper:

We use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 
1/
N
‚àí
‚àí
‚àö
1
/
ùëÅ
 where 
N
ùëÅ
 is the number of residual layers. The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batch size of 512 is used.

GPT-3 is GPT-2 scaled up and with some modifications. From the GPT-3 paper published at NeurIPS'20:

We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3.

InstructGPT (see the paper) is a fine-tuned version of GPT-3. From the paper

[...]¬†we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning

GPT-3.5 (see OpenAI announcement) (text-davinci-003) is a fine-tuned version of InstructGPT. From the announcement:

code-davinci-002 is a base model, so good for pure code-completion tasks

text-davinci-002 is an InstructGPT model based on code-davinci-002

text-davinci-003 is an improvement on text-davinci-002

ChatGPT (get-3.5-turbo*) is a GPT-3.5 fine-tuned on human instructions by Reinforcement Learning with Human Feedback (RLHF). From the OpenAI website:

gpt-3.5-turbo-0301 is an improvement on text-davinci-003, optimized for chat

From the ChatGPT presentation page:

We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides‚Äîthe user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format.

To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process.

To follow in detail the GPT evolution, I recommend the article How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources."
Usage of Word2Vec,"Sure, you can average the word2vec vectors of all words in the sentence and train a linear classifier with labeled data. Before doing that, you may remove stopwords that do not add meaning.

This approach, of course, disregards word order, so your results may not be good depending on the kind of text you are classifying.

If you google ""word2vec text classifications"" you will find many helpful resources."
What are some methods to reduce a dataframe so I can pass it as one sample to an SVM?,"SVM are not meant to solve ""arbitrarily long"" classification problem, therefore you have few choices:

use PCA for sequences, however it takes very long since it has to build a giant matrix over which it can perform PCA
change model, and pick a better suited one (eg RNN)
pad and cut your data (most often is not needed the whole phrase to predict the output)
introduce some prior knowledge, for example order the most recurrent words, and remove all of those that don't add anything to the meaning (be careful, this might lead to many problems, for example if you remove ""not"")
use recurrent autoencoders, to transform a sentence to a fixed size vector, over which you can perform any ML algorithm (but this might cause some problem from the POV of explainability)

In my opinion, cut&pad is the best option to start with, simple to implement and often very powerful, but this might change from context to context"
Which machine learning problem is this?,"Technically this is sequence labeling, the most common application being Named Entity Recognition.

However it looks like in this case you're trying to solve a problem of coreference resolution, which is a quite difficult task in general. I think this usually involves a more complex model than simple sequence labeling, but I'm not an expert in this. You might want to search around this topic, there are certainly some relevant papers and tools about it."
Interpolation in nlp - definition of O term,"Terminology point: those symbols aren't O, but thetas Œò.

Confusingly, these values labelled theta are normally referred to as lambdas, as in the page you quote. They are weights used in interpolation (as opposed to backoff) that sum to 1, and can be calculated from the corpus itself via a variety of methods:

How are these Œª values set? Both the simple interpolation and conditional interpolation Œªs are learned from a held-out corpus. A held-out corpus is an additional training corpus that we use to set hyperparameters like these Œª values, by choosing the Œª values that maximize the likelihood of the held-out corpus. That is, we fix the N-gram probabilities and then search for the Œª values that when plugged into Eq. 4.24 give us the highest probability of the held-out set. There are various ways to find this optimal set of Œªs. One way is to use the EM algorithm defined in Chapter 7, which is an iterative learning algorithm that converges on locally optimal Œªs (Jelinek and Mercer, 1980).

Speech and Language Processing. Chapter 4: N-Grams (4.4.3 Backoff and Interpolation) (p.15)"
"How to deal with ""Erg√§nzungsstrichen"" and ""Bindestrichen"" in German NLP?","Tested bert-base-german-cased from huggingface today. The results are still different, but much more similar. Also, the tokenizer splits words in a desired way. This might already work for my use case, so marking the question as answered.

token1	token2	cos-similarity
[CLS]	[CLS]	0.982
Haupt	Haupt	0.933
-	##satz	0.824
und	und	0.967
Neben	Neben	0.951
##satz	##satz	0.958
[SEP]	[SEP]	0.977

Code to reproduce:

from transformers import BertTokenizer, BertModel, AutoModel
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(""bert-base-german-cased"")
model = AutoModel.from_pretrained(""bert-base-german-cased"")

def calc_cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def calc_bert_embeddings(text):
  """"""Returns a (N,768) vector representing embeddings for the N tokens.""""""
  tokens = tokenizer(text, return_tensors='pt') 
  output = model(**tokens)
  return tokens.tokens(), output[""last_hidden_state""][0,:,:].detach().numpy()

tokens1, vec1 = calc_bert_embeddings(""Haupt- und Nebensatz"")
tokens2, vec2 = calc_bert_embeddings(""Hauptsatz und Nebensatz"")
for idx in range(len(tokens1)):
  similarity = calc_cosine_similarity(vec1[idx,:], vec2[idx,:])
  print(f""{tokens1[idx]};{tokens2[idx]};{similarity:.3f}"")"
What is the reason behind having low results using the data augmentation technique in NLP?,"Text data is at the same time:

very structured, because swapping only a few words in a sentence can make it complete gibberish,
and very flexible, because there are usually many ways to express the same idea in a sentence.

As a consequence, it's very hard to have a text sample which is representative enough of a ""population"" text, i.e. which covers enough cases for all the possible inputs. But augmentation methods are practically sure to fail, because either they are going to make the text gibberish or just cover minor variations which don't improve the coverage significantly.

That's why a lot of the work in NLP is about experimental design and preprocessing."
Why dont we use 2d cnn filters for Nlp tasks?,"Text is a 1D sequence, but is typically treated as a sequence of embedding vectors. So yes it is in some sense 2D input. But the embedding dimension doesn't really have any spatial meaning; adjacent dimensions aren't any more related than any others. There is no invariance across the embedding dimension either; the same values in one part of the embedding don't mean the same thing. So the assumptions a 2D convolution don't make sense for this type of input."
NLP : variations of a text without modifying it's meaning,"Text summarization can be divided into two categories 1. Extractive Summarization and 2. Abstractive Summarization

Extractive Summarization: These methods rely on extracting several parts, such as phrases and sentences, from a piece of text and stack them together to create a summary. Therefore, identifying the right sentences for summarization is of utmost importance in an extractive method.
Abstractive Summarization: Abstractive methods select words based on semantic understanding, even those words did not appear in the source documents. It aims at producing important material in a new way. They interpret and examine the text using advanced natural language techniques to generate a new shorter text that conveys the most critical information from the original text.

What you are looking for is abstractive summarisation. Since you are working in R there is a nice library called lexRank taking an example from here would look something like

#load needed packages
library(xml2)
library(rvest)
library(lexRankr)

#url to scrape
monsanto_url = ""https://www.theguardian.com/environment/2017/sep/28/monsanto-banned-from-european-parliament""
   
#read page html
page = xml2::read_html(monsanto_url)
#extract text from page html using selector
page_text = rvest::html_text(rvest::html_nodes(page, "".js-article__body p""))

#perform lexrank for top 3 sentences
top_3 = lexRankr::lexRank(page_text,
                          #only 1 article; repeat same docid for all of input vector
                          docId = rep(1, length(page_text)),
                          #return 3 sentences to mimick /u/autotldr's output
                          n = 3,
                          continuous = TRUE)

#reorder the top 3 sentences to be in order of appearance in article
order_of_appearance = order(as.integer(gsub(""_"","""",top_3$sentenceId)))
#extract sentences in order of appearance
ordered_top_3 = top_3[order_of_appearance, ""sentence""]

> ordered_top_3
[1] ""Monsanto lobbyists have been banned from entering the European parliament after the multinational refused to attend a parliamentary hearing into allegations of regulatory interference.""
[2] ""Monsanto officials will now be unable to meet MEPs, attend committee meetings or use digital resources on parliament premises in Brussels or Strasbourg.""                                
[3] ""A Monsanto letter to MEPs seen by the Guardian said that the European parliament was not ‚Äúan appropriate forum‚Äù for discussion on the issues involved.""  


EDIT: How I like to think about abstractive summarisation: Y

Using encoder-decoder architecture (extendended with transformers) for seq2seq problems you can essentially get an embeding of your text, where same sentences can be embedded differently in different context, giving same/similiar output."
How to classify objects from a description in natural language,"Text vectorisation is a good way to have a reliable classification.

You have several libraries like doc2vec that you can use together with logistic regression or dimensional reduction technique like tSNE or UMAP. https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html

On the other hand, you can also use libraries like BERT or TF-IDF:

https://pypi.org/project/bert-document-classification/

https://medium.com/swlh/text-classification-using-tf-idf-7404e75565b8"
Methods for finding characteristic words for a group of documents in comparison to another group of documents?,"TF-IDF and Topic Modelling wouldn't be suitable as they do not take classes into account. One approach would be to train a basic classifier and extract important features per class.

The steps:

Create a TF-IDF matrix for the text corpus.
Train a basic classifier using the TF-IDF Matrix as feature matrix and the classes as target. (A decent accuracy is enough.)
Get the feature_importances from the trained classifier.
Sort to get most important features and their corresponding classes.
import numpy as np
from collections import defaultdict
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

# Loading sample data
categories = ['comp.sys.mac.hardware', 'rec.autos', 'sci.space', 'rec.sport.baseball']
newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories=categories)

# 1. Fit corpus to tfidf vectorizer
tfidf = TfidfVectorizer(min_df=15, max_df=0.95, max_features=5_000)
tfidf_matrix = tfidf.fit_transform(newsgroups.data)

# 2. Train classifier
clf = RandomForestClassifier()
clf.fit(tfidf_matrix, newsgroups.target)

# 3. Get feature importances
feature_importances = clf.feature_importances_

# 4. Sort and get important features
word_indices = np.argsort(feature_importances)[::-1] # using argsort we get indices of important features
feature_names = tfidf.get_feature_names() # Lookup to get words from index

top_n = 50 # Top N features to be considered
top_words_per_class = defaultdict(list)
for word_idx in word_indices[:top_n]:
    word = feature_names[word_idx]
    word_class = newsgroups.target_names[clf.predict(tfidf.transform([word]))[0]]
    top_words_per_class[word_class].append(word)


The top_words_per_class would be like:

{
  ""rec.autos"": [""car"", ""cars"", ""engine"", ""ford"", ""like"", ""dealer"", ""oil"", ""toyota""],
  ""sci.space"": [""space"", ""nasa"", ""orbit"", ""launch"", ""earth"", ""moon"", ""shuttle"", ""thanks"", ""program"", ""project"", ""spacecraft""], 
  ""comp.sys.mac.hardware"": [""mac"", ""apple"", ""drive"", ""scsi"", ""centris"", ""video"", ""quadra"", ""monitor"", ""se"", ""card"", ""powerbook"", ""use"", ""problem"", ""simms"", ""software"", ""modem""],
  ""rec.sport.baseball"": [""baseball"", ""game"", ""team"", ""games"", ""season"", ""players"", ""year"", ""league"", ""runs"", ""hit"", ""player"", ""braves"", ""teams"", ""pitching""]}
}"
Using TF-IDF for feature extraction in Sentiment Analysis,"TF-IDF is a vectorization technique used to convert documents (a single tweet in your case is a document) to vectors. After you train the TF-IDF model, the only words/vocabulary it has learnt, would be from the set of documents (aka corpus, the entire set of 3k tweets).

Since you mentioned that there were 570 unique feature words after TF-IDF, that would be the vocabulary your model has learnt. If you give this model a document with words that are present in its vocabulary, it will successfully vectorize it. However, if one or more words in your new document are not present in the model's vocabulary, those words won't be included in the vectorization at all. The words in a new sentence will be given weights only if the model had encountered those words in its training. In other words, every sentence will be vectorized w.r.t the model's vocabulary.

Ex -

Model's vocabulary - a, big, hat, have, I, mat

Input - ""I have a big mat""

Vector - [ sequence of tfidf weights in the order of the model's vocabulary. If word in vocab is not present in the sentence, weight assigned is 0 ]

Input - ""I have a dog""

Vector - [ sequence of tfidf weights calculated for all the words in the vocabulary, in the same order as the vocabulary ]

Since ""dog"" wasn't a part of the vocabulary, it's not included in the vectorization.

What if the new tweets which this model has never seen do not have feature words that I have used for training, will the model fail to make correct predictions for them (in my case there are only 3 possible predictions viz. positive, negative and neutral) correctly for them?

I can't say much about the final prediction, but your feature vectors from TF-IDF can be way off, if you're expecting it to vectorize documents with a large number of words that the model hasn't trained on. Improper vectorization can affect your prediction accuracy.

If yes, then how should I handle this scenario

My suggestion would be to consider the following -

Pre-process the train data thoroughly. Removing punctuation, substituting abbreviations by their full forms and other steps can help the TF-IDF model train well.
If gathering more data is an option, try training the model on a large set of tweets that fall in the context of your prediction. The more the vocabulary, the better equipped the model is, to vectorize new unseen documents.
Try using a pre-trained model like BERT."
What happens when the vocab size of an embedded layer is larger than the text corpus used in training?,"tf.keras.layers.Embedding(..., embeddings_initializer=""uniform""*,..., *kwargs)

All the weights are initialized with the init strategy
All learn the optimum values with the backprop
Weights for which there is no input will have zero output every time, hence no learning.
Hence these extra weights will remain at their initialization value


You may check these extra weights before and after.

weight = model.layers[0].get_weights() # Save before training
history = model.fit(x, y)

# These two should be same
weight[0][-1]  # Last weight - Before
model.layers[0].get_weights()[0][-1] # Last weight - After"
What affect will replacing words with bigrams have on TfIDF?,"TFIDF decreases as term frequency will be decreased linearly and idf increases log linearly.

Document similarity will decrease as value of tfidf vectors should decrease as reputation of bigrams are more less than each single word."
Is there a way to map words to their synonyms in tfidf?,"TfIdf vectors require much more data than that to be useful, but also don't give you the ability to identify synonyms. To do that with vectors and the amount of data you're working with, you'll need a pre-trained vector vocabulary. GloVe vectors are a popular choice to start with, but there will be others you can find and play with that may work better for your explicit purpose.

Note that if you don't limit yourself to vector-based approaches, there are many classical approaches to this problem. WordNet would probably be the first thing I reach for here."
Why is the result of CountVectorizer * TfidfVectorizer.idf_ different from TfidfVectorizer.fit_transform()?,"TfidfVectorizer will by default normalize each row. From the documentation we can see that:

norm : ‚Äòl1‚Äô, ‚Äòl2‚Äô or None, optional (default=‚Äôl2‚Äô)
Each output row will have unit norm, either: * ‚Äòl2‚Äô: Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. * ‚Äòl1‚Äô: Sum of absolute values of vector elements is 1. See preprocessing.normalize

Setting norm to None will give the result you expect:

tfidf_docs = TfidfVectorizer(vocabulary=vocab_docs, norm=None)"
How to initialize a new word2vec model with pre-trained model weights?,"Thank Abhishek. I've figure it out! Here are my experiments.

1). we plot a easy example:

from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot
# define training data
sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
            ['this', 'is', 'the', 'second', 'sentence'],
            ['yet', 'another', 'sentence'],
            ['one', 'more', 'sentence'],
            ['and', 'the', 'final', 'sentence']]
# train model
model_1 = Word2Vec(sentences, size=300, min_count=1)

# fit a 2d PCA model to the vectors
X = model_1[model_1.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model_1.wv.vocab)
for i, word in enumerate(words):
    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()


From the above plots, we can see that easy sentences cannot distinguish different words' meaning by distances.

2). Load pre-trained word embedding:

from gensim.models import KeyedVectors

model_2 = Word2Vec(size=300, min_count=1)
model_2.build_vocab(sentences)
total_examples = model_2.corpus_count
model = KeyedVectors.load_word2vec_format(""glove.6B.300d.txt"", binary=False)
model_2.build_vocab([list(model.vocab.keys())], update=True)
model_2.intersect_word2vec_format(""glove.6B.300d.txt"", binary=False, lockf=1.0)
model_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)

# fit a 2d PCA model to the vectors
X = model_2[model_1.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model_1.wv.vocab)
for i, word in enumerate(words):
    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()


From the above figure, we can see that word embeddings are more meaningful.
Hope this answer will be helpful."
imbalanced dataset in text classififaction,"Thank you for your message Ahmed. There are things to point out:

Is this an imbalanced problem? Which problem? THIS is not a problem. This is data.
What analysis is going to be done? In some cases you need posts and in some you need these keywords.
What method is going to be done for that analysis? Some methods get keywords as input and some get posts.

But about the numbers themselves; Not necessarily. The smallest class has 20% of the largest population and moreover, the scale is pretty high (20000 samples). So it is not necessarily an imbalanced class distribution. Again, see what you want to do with this data. That determines the answer much more accurate.

Hope it helped. If you write about the task you want to do I can post the solution here.

Cheers,

UPDATE

Well, then the problem is pretty straight-forward. These unique words are probably not much meaningful here. I certainly recommend that you try BoW models first (TF-IDF and classic BoW) for modeling your corpus. Then tune the hyperparameters of models and using a simple Multinomial Naive Bayes you will get an acceptable result.

Data is not counted that imbalanced. I had a problem in which some classes had 3000-4000 samples and some only 20! That is certainly called imbalanced but here you still have enough data to represent your minority class and also you will use Precision-Recall for evaluation instead of Accuracy so you will be fine. I strongly recommend you to have a look at this for Python implementation and also seeing some imbalanced data in practice.

The DL thing is answered in the comment."
How to define person's gender from the fullname?,"That dataset looks like a good starting point. Keep in mind that when you make your own dataset from those datasets you'll want to keep the male to female ratio balanced if you want it to predict both well.

It should not matter what machine learning software you use (Apache Mahout, scikit-learn, weka, etc.). Pick one that fits your language of choice since speed will probably not be too much of a concern with the smallish dataset size. As for features, you'd generally use ngrams as your baseline for NLP classification tasks. If you use ngrams here you won't end up with anything very interesting because the model won't generalize to any unseen names. I'd suggest as a feature baseline that you try character ngrams, and maybe something like syllable ngrams for something slightly more advanced (for syllable tokenization see https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word)."
Which text clustering algorithms should I use to group free text fields in Python?,"That is called ""entity resolution"" or ""record linkage"". It is a very hard problem.

If it all possible, I would try to add ""type ahead"" to the form. That would encourage users to select already used categories.

fuzzywuzzy is a Python package for approximate string matching.

dedupe is ""a Python library that uses machine learning to perform fuzzy matching, deduplication, and entity resolution quickly on structured data."""
Mapping a set of corrupted strings to the correct ones,"That problem is called approximate string matching or fuzzy string searching. It has been well studied in computer science.

If there is a limited, known collection of valid strings (aka, a dictionary), then the problem can be framed as spell correction."
Predicting word from a set of words,"That problem is commonly called multi-label classification, learn to predict zero or more text labels for each instance.

A multilayer perceptron (MLP) architecture could work."
Check If Answer is Correct by Similarity,"That problem is commonly called question answering. It sounds like the particular framing is answer selection which can be framed as learning to rank.

A recent, revelvant paper is Learning to Rank Question-Answer Pairs using Hierarchical Recurrent Encoder with Latent Topic Clustering by Yoon et al."
Find multiple categories of promises in texts,"That would be a sequence labeling task, the most common type is Named Entity Recognition, you'll find many examples about it but you can train a custom system with your data. The traditional method is Conditional Random Fields, there are a good few libraries available.

Side note: usually a single CRF model is used to do both detecting and labeling at once (your steps 1 and 2)."
Consider ratings as sentiment labels?,"That would depend on the exact goal of the task and the specifics of the dataset, but in general I would say that it's always better to use the information specifically provided with the data if it's relevant for the task. In this case the rating for the product is indeed very likely to reflect the sentiment of the text, so I would go with it. Notice that you could also do both and compare the cases where the predicted sentiment differs from the one derived from the rating.

Given that the ratings are provided as 1-5 scores I would also consider the option of treating the task as a regression problem, instead of the standard binary classification setting."
Is it possible to generate syllogisms using an NLP algorithm?,"That would probably be related to textual entailment and also relation extraction.

I'm not aware of any specific work but I would check in the biomedical domain, because there are resources such as SemRep and I wouldn't be surprised if people tried to use it for similar purposes."
Can CBOW model only accept fixed number of words?,"That's a good question. The answer is yes and no.

No because the input layer of the CBOW model expects a fixed number of words. So you'll either always input 3 words or 9 words.

Yes because you can however set the sequence length as 9 words and provide just 3 words as context while the remaining 6 words can just be zero vectors. Remember in a CBOW each word is represented by a vector."
Antonym search for expanding search terms,"That's an interesting problem.

This according to me is the most comprehensive way(if speed is not a problem. Or you can just pull all these words and create a dictionary/database of your own).

You can this , https://wordsapiv1.p.mashape.com/words/love/antonyms (More info about this API at this link)

However, you can restrict results to antonyms with this api.

You can use requests to make API calls.

import requests
import simplejson
response = requests.get(url)
result = simplejson.loads(response)


Then search for the antonyms from the result. If you are getting a big list of antonyms, use only the top n results for your searching.

Although, W2V gives most commonly used words in the context of the keyword, it's hard to guess which is the antonym of the keyword."
Future of deep learning (compared to traditional machine learning) [closed],"That's very interesting topic. So thumbs up from my side. Now coming to the point. Deep learning may be hot now but some variants of it or something new all together may emerge later. Let me point out the reasons I feel deep learning maybe getting old or will get.

Slow Learner

Slow learner as in, it converges to an optimal solution slowly but with GPU acceleration the training speed can be improved dramatically. The slowness is affected by the learning rate.

Adjusting the learning rate affects the reliability of the resulting deep neural net.

Huge Training Data Requirement

Deep learning requires very huge training data to achieve good performance. The presence of a huge number of parameters to adjust requires some huge example sets. The fact that deep learning requires such a large number of training examples makes it dull in some way, despite such huge training set requirement deep neural nets do have error rates of about 10%.

Overfitting

Deep learning tends to overfit easily but with the new dropout algorithm, this problem can be avoided but with some consequences such as an increase in error rates.

Poor Initial State

Deep neural nets have parameters that need to be initialized. The most used method is random initialization, this results in neural nets with very poor initial states. Compare this to a mammalian brain, the brain is born with some rigid instincts such as basic survival behavioral patterns.

Sensory Data Transformations

For example in visual object recognition tasks, images undergo various geometric and photometric transformations that need to be modeled by a recognition system to rectify new image observations. Deep learning as used today does not take such transformations into account, this is one of the reasons why deep neural nets still suffer from relatively high error rates (relative to a human).

Some other reason can be the slow transformation, algorithms mayb emerge together in future, strcture and the approaches. These are some pin points from my side. You can also go through these posts, they're also on it and to the point-

KDnuggets
Huffington Post

Cheers! :)"
How is an ASR's output compared to ground truth for validation?,"The answer I was looking for is Word Error Rate. It is the most standard way of comparing ASR transcriptions wrt ground truth. It is less granular than what I had in mind, it is basically Levenshtein Distance on a word level rather than a character level.

jiwer in python has a few other metrics and is easy to use."
Does Word2Vec's skip-gram NNLM even produce context words?,"The answer is 3, but keep in mind that the original algorithm iterates over all available context words for each center word, thus all possible pairs are included for each center word (some frequent center words that may be stop words can be discarded depending on parameters). Also the original work uses a faster alternative algorithm to replace the standard softmax.

Depending on the negative sampling parameter, the training set can also have more pairs, where the second word is chosen at random.

See section 2 of another paper of Mikolov et al https://arxiv.org/pdf/1310.4546.pdf and the original source code https://code.google.com/archive/p/word2vec/.

Edit:

OK, I've looked at the paper and the code I suggested above. I think they contain all the information to correctly answer your question.

My (wrong) answer above was based on a simplification of the method. Actually softmax is only discussed as a simpler and slower alternative to hierarchical softmax. Pure softmax over all dictionary words does not seem to be used in the original word2vec implementation.

I think the correct answer is that Skip-gram learns to maximize the log-likelihood of the ""Hierarchical Softmax"" probability given in formula (3) of the paper (which I gave a link to) over the pairs of nearby words.

For a single data point the ""Hierarchical Softmax"" loss can be expressed as a sum of terms over nodes on the tree path leading from the root to the center word. Each of those terms looks like ordinary log loss (binary cross entropy), as in your item 4., but because the summing over the path in a tree, overall it is certainly more complicated than your 4.

If negative sampling is turned on, the method just adds (4) from the paper to the loss. This would be almost exactly like your item 4."
What is a better input for Word2Vec?,"The answer to this question is that it depends. The primary approach is to pass in the tokenized sentences (so SentenceCorpus in your example), but depending on what your goal is and what the corpus is you're looking at, you might want to instead use the entire article to learn the embeddings. This is something you might not know ahead of time -- so you'll have to think about how you want to evaluate the quality of the embeddings, and do some experiments to see which 'kind' of embeddings are more useful for your task(s)."
"In sequence models, is it possible to have training batches with different timesteps each to reduce the required padding per input sequence?","The answer to your needs is called ""bucketing"". It consists of creating batches of sequences with similar length, to minimize the needed padding.

In tensorflow, you can do it with tf.data.experimental.bucket_by_sequence_length. Take into account that previously it was in a different python package (tf.contrib.data.bucket_by_sequence_length), so the examples online may containt the outdated name.

To see some usage examples, you can check this jupyter notebook, or other answers in stackoverflow, or this tutorial."
Vectorizing Skipgrams in sklearn,"The answer to your question can be found at: https://stackoverflow.com/a/45997893/5312422

To vectorize text with skip-grams in scikit-learn simply passing the skip gram tokens as the vocabulary to CountVectorizer will not work. You need to modify the way tokens are processed which can be done with a custom analyzer. Below is an example vectorizer that produces 1-skip-2-grams,

from toolz import itertoolz, compose from toolz.curried import map as cmap, sliding_window, pluck from sklearn.feature_extraction.text import CountVectorizer

class SkipGramVectorizer(CountVectorizer):
    def build_analyzer(self):    
        preprocess = self.build_preprocessor()
        stop_words = self.get_stop_words()
        tokenize = self.build_tokenizer()
        return lambda doc: self._word_skip_grams(
                compose(tokenize, preprocess, self.decode)(doc),
                stop_words)

    def _word_skip_grams(self, tokens, stop_words=None):
        # handle stop words
        if stop_words is not None:
            tokens = [w for w in tokens if w not in stop_words]

        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)


For instance, on this Wikipedia example,

text = ['the rain in Spain falls mainly on the plain']

vect = SkipGramVectorizer()
vect.fit(text)
vect.get_feature_names()


this would vectorizer would yield the following tokens,

['falls on',  'in falls',  'mainly the',  'on plain',  'rain spain', 
'spain mainly',  'the in']"
Is it possible to target a specific output length range with BART seq2seq?,"The answer was to go lower level into the actual transformer config, then force the model to create sequences of 64-128 tokens. Doing this before training forces the model to adapt to this constraint, and obviously, these hard bounds ultimately lead to outputs in the specified range only."
How to use regularizer in AllenNLP?,"The API is a bit confusing. You pass a RegularizerApplicator instance to the model, which takes a list of tuples of the form (regex, regularizer). The regex matches against your model's parameter's name. For example, if you had layer called linear_relu_stack.0.bias, linear_relu_stack.0.weight, you could apply a single regularizer to both with the regex ""^linear_relu_stack.0.(bias|weight)$"".

The regularizer itself is an just instance of L1Regularizer or L2Regularizer, where you can specify the alpha.

In all, your model would look something like this after adding regularization:

from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator
from allennlp.nn.regularizers.regularizers import L2Regularizer

AcademicPaperClassifier(*model_args, regularizer=RegularizerApplicator([
    ("".*LSTM.*"", L2Regularizer(alpha=0.01)),
    ("".*FFN.*"",  L2Regularizer(alpha=0.01)),
]))


The API documentation is the best resource for this: http://docs.allennlp.org/v0.9.0/api/allennlp.nn.regularizers.html#allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator.from_params"
How to calculate Pointwise Mutual Information (PMI) when working with multiple ngrams,"The application of PMI to text is not so straightforward, there can be different methods.

PMI is originally defined for a standard sample space of joint events, i.e. a set of instances which are either A and B, A and not B, not A and B or not A and not B. In this setting 
N
ùëÅ
 is the size of the space, of course.

So the question when dealing with text is: what is the sample space?

Sometimes it makes sense to consider specific units of text as instances, for example small documents (e.g. tweets) or sentences. In this option the different cases are whether word A and B appear at least once individually/jointly in the document, and then we count the number of documents as frequency. 
N
ùëÅ
 is the total number of documents, of course.
Sometimes there's no natural unit to consider, only the full text. In this case the sample space is defined by a moving windows of length 
m
ùëö
 in the text, i.e. the window starting at position 1, 2, 3, etc. Every window is a 'document' which can have combination of [not] A/B."
"How do you compare term counts between two different periods, with different underlying corpus sizes, without bias?","The approach suggested by j.a.gartner is good if you want to analyze the quarter-to-quarter change in frequency for a given term, considering only that term in isolation. What it doesn't do is evaluate the relative frequency compared to all other terms in the corpus. For this you could compare frequency rank for all terms quarter-to-quarter. This is the analysis used in Zipf's Law. Another advantage of this analysis is that you can test whether the generating process is or is not stationary (e.g. governed by same distribution quarter to quarter).

The advantage of comparing frequency rank is that it doesn't depend on the relative size of the corpus for each quarter (as long as they are all ""large""). In simple language, the frequency rank of the word ""Christmas"" in the 4th quarter will probably be the same, regardless of whether the (US English news) corpus is 10,000 articles or 1,000,000 articles."
Checking if english sentences have impact catpured in them using NLP,"The approach you propose is ok but it can be improved in my opinion:

Find a corpus (or several corpora), chosen to be representative of the kind data you expect to process eventually. Ideally it would be directly a sample of the target data.
Label all the sentences in the corpus or a random subset. This is because you need to preserve the distribution as much as possible, especially the proportion of positive/negative instances. If you start from the set of filtered sentences you'll have two problems:
a proportion of positive cases higher than in regular text, and this is likely to make your model over-predict positive cases;
sentences containing only the selected trigger words, and this will cause your model to predict as negative any sentence which doesn't have a trigger word (that's a problem since your list of trigger words can't be exhaustive).

Ideally you would manually label all your corpus, but that's probably not realistic. That's why you could try to use your idea of trigger words in a slightly different way in order to label all the instances efficiently:

Filter the sentences which contain any trigger word and label these; this is your initial training set
Use a semi-supervised approach to label the rest of the sentences (maybe you could consider active learning)."
When to use GloVe vocabulary vs. building a vocabulary from the training data?,"The arguments that you say are pretty much correct.

The primary reason for using pre-trained embeddings is typically the lack of task-specific training data. In tasks that (at least for some languages) have vast amounts of training data, such as machine translation, the word embeddings are always trained jointly with the rest of the model.

The mere fact that you are using GloVe indicates that your training data is not that large. In that case, it is very likely that at inference time, words can appear that were not in the training data, but GloVe still has a good representation for them and the rest of the model knows how to use it. GloVe has a really large vocabulary, it is much more likely that there will be GloVe words at inference time than unknown training words.

You can also opt for a hybrid solution. If there are words in the training data, that are not in GloVe, but still are frequent enough, you can learn their embeddings jointly with the rest of the model while keeping the rest of the GloVe embeddings frozen. (But implementing this might be a little bit tedious.)"
How to extract titles from documents?,"The automate the extraction of a title is an example of extractive summarization. It is typically solved with a combination of supervised and sem-supervised learning. More recently Deep Learning models with attention have been giving state-of-the-art results.

You'll need labeled to train your own model or used established packages such as symy."
Is applying pre-trained model on a different type of corpus called transfer learning?,"The basic concept of transfer learning is: Storing knowledge gained from solving one problem and applying it to different but related problem

I guess to be precise this is called Transductive Transfer Learning. In this we learn from the already observed training dataset and then predict the labels of the testing dataset. Even though we do not know the labels of the testing datasets, we can make use of the patterns and additional information present in this data during the learning process.

Refer: Ruder"
Identifying templates with parameters in text fragments,"The basic rationale behind the following suggestion is to associate ""eigenvectors"" and ""templates"".

In particular one could use LSA on the whole corpus based on a a bag-of-words. The resulting eigenvectors would serve as surrogate templates; these eigenvectors should not be directly affected by the number of words in each template. Subsequently the scores could be used to cluster the documents together following a standard procedure (eg. 
k
ùëò
-means in conjunction with AIC). As an alternative to LSA one could use NNMF. Let me point out that the LSA (or NNMF) would probably need to be done to the transformed TF-IDF rather than the raw word-counts matrix."
Can we use Stop Words while using multinomial navies theorem?,"The basic task here I get is sentiment analysis of tweets. So for this, we can first extract features and then use a classifier. Stop words are those which are present for grammatical purposes but do not add any value/meaning to the sentence. So, yes you can remove the stop words from the tweet, extract the features and then pass it to the classifier. You can get already existing stop words from

from nltk.corpus import stopwords
stop_words = stopwords.words('english')


Removing the stop words doesn't always improve the accuracy. Take a look at this as it is a very similar problem statement as yours. Hope it helps!"
Is it possible to identify different queries/questions in sentence?,"The basic thing, you can do in that situation, is to split your query into N simple sentences each of which should be processed in order to receive YES/NO answer considering if the sentence is a query. That way you will receive following results:

Input: Gandhi is good guys, Where he was born?
->
Gandhi is good guys - not query
Where he was born?  - query
===
1 query

Input: who is MGandhi and where was he born?
->
who is MGandhi     - query
where was he born? - query
===
2 queries


This approach will require anaphora resolution (in order to convert he into Gandhi in first example) and a parser to correctly divide complex sentence into simple ones."
finding themes from text documents,"The best method to find themes in a collection of documents is topic modeling. Topic modeling finds the hidden (aka, latent) themes beyond just keyword counts.

There are many approaches to topic modeling. Latent Dirichlet allocation (LDA) is a standard topic modeling approach. LDA is a probabilistic graphical model that assumes that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. The number of topics is a selected hyperparameter."
NLP food review classification,"The best way to solve your problem would be to collect the right labels for your task. Currently, you have the input i.e. text of a review and the rating which can be used to predict the sentiment of the review, but not whether it mentions safety issues. To collect the right labels, you should look at each review and assign it a binary label - 0 means food safety issues are not mentioned, 1 means they are mentioned. The feasibility of such data collection depends on the size of the dataset and the importance of your project. How big is your dataset? How much time/money are you willing to commit to that project? Once you collect the labels, you can train and evaluate a neural network based on them. If this answer helps you, feel free to ask for elaboration, I don't know what's your level of data science expertise and don't want to state the obvious."
How to choose and create natural language data for machine learning,"The BIO format (and its variants) is a standard format for training a sequence labeling model, in particular a Named Entity Recognition (NER) model.

Sequence labeling consists in assigning a label to every token in the sequence, so at the ""low level"" stages of training and predicting the system must deal with the token and its label, as well as (possibly) other features associated with the token. There are several possible choices to represent an entity through the labels: there must be at least two obviously, and it has been proved that adding at least a special B for the first token in the entity is beneficial.

A json-like format like the one you present can be used as a simplified output of a NER system, typically for applications which only need a list of recognized entities with their type. It's usually more convenient to manipulate but it cannot be used directly by the NER system: it doesn't even contain the full text, it's not tokenized and it doesn't have a label for each token. But assuming that the full text is also provided, this format could also be converted to BIO or some variant but it's more work.

If the goal is to provide a dataset which can be fed immediately to train a NER model, then the BIO format is clearly more suitable.
If the goal is to provide a convenient format for other usages then something like this JSON format is fine, it's not really a matter of NER."
"Continous bag of words claimed to be unsupervised, how is it working?","The CBOW approach is unsupervised because the network learns the distribution of word co-occurrences around each word, and this doesn't require labelling or additional input, just sequences of words.

As Mikolov et al state in one of the original articles, ""the training objective is to learn word vector representations that are good at predicting the nearby words; in another, Mikolov et al say their aim is ""building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word"". So if you can see that for each sequence, you're taking 'x' words and training the network to predict one given the others, then no supervision is involved.

It's a little unconventional because it's not the output of the network which is important, but the weights that are learned during training - these are what is taken and used as embeddings in other tasks.

Adrian Colyer does a great general write-up of word2vec here, and the explanation from Chris McCormick here is good and quite accessible."
Information Extraction from News Headlines,"The closest NLP task that I can think of is automatic summarization: given some text of any length, the system is supposed to produce a short summary of the most important points.

I would imagine that if one provides multiple similar headlines to a good summarization system, the system should be able to output only the main information as output. It's not guaranteed that it would be exactly one sentence though."
Similarity between two words,"The closest would be like Jan has mentioned inhis answer, the Levenstein's distance (also popularly called the edit distance).

In information theory and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other.

It is a very commonly used metric for identifying similar words. Nltk already has an implementation for the edit distance metric, which can be invoked in the following way:

import nltk
nltk.edit_distance(""humpty"", ""dumpty"")


The above code would return 1, as only one letter is different between the two words."
How is attention different from linear MLPs?,"The crucial difference here is that attention allows working with arbitrarily long sequences (or rather sets) of vectors.

A linear layer has a constant-sized input. Each output activation in a linear layer is a linear combination of the activations in the previous layer. However, the input is always exactly one vector, so linear layers cannot in principle consider any context. Processing a sequence with linear layers only is equivalent to processing each vector in the sequence independently. (A straightforward update would be doing a sliding window over the vector sequence, this is called 1D convolution.)

Attention can work with arbitrarily long input. It computes the similarity between a query vector with all key vectors and retrieves corresponding values. Unlike linear layers, attention brings information about the context of the other vectors. In the self-attention case, all vectors interact with each other."
Are there any good out-of-the-box language models for python?,"The currently accepted answer by @lads is outdated and the answer of @zijun is too complex. This is what I used (mix of the 2 answers) :

import math
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')
model = GPT2LMHeadModel.from_pretrained('distilgpt2')
model.eval()

def score(sentence):
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    loss = model(tensor_input, labels=tensor_input)[""loss""].item()
    return math.exp(loss)

a=['there is a book on the desk', 'there is a plane on the desk', 'there is a book in the desk']

print([score(i) for i in a])
# > [156.42900910254363, 289.88481709498836, 186.85249335013398]
```"
bert-as-service maximum sequence length,"The default setting for max_seq_len is 25 as seen here under heading Server API: bert-as-service readme

There is an open issue regarding this on the Github repo here and the creator seems to be implementing a feature: bert-as-service issues"
What's an LSTM-LM formulation?,"The definition of a Language Model (LM) is a probability distribution over sequences of words.

The simple illustration of an LM is predicting the next word given the previous word(s).

For example, if I have a language model and some initial word(s):

I set my initial word to My
My model predicts there is a high probability that name appears after My.
By setting the initial words to My name, my model predicts there is a high probability that is appears after My name.
So it's like: My -> My name -> My name is -> My name is Tom, and so on.

You can think of the autocompletion on your smartphone keyboard. In fact, LM is the heart of autocompletions.

So, LSTM-LM is simply using an LSTM (and softmax function) to predict the next word given your previous words.

By the way, Language Model is not limited to LSTM, other RNNs (GRU), or other structured models. In fact, you can also use feedforward networks with context/sliding/rolling window to predict the next word given your initial words."
NLP LSTM input basic doubt,"The difference between the traditional bag of words representation and the word embedding representation is that:

bag of words: every index of a vector represents a specific word. Since there must be an index for every possible word, the dimension of every vector must indeed be the full vocabulary size.
embedding: words and sentences are represented in an (usually pre-trained) embedding space. The dimension of this space is predefined and arbitrary, and there's no way to know directly what every index represents. Indirectly, it can be proved that indexes can represent quite precise semantic concepts.

Anyway, in both cases the features values (which can vary) don't carry the meaning, it's always the fixed indexes which represent a particular semantic concept.

In your example, say the word ""Hi"" has index 1234: the fact that this specific index contains 6 or 4 allows the model to recognize a similarity between these 2 sentences. Note that in an embedding representation it's also the indexes which carry the concept. For example maybe ""Hi"" would have a important value for the dimension related to ""salutations"" and this would allow the model to find a similarity with worlds like ""hello"", ""dear X"", etc."
Difference between IOB and IOB2 format?,"The difference is not related to the length of the named entities. Rather, it deals with how two adjacent named entities of the same type are labeled.

In IOB1 (IOB), B- is only used to separate two adjacent entities of the same type:

Today    O
Alice    I-PER
Bob      B-PER
and      O
I        O  # or I-PER if pronominals are being tagged
ate      O
lasagna  O


In IOB2, all entities begin with B-:

Today    O
Alice    B-PER
Bob      B-PER
and      O
I        O  # or B-PER if pronominals are being tagged
ate      O
lasagna  O


See Wikipedia"
Does BERT need supervised data only when fine-tuning?,"The distinction between supervised and unsupervised is a little bit tricky here. BERT pre-training is unsupervised with respect to the downstream tasks, but the pre-training itself is technically a supervised learning task. BERT is trained to predict words that have been masked in the input, so the target words are known at training time. The term unsupervised fine-tuning is thus a little confusing here.

If we use BERT in a clever way (e.g., using a method called iPET), we can use its language modeling abilities to perform some tasks in an (almost) zero-shot setup, which basically means that BERT learned to perform the task in an unsupervised way. However, it is disputable, if this could be called unsupervised fine-tuning.

BERT can be of course fine-tuned in an unsupervised way by continued pre-training. It can be viewed as a way of domain adaptation of the model, which is typically again followed by supervised fine-tuning. Imagine you want to do a task on legal text, so you can first adapt BERT for legal text using large amounts of plain text, and fine-tune it using on much smaller labeled data."
How to develop small workable embeddings for debugging,"The embeddings are floating point vectors, it's very unlikely that they are exactly the same. And even if they were, it will only minorly impact the performance of your model, which is not what you are after anyway, because you are debugging. That said, if you want to use it for model selection you have a few alternatives. The easiest one is to apply PCA one time to your whole embedding space and take the first n dimensions. This way you reduce your dimensionality without throwing away too much information. A better but more difficult approach would be to train your own Glove vectors, either on a public corpus or a private one, and set a low dimensionality during the training procedure. It might be difficult to use your findings on these embeddings for model selection because more expressiveness could lead to much better performance with deeper models or could lead to more overfitting."
Wordnet Lemmatization,"The exception list files are used to help the processor find base forms from 'irregular inflections' according to the man page. They mean that some words, when plural or a different tense, can't be algorithmically processed to find the base/root word. More details can be found in the morphy man. I'm not a language processing expert, but this is likely a result of English words that 'break the rules'. If you think about the code like a human trying to learn English: the student learns rules to use (algorithm) and then has to memorize exceptions for the rules (exception lists). An over-simplified analogy that does not involve endings/conjugation would a spell checking program. An algorithm might check for 'i before e, except after c' but would first have to check the word against an exception list to make sure it isn't 'weird' or 'caffeine' - please don't start a linguistics fight about this rule, I am not commenting on the validity of it/that's not the point I'd like to make."
Unigram tokenizer: how does it work?,"The explanation in the documentation of the Huggingface Transformers library seems more approachable:

Unigram is a subword tokenization algorithm introduced in Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018). In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it‚Äôs used in conjunction with SentencePiece.

At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, i.e. those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.

Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:

[""b"", ""g"", ""h"", ""n"", ""p"", ""s"", ""u"", ""ug"", ""un"", ""hug""],

""hugs"" could be tokenized both as [""hug"", ""s""], [""h"", ""ug"", ""s""] or [""h"", ""u"", ""g"", ""s""]. So which one to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. The algorithm simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their probabilities.

Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of the words ùë•1,‚Ä¶,ùë•ùëÅ and that the set of all possible tokenizations for a word ùë•ùëñ is defined as ùëÜ(ùë•ùëñ), then the overall loss is defined as

L=‚àí
‚àë
N
i=1
log(
‚àë
x‚ààS(
x
i
)
p(x))
ùêø
=
‚àí
‚àë
ùëñ
=
1
ùëÅ
log
‚Å°
(
‚àë
ùë•
‚àà
ùëÜ
(
ùë•
ùëñ
)
ùëù
(
ùë•
)
)

There are some parts that are not very detailed, though, for instance, how it initializes the base (seed) vocabulary to a large number of symbols"". This part is more clearly explained in the original article by the end of section 3.2:

There are several ways to prepare the seed vocabulary. The natural choice is to use the union of all characters and the most frequent substrings in the corpus. Frequent substrings can be enumerated in 
O(T)
ùëÇ
(
ùëá
)
 time and 
O(20T)
ùëÇ
(
20
ùëá
)
 space with the Enhanced Suffix Array algorithm (Nong et al., 2009), where T is the size of the corpus.

About the details of the expectation maximization algorithm used to compute probabilities, this is what happens:

[Expectation] Estimate each subword probability by the corresponding frequency counts in the vocabulary
[Maximization] Use the Viterbi algorithm to segment the corpus, returning the optimal segments.

You can check the details, together with practical examples, in this tutorial."
Named entity recognition (NER) features,"The features for a token in a NER algorithm are usually binary. i.e The feature exists or it does not. For example, a token (say the word 'hello'), is all lower case. Therefore, that is a feature for that word.

You could name the feature 'IS_ALL_LOWERCASE'.

Now, for POS tags, lets take the word 'make'. It is a verb and hence the feature ""IS_VERB"" is a feature for that word.

A gazetter can be used to generate features. The presence (or absence) of a word in the gazatter is a valid feature. Example: the word 'John' is present in the gazetter of Person names. so ""IS_PERSON_NAME"" can be a feature."
Which algorithm is best for predicting diseases if symptoms are given? [closed],"The first challenge you will face is transforming the textual labels of symptoms into numeric features that can be understood by a model. The simplest techniques are

ordinal encoding, where each word is assigned to a number between 1 and N
and one-hot encoding (OHE), where each word becomes an unique N-length vector, with all elements == 0 except for one.

A good starting point for your project would be to one-hot encode each symptom, and for each entry in your dataset, sum the one-hot encoded symptoms to obtain a vector that describes all symptoms. This vector would be the input to your model.

For example, if you had the following one-hot encodings:

""vomiting"" -> [1, 0, 0]
""nausea"" -> [0, 1, 0]
""irritability"" -> [0, 0, 1]

Then an instance where there is both vomiting and nausea would be encoded as [1, 1, 0].

The drawback of the simple techniques is that the word encodings will not be very semantically meaningful. Notice that the distance between ""vomiting"" and ""nausea"" is the same as the distance between ""vomiting"" and ""irritibility"", even though vomiting and nausea are probably semantically closer in this context.

So if the simple techniques don't work well enough for you, then you can try word embeddings. Models such as Word2Vec and GloVe transform each word into a vector where distance in the vector space is meaningful. So ""nausea"" and ""vomiting"" will be close to each other, but ""vomiting"" and ""irritibility"" will be farther away.

Nowadays there are even more powerful embeddings trained on huge corpura of text. Examples include BERT and GPT-2. But I think this would be major overkill for your use case.

The next challenge is selecting a model that can map the encoded words to the disease. This is a multi-class classification problem, so that rules out binary classifiers. If there are a lot of different symptoms then your input vector is going to be high-dimensional, so it would be nice to have a model with built in feature selection or dimensionality reduction.

Starting out, I would recommend a ensembled decision tree model like Random Forest or XGBoost. These models are easy to train and generally perform well on this type of problem. Also, if you're using scikit-learn, you can easily get probabilities for each class with the predict_proba() method.

If the tree-based models don't work well, you could try other simple approaches like kNN or Naive Bayes

If you need more complexity, neural networks and deep learning are always on the table. But if your dataset has only 1450 examples, that's probably not enough to train most DNNs.

So to summarize, you need to encode the symptoms numerically and a good first try would be one-hot-encoding. You also need to pick a classifier that works well for this type of problem and can output probabilities rather than a single prediction. I think a good start would be the tree ensembles from scikit-learn. If the simple options don't work well enough, you can step up complexity on either the encoding side or the modeling side."
Why is word prediction an obsession in Natural Language Processing?,"The first line of the BERT abstract is

We introduce a new language representation model called BERT.

The key phrase here is ""language representation model"". The purpose of BERT and other natural language processing models like Word2Vec is to provide a vector representation of words, so that the vectors can be used as input to neural networks for other tasks.

There are two concepts to grasp about this field; vector representations of words and transfer learning. You can find a wealth of information about either of these topics online, but I will give a short summary.

How can BERT be expected to get this right, and how can humans or another algorithm be expected to evaluate whether ""soup"" is a better answer than ""coffee""?

This ambiguity is the strength of word prediction, not a weakness. In order for language to be fed into a neural network, words somehow have to be converted into numbers. One way would be a simple categorical embedding, where the first word 'a' gets mapped to 1, the second word 'aardvark' gets mapped to 2, and so on. But in this representation, words of similar meaning will not be mapped to similar numbers. As you've said, ""soup"" and ""coffee"" have similar meanings compared to all English words (they are both nouns, liquids, types of food/drink normally served hot, and therefore can both me valid predictions for the missing word), so wouldn't it be nice if their numerical representations were also similar to each other?

This is the idea of vector representation. Instead of mapping each word to a single number, each word is mapped to a vector of hundreds of numbers, and words of similar meanings will be mapped to similar vectors.

The second concept is transfer learning. In many situations, there is only a small amount of data for the task that you want to perform, but there is a large amount of data for a related, but less important task. The idea of transfer learning is to train a neural network on the less important task, and to apply the information that was learned to the other task that you actually care about.

As stated in the second half of the BERT abstract,

...the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.

To summarize, the answer to your question is that people DON'T care about the masked word prediction task on its own. The strength of this task is that there is a huge amount of data readily and freely available to train on (BERT used the entirety of wikipedia, with randomly chosen masks), and the task is related to other natural language processing tasks that require interpretations of the meaning of words. BERT and other language representation models learn vector embeddings of words, and through transfer learning this information is passed to whatever other downstream task that you actually care about."
FastText Model Explained,"The formula would make sense if 
y
n
ùë¶
ùëõ
 is a row vector representing the one-hot encoded label of the classes, and the multiplication is with the single column matrix 
log(f(BA
x
n
))
ùëô
ùëú
ùëî
(
ùëì
(
ùêµ
ùê¥
ùë•
ùëõ
)
)
 representing the log likelihood over all the clases given by the softmax function 
f
ùëì
.

As for 
x
n
ùë•
ùëõ
, it of course must be a vector as well, representing the 
N
ùëÅ
-grams in the 
n
ùëõ
-th document."
Measuring sentiment using a dictionary-based model,"The formula:

positive‚àínegative
total
positive
‚àí
negative
total

is intuitive and simple. The only problem is that it assumes that all sentiment -related words are of equal strength.

For example, the words good and great are both positive but do not express sentiment of the same strength.

So one improvement would be to assign weights to sentiment words (in the dictionary) and adjust the formula to take that into account.

‚àë
i
p
i
‚àí
‚àë
j
n
j
‚àë
ùëñ
ùëù
ùëñ
‚àí
‚àë
ùëó
ùëõ
ùëó

p
i
ùëù
ùëñ
 is weight of positive word in text 
n
j
ùëõ
ùëó
 is weight of negative word in text"
The principle of LM deep model,"The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language.

That is, to learn 
P(
w
1
,...,
w
N
)
ùëÉ
(
ùë§
1
,
.
.
.
,
ùë§
ùëÅ
)
 (resource).

This modeling can be accomplished by

Predicting the next word given the previous words: 
P(
w
i
|
w
1
,...,
w
i‚àí1
)
ùëÉ
(
ùë§
ùëñ
|
ùë§
1
,
.
.
.
,
ùë§
ùëñ
‚àí
1
)
, or
Predicting the neighbor words given the center word (Skip-gram): 
P(
w
i+k
|
w
i
),k‚àà{‚àí2,‚àí1,1,2}
ùëÉ
(
ùë§
ùëñ
+
ùëò
|
ùë§
ùëñ
)
,
ùëò
‚àà
{
‚àí
2
,
‚àí
1
,
1
,
2
}
, or
Predicting the center word given the neighbor words (CBOW or Continuous Bag-of-Words): 
P(
w
i
|
w
i‚àí2
,
w
i‚àí1
,
w
i+1
,
w
i+2
)
ùëÉ
(
ùë§
ùëñ
|
ùë§
ùëñ
‚àí
2
,
ùë§
ùëñ
‚àí
1
,
ùë§
ùëñ
+
1
,
ùë§
ùëñ
+
2
)
, or other designs.

Does the deep model need the encoder? From the ptb code of tensor2tensor, I find the deep model do not contains the encoder.

Yes. Modern LM solutions (all deep ones) try to find an encoding (embedding) that helps them to predict the next, neighbor, or center words as close as possible. However, a word encoding can be used as a constant input to other models. The ptb.py code calls text_encoder.TokenTextEncoder to receive such word encodings.

Both with-encoder and without-encoder can do the LM task?

LM task can be tackled without encoders too. For example, we can use frequency tables of adjacent words to build a model (n-gram modeling); e.g. all pairs (We, ?) appeared 10K times, pair (We, can) appeared 100 times, so P(can | We) = 0.01. However, encoder is the core of modern LM solutions."
What are the rules when extracting SVO triples from preprocessed text?,"The goal of Subject-Verb-Object (SVO) triples is to extract a single triple for a sentence.

The sentence:

A rare black squirrel has become a regular visitor to a suburban garden.

results in the following SVO:

(squirrel, become, visitor)

Triplet Extraction From Sentences by Rusu et al. outlines how to do that. First, you'll need a parse tree ( Stanford Parser and OpenNLP are the most common). The three items then can be extracted:

The subject will be found by performing breadth first search and selecting the first descendent of NP that is a noun.

‚Ä¶ the predicate of the sentence, a search will be performed in the VP subtree. The deepest verb descendent of the verb phrase will give the second element of the triplet.

‚Ä¶ we look for objects. These can be found in three different subtrees, all siblings of the VP subtree containing the predicate. The subtrees are: PP (prepositional phrase), NP and ADJP (adjective phrase). In NP and PP we search for the first noun, while in ADJP we find the first adjective."
Why TREC set two task: document ranking and passage ranking,"The idea behind the two tasks is to explore how document length affects the effectiveness of the different retrieval models.

Ellen Voorhees TREC project manager NIST"
Identify outliers for annotation in text data,"The idea is to find the documents which are not well represented in the current labeled data. The first point is indeed a bit vague and can probably be interpreted in different ways. My interpretation would be something like this:

For every document 
d
u
ùëë
ùë¢
 in the unlabeled data, count the number of words in common with every document 
d
l
ùëë
ùëô
 in the labeled data. This value is the ""match score"" between 
d
u
ùëë
ùë¢
 and 
d
l
ùëë
ùëô
.
Note: I think that this value should be normalized, for example using the overlap coefficient. Note that other similarity measures could be used as well, for instance cosine-TFIDF.
As output from the above step, for a single document 
d
u
ùëë
ùë¢
 one obtains a ""match score"" for every labeled document. The average across the labeled documents gives the ""average match"" for 
d
u
ùëë
ùë¢
."
Extracting sections from document based on list of keywords - Python,"The ideal way to get the related sentences would be to try to get a sentence vector for the sentences you want to categorise and then compare the vectors of your predefined keywords with the obtained sentence vectors . You can get the sentence vectors by just averaging the word vectors of the words present in the sentences . Once the sentence vectors are obtained , you can use cosine similarity to compare the keyword vectors and the sentence vectors . The one with the max cosine similarity will give you the result ."
Sentiment Analysis model for Spanish,"The Indico.io API supports Spanish (and Chinese (Mandarin), Japanese, Italian, French, Russian, Arabic, German, English).

eg in Python:

>>> import indicoio
>>> indicoio.config.api_key = <YOUR_API_KEY>
>>> indicoio.sentiment(""¬°Jam√°s voy a usar esta maldita aplicaci√≥n!  No funciona para nada."")
0.02919392219306888
>>> indicoio.sentiment(""¬°Es patr√≥n!  La mejor que he visto.  Punto."")
0.8860221705630639


If this isn't your area, then that's probably the easiest sort of solution to integrate."
Can BRAT be installed on Microsoft Windows?,"The installation documentation shows that the easiest way to run a brat server on Windows is in a virtual machine running a UNIX-like operating system such as Ubuntu. Also, there's an issue (still opening) about running brat server on windows. I think you're on your own right now."
Unable to save the TF-IDF vectorizer,"The issue is due to your lamda function with the tokenizer key word argument.

>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> from joblib import dump
>>> t = TfidfVectorizer()
>>> dump(t, 'tfidf.pkl')
['tfidf.pkl']


No issues. Now let's pass a lambda function to tokenizer

>>> t = TfidfVectorizer(tokenizer=lambda x: x.split())
>>> dump(t, 'tfidf.pkl')


Which throws the following error:

_pickle.PicklingError: Can't pickle at 0x100e18b90>: it's not found as main.

To get around this, create a function to split the text:

>>> def text_splitter(text):
...     return text.split()


Try dumping again:

>>> t = TfidfVectorizer(tokenizer=text_splitter)
>>> dump(t, 'tfidf.pkl')
['tfidf.pkl']


Now you can you save the vectorizer."
How to extract insights from the given data?,"The language that are used on review text just happened to be my native language. I can confirm that the review_text at least from what you showed above is a direct translation of the raw text (although I would say the translation is not perfect).

Maybe you can consider making unsupervised model and probably compare between these two, see how much they match (theoretically since they are the same text they should have many overlaps)."
Can we use sentence transformers to embed sentences without labels?,"The link you provided of Siamese Bert is an instance of a Bert or Roberta finetuned on STS or NLI data. Which can have the format sentence 1 is similar 3 out of 5 to sentence 2 (STS). Hence, is supervised, it does not fit your purpose.

Nonetheless, do not despair, there are some that do not require training, although may not perform as good as the supervised one. The below use word embeddings which you can train on your data corpora to generate sentence embeddings:

Word Mower's Distance
Sentence Embedding S3E

Or by feeding just sentences line by line:

DeCLUTR
Sent2Vec

P.S. I have not tried all of the solutions, to my knowledge I suggest these, cause either they are quite known or are quite recent."
How to justify logarithmically scaled frequency for tf in tf-idf?,"The logic is that you are taking just the tf part this might be weighted on the single document dimension or not (and in the logarithmic case it is not, in principle you could take a boolean scale as well, having 
1
1
 if the words appear in the document and 
0
0
 otherwise). You are missing the idf part, precisely the one weighing the importance of the word in the document (related to the number of times the term appears in all corpus.s)"
Python Text Classification - Data that does not fit into any category,"The main assumption in supervised learning is that the training set is a representative sample of the space of all possible instances for the problem.

This is why in the regular classification setting there is simply no way to consider a ""not in any class"" option: what the model learns is to distinguish between the classes seen in the training set, nothing else. In the standard one-vs-rest multi-class setting It's important to understand that the model doesn't predict whether an instance is likely to belong to ""class A"" in general, it only predicts whether the instance is more likely to belong to class A compared to any other known class. This is a common misunderstanding, but one-vs-rest is not an answer to the problem of ""not in any class"".

There are options to deal with this problem, but one must design the system so that the model can deal with such cases. In general the problem is actually open-set classification: in regular classification the set of possible classes is closed, as mentioned above. The open-set classification setting is much less standard and usually harder, since the model has to make a prediction about something it does not know from its training data. Here are a few options:

Treat the problem as an outlier detection problem: before running the regular classifier, eliminate abnormal instances which don't look like the regular training set cases.
Multi-label classification: in multi-label classification an instance may belong to zero, one or several classes. A binary classifier is trained for every class, and an instance may belong to none of the classes. This is more flexible than multi-class classification, but it's still closed-set classification: for example the model is supposed to have seen examples in the training set which don't belong to any class.
One-class classification is a type of classification where the model learns to identify a single class ""in general"", not by contrast to any other class. This is proper open-set classification: if a model is trained for every known class and every model predicts the instance as negative, then the instance doesn't belong to any known class."
What is the difference between a hashing vectorizer and a tfidf vectorizer,"The main difference is that HashingVectorizer applies a hashing function to term frequency counts in each document, where TfidfVectorizer scales those term frequency counts in each document by penalising terms that appear more widely across the corpus. There‚Äôs a great summary here.

Hash functions are an efficient way of mapping terms to features; it doesn‚Äôt necessarily need to be applied only to term frequencies but that‚Äôs how HashingVectorizer is employed here. Along with the 45339 documents, I suspect the feature vector is of length 1048576 because it‚Äôs the default 2^20 n_features; you could reduce this and make it less expensive to process but with an increased risk of collision, where the function maps different terms to the same feature.

Depending on the use case for the word vectors, it may be possible to reduce the length of the hash feature vector (and thus complexity) significantly with acceptable loss to accuracy/effectiveness (due to increased collision). Scikit-learn has some hashing parameters that can assist, for example alternate_sign.

If the hashing matrix is wider than the dictionary, it will mean that many of the column entries in the hashing matrix will be empty, and not just because a given document doesn't contain a specific term but because they're empty across the whole matrix. If it is not, it might send multiple terms to the same feature hash - this is the 'collision' we've been talking about. HashingVectorizer has a setting that works to mitigate this called alternate_sign that's on by default, described here.

‚ÄòTerm frequency - inverse document frequency‚Äô takes term frequencies in each document and weights them by penalising words that appear more frequently across the whole corpus. The intuition is that terms found situationally are more likely to be representative of a specific document‚Äôs topic. This is different to a hashing function in that it is necessary to have a full dictionary of words in the corpus in order to calculate the inverse document frequency. I expect your tf.idf matrix dimensions are 45339 documents by 663307 words in the corpus; Manning et al provide more detail and examples of calculation.

‚ÄòMining of Massive Datasets‚Äô by Leskovec et al has a ton of detail on both feature hashing and tf.idf, the authors made the pdf available here."
Next-word Generation in Tabular Dataset,The model is currently predicting the next single character. The output of the model should be the next token.
Why does joint embedding of word and images work?,"The model is more complex than point-wise multiplication of word and image embedding. It is a single neural network model thus backpropagation can improve the weights throughout the entire model. The training signal will adjust all layers to do better at the task, in this case the task appears to be question answering.

The figure is only showing the forward pass."
Should I rescale tfidf features?,"The most accepted idea is that bag-of-words, Tf-Idf and other transformations should be left as is.

According to some: Standardization of categorical variables might be not natural. Neither is standarization of Tf-Idf because according to stats stack exchange:

(it's) (...) usually is a two-fold normalization.

First, each document is normalized to length 1, so there is no bias for longer or shorter documents. This equals taking the relative frequencies instead of the absolute term counts. This is ""TF"".

Second, IDF then is a cross-document normalization, that puts less weight on common terms, and more weight on rare terms, by normalizing (weighting) each word with the inverse in-corpus frequency.

Tf-Idf is meant to be used in its raw form in an algorithm. Other numerical values are the ones that could be normalized if the algorithm needs normalization or the data is just too small. Other options can be using algorithms resistant to different ranges and distributions like tree based models or simply using regularization, it's up to the cross-validation results really.

But categorical features like bag-of-words, tf-idf or other nlp transformations should be left alone for better results.

However, there is also the idea of normalizing one-hot coded variables as something that can be done as a standard step same as in other datasets. And it's presented by a prominent figure in the field of statistics.

https://stats.stackexchange.com/a/120600/90513"
How to filter Named Entity Recognition results,"The most advanced (and complicated) approach to this is some sort of weakly-supervised system like Holoclean. It seems promising, but not easy.

The other end of the spectrum are heuristics like the one you propose. If you want to use a string distance approach like you describe, I would create some metric that gives points for both string edit distance and having matching words. If you can break the pipeline into smaller steps, maybe you can use some sort of topic analysis, then look for two similar entities and that topic in a knowledge graph, and see if the point to the same area.

The 2018 Alexa Prize winner paper, Gunrock is full of clever heuristics that you might be able appropriate.

Edit: you should also look at Entity Linking, which is more or less the problem you describe."
How to solve mismatched code dependency issues between two or more different ML models/structures/frameworks?,"The most common and used approach is to use virtual environment for both the model.

""A virtual environment is a Python environment such that the Python interpreter, libraries and scripts installed into it are isolated from those installed in other virtual environments, and (by default) any libraries installed in a ‚Äúsystem‚Äù Python, i.e., one which is installed as part of your operating system."" (Quoted from Python Doc)

The most used virtual environment packages are : virtualenv and conda. I would suggest you to use conda as it is easy to create different python environment without having them installed on the machine.

You can find a detailed discussion of both of them here : https://realpython.com/python-virtual-environments-a-primer/"
Which pre-trained model to select to generate embeddings from shop names written in English?,"The most common approach is to write custom preprocessing steps to standardize the names, examples include tokenizing, stemming, and lower casing.

After extensive preprocessing, the resulting tokens can be mapped to existing vector embeddings. One useful example is a model trained on Common Crawl, such as FastText."
What is the best way to create sequences for a text prediction model?,"The most common way to create sequences for NLP is masking. Mask (aka, redact) some of the words in a sentence and predict which words should replace those masked words.

Your examples are suboptimal. Both examples use fragments of sentences, the entire sentence should be used. Also both examples, only the last words are targets, any word(s) can be a target(s)."
Matching 2 keywords list using NLP,"The most simplest implementation would be using following steps :

Step 1 : Iterate through both the list 
Step 2 : Calculate the Cossine Similarity between each word in list1 with list2
Step 3 : Decide the threshold on cossine similarity. Higher means stricter


Code would be as follows:

list_1 = [ US, Apple, Trump, Biden, Mango, French, German]
list_2 = [State, iphone, ipad, ipod, president, person, Fruit, Language, Country]


# Download the package and model : 
from gensim.models import Word2Vec

similarity_dict = {}
for word_list1 in list_1:
    for word_list2 in list_2:
         model = Word2Vec.load(path/to/your/model)
         cosine_similarity = model.wv.similarity(word_list1, word_list2)
         


Pros :

Easy to implement

Uses Word2vec aso very reliable as word2vec ensure context

Easy to undertsand

Cons :

The code has complexity of O(n*n)

It will not work with Words which are out of vocabulary in word2vec"
Is there a dataset with news articles and their headlines? [closed],"The most widely used ones in text summarization research is the DUC dataset. If you see a paper using dataset ""DUC 2015"" or ""DUC 2016"" that's from here.

I have also personally used the Reuters arcihve. You just need to download each article with wget or something similar. See also here.

The CNN / DailyMail dataset is also widely used in summarization especially in recent years, although it labels itself as a Q&A dataset."
NLP: What are some popular packages for multi-word tokenization?,"The multiword tokenizer 'nltk.tokenize.mwe' basically merges a string already divided into tokens, based on a lexicon, from what I understood from the API documentation.

One thing you can do is tokenize and tag all words with it's associated part-of-speech (PoS) tag, and then define regular expressions based on the PoS-tags to extract interesting key-phrases.

For instance, an example adapted from the NLTK Book Chapter 7 and this blog post:

def extract_phrases(my_tree, phrase):
   my_phrases = []
   if my_tree.label() == phrase:
      my_phrases.append(my_tree.copy(True))

   for child in my_tree:
       if type(child) is nltk.Tree:
            list_of_phrases = extract_phrases(child, phrase)
            if len(list_of_phrases) > 0:
                my_phrases.extend(list_of_phrases)

    return my_phrases



def main():
    sentences = [""The little yellow dog barked at the cat"",
                 ""He studies Information Technology""]

    grammar = ""NP: {<DT>?<JJ>*<NN>|<NNP>*}""
    cp = nltk.RegexpParser(grammar)

    for x in sentences:
        sentence = pos_tag(tokenize.word_tokenize(x))
        tree = cp.parse(sentence)
        print ""\nNoun phrases:""
        list_of_noun_phrases = extract_phrases(tree, 'NP')
        for phrase in list_of_noun_phrases:
            print phrase, ""_"".join([x[0] for x in phrase.leaves()])


You defined a grammar based on regex over PoS-tags:

grammar = ""NP: {<DT>?<JJ>*<NN>|<NNP>*}""
cp = nltk.RegexpParser(grammar)


Then you applied it to a tokenized and tagged sentence, generating a Tree:

sentence = pos_tag(tokenize.word_tokenize(x))
tree = cp.parse(sentence)


Then you use extract_phrases(my_tree, phrase) to recursively parse the Tree and extract sub-trees labeled as NP. The example above would extract the following noun-phrases:

Noun phrases:
(NP The/DT little/JJ yellow/JJ dog/NN) The_little_yellow_dog
(NP the/DT cat/NN) the_cat

Noun phrases:
(NP Information/NNP Technology/NNP) Information_Technology


There is a great blog post by Burton DeWilde about many more ways to extract interesting keyphrases: Intro to Automatic Keyphrase Extraction"
Punctuation removal effect on n gram detection for keyword generation,"The n-gram model is often built after segmenting into words and sentences. If the data is segmented by sentence it's easy to avoid any overlap between sentences: one can simply extract n-grams sentence by sentence independently. In case it's more convenient to extract all the n-grams at once, padding can be used to mark the beginning/end of sentences like this:

The sign was red #SENT# Balls are the toys of children #SENT#

Dealing with other punctuation signs which don't mark the end of a sentence can be a bit more tricky, especially if you want to keep the possibility of a keyword which spans over some punctuation signs (for instance in ""red-handed"" or ""tl;dr"")."
What is the difference between BERT architecture and vanilla Transformer architecture,"The name provides a clue. BERT (Bidirectional Encoder Representations from Transformers): So basically BERT = Transformer Minus the Decoder

BERT ends with the final representation of the words after the encoder is done processing it.

In Transformer, the above is used in the decoder. That piece of architecture is not there in BERT"
Why is the decoder not a part of BERT architecture?,"The need for an encoder depends on what your predictions are conditioned on, e.g.:

In causal (traditional) language models (LMs), each token is predicted conditioning on the previous tokens. Given that the previous tokens are received by the decoder itself, you don't need an encoder.
In Neural Machine Translation (NMT) models, each token of the translation is predicted conditioning on the previous tokens and the source sentence. The previous tokens are received by the decoder, but the source sentence is processed by a dedicated encoder. Note that this is not necessarily this way, as there are some decoder-only NMT architectures, like this one.
In masked LMs, like BERT, each masked token prediction is conditioned on the rest of the tokens in the sentence. These are received in the encoder, therefore you don't need an decoder. This, again, is not a strict requirement, as there are other masked LM architectures, like MASS that are encoder-decoder.

In order to make predictions, BERT needs some tokens to be masked (i.e. replaced with a special [MASK] token. The output is generated non-autoregressively (every token at the output is computed at the same time, without any self-attention mask), conditioning on the non-masked tokens, which are present in the same input sequence as the masked tokens."
Extracting Names using NER | Spacy,"The NER model performance on a particular text depends on which data it was trained with originally, and naturally the standard models (like en_core_web_sm) are trained with English data which doesn't contain a lot of names from non-US/UK origin (same for other kinds of entities like organizations or locations).

Better performance can be achieved by training your own model with your own labelled data, but that requires you (or somebody) to annotate a reasonably large sample of data manually."
Is the Transformer decoder an autoregressive model?,"The normal Transformer decoder is autoregressive at inference time and non-autoregressive at training time.

The non-autoregressive training can be done because of two factors:

We don't use the decoder's predictions as the next timestep input. Instead, we always use the gold tokens. This is referred to as teacher forcing.
The hidden states of all time steps are computed simultaneously in the attention heads. This is different in recurrent units (LSTMs, GRUs), where we need to have the previous timestep's hidden state to compute the current timestep's one.

At inference time, we don't have gold tokens, so we use the prediction of the decoder as the next timestep input.

Note that, despite being autoregressive at inference time, efficient implementations normally cache the hidden states of the previous timesteps, so they are not re-computed at each step.

As a whole, the Transformer model is autoregressive, because its decoder is autoregressive.

There are non-autoregressive variants of the Transformer (e.g. this), but they are more research topics than out-of-the-box solutions."
Clarification on the Keras Recurrent Unit Cell,"The notation used for LSTM is quite confusing, and this took me some time to get my head around this as well. When you see this graphic (often used to explain RNNs):

You need to consider that X is a sequence of data the timestep t, it's not just a single scalar input value (as we're used to with feed-forward networks); it's an array / tensor of data. The diagram shows how there is an output at time-step t, but that it then also feeds into the next time-step, t+1, when the next array/tensor is then fed in.

A better/clearer way (in my opinion) is to look at it is like this:

So a LSTM 'cell' is actually what you might consider a layer. And a unit (one of the circles) is one of these:

which you can consider a neuron in a hidden layer.

Initially, I thought this was a cell, but it isn't, it's a single unit. So when you specify 32 units, for example, you're actually saying how many of these units (neurons) you want in the cell (layer).

This is what gives the model its capacity to learn the data that's presented to it. And just like hidden layers/neurons in a feed-forward, it's a hyperparameter that you'll need to experiment with; too low and the model will under-fit, too high and it will over-fit."
"""Random Forest"" variant of other classifiers","The notion of ensembles of models leading to better outcomes is widespread in the Data Science and Machine Learning communities. For example, the widely read text 'The Elements of Statistical Learning' by Hastie, Tibshirani and Friedman, devotes space to the general notion of model averaging (section 8.7-8.8) with further discussion of boosting in relation to trees (e.g. chapter 10). Hence, it is common to see examples of ensembled models in the wild, using any conceivable algorithm as the base, even to the point of effectively seeing ensembles of ensembles.

Specifically with respect to Support Vector Machines, there are at least a few attempts at improving SVM performance by using ensembles. A recent example is EnsemblesSVM (Claesen,De Smet, Suykens, De Moor - see homes.esat.kuleuven.be/~claesenm/ensemblesvm/). The authors promise that this algorithm will lead to better training times by ensembling lightly trained SVMs, and distribute a free implementation via the website above."
How to create a training set and classify them as positive or negative,"The only way to obtain a high-quality dataset in your specific domain is to do it manually. There exists no other method that can give you the sentiment labels for texts in arbitrary domains. If there would exists such a method, why would you even bother to create your own model.

You should probably find/hire people that can do this work for you. Adding the sentiment meta-data to the texts could be as easy as creating an additional column in excel. But you could also smoothen the process by creating a little app that shows the sentence and let the user decide what sentiment it is (e.g. by swiping to left or right). Find a bunch of people that are willing to classify 10 sentences a day and your dataset will grow steadily. Even better, apply a little gamification and give people points or a rank and give an award to the best performing user to boost their performance.

Keep in mind that if these texts are really domain-specific you probably need domain-experts to do the classification.

When having enough training data you could learn a (simple) that predicts the output. Then you could let the human classifiers focus on the sentences with low confidence, assuming that high confidence automatic classifications are correct."
What algorithm is used to extract keywords from unstructured texts?,"The OP asks two different questions: (1) how to extract key words and (2) how to assign keywords a sentiment class (pos/neg/neu). I will address the keyword identification piece in this answer as many others have discussed how to do sentiment analysis (e.g., this post).

The approach I would suggest is a key keyword approach advocated by Mike Scott (author of WordSmith Tools) and Chris Tribble (a computational linguist). As discussed here, the basic approach is to create two corpora, your target corpus which is composed of texts sampled from the texts you are interested in, and a reference corpus, which is typically a much larger corpus (which is often more general in content).

The procedure begins by computing word (or n-gram) frequencies for both corpora. During this process, if a word‚Äôs frequency in the target corpus is found to be statistically probable in comparison to the reference corpus (as computed by a chi-squared test and a user defined p-value), it is considered a keyword (Baker, 2004). According to Scott (2006), the process typically identifies three types of words as key: proper nouns, words that characterize a text‚Äôs ‚Äúaboutness,‚Äù and high frequency words that are indicators of style or genre. I discuss the approach in greater length in this 2007 article where I use the method to extract the salient features of academic discourse.

To illustrate with a concrete example, imagine your are interested in identifying the key topics surrounding a target brand (say, Pepsi Cola) as expressed in social media (e.g., twitter). Create two corpora: for your target corpora you create a search for ""pepsi"" and for the reference corpora you could search for competitor brands of soft drinks (""coke"", ""mountain dew"", ""dr. pepper"", etc,). When the keyword process has terminated you will be left with all the keywords/topics that differentiate pepsi from other soft drink brands (and as a bonus you will also identify negative keywords... words that occur statistically less frequent in the target corpus).

As you might surmise, the results you get depend upon how the reference corpus is constructed. This, in my opinion, is a feature - as it gives the researcher much greater flexibility in hypothesis testing and data exploration."
Cross between an edit distance algorithm and a phonetic algorithm,"The operations in the edit distance can have different weights that you can try to set manually. In Python, you can use e.g., strsympy or weighted-levenshtein package.

There is also Learnable Edit Distance algorithm that allows learning the operation weights from data. I think it might be a good idea first to estimate the operation cost first manually and then fine-tune it with the learning algorithm. Unfortunately, I am not aware of any reasonable implementation.

There is also a CRF-based classifier that is based on the learnable edit distance. Based on training data, it estimates the probability of one string being a transcription of another one. This approach requires positive and negative training examples."
Is it normal when BLEU score on filtered data by length is greater than BLEU score on whole data,"The original BLEU scores 25.9 and 25.7 are very close, there might not even be any significant difference. It's totally possible that model B performs better than model A on the filtered data only by chance. It's also possible that model B actually performs better than model A on shorter sentences. And finally it's worth noting that BLEU score is based on the number of n-grams in common, so it's likely to be affected by the length of sentences independently from the model being tested.

Conclusion: based on the information provided, this difference seems perfectly reasonable."
How special tokens in BERT-Transformers work?,"The original code from the Bert model doesn't have any mention of special tokens [CLS], [SEP], or [EOS].

So it seems that the data in input is already organized to fit Bert's input format.

There is a Bert convention about those special tokens:

# The convention in BERT is:
# (a) For sequence pairs:
#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
#  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
# (b) For single sequences:
#  tokens:   [CLS] the dog is hairy . [SEP]
#  type_ids: 0     0   0   0  0     0 0
#
# Where ""type_ids"" are used to indicate whether this is the first
# sequence or the second sequence. The embedding vectors for `type=0` and
# `type=1` were learned during pre-training and are added to the wordpiece
# embedding vector (and position vector). This is not *strictly* necessary
# since the [SEP] token unambiguously separates the sequences, but it makes
# it is easier for the model to learn the concept of sequences.
#
# For classification tasks, the first vector (corresponding to [CLS]) is
# used as the ""sentence vector"". Note that this only makes sense because
# the entire model is fine-tuned.


This is from the feature extraction process.

Consequently, [CLS] tokens are useful to get a row of position tokens, and [SEP] tokens are useful to differentiate the questions from answers through type_ids.

The position and type tokens are converted to tensors. However, the [CLS] and [SEP] tokens are not converted to tensors because their function is just to delimit the input data.

In the Bert model, the token_type_ids are used in the post-processor embedding, together with the position_ids to an output.

  if use_token_type:
    if token_type_ids is None:
      raise ValueError(""`token_type_ids` must be specified if""
                       ""`use_token_type` is True."")
    token_type_table = tf.get_variable(
        name=token_type_embedding_name,
        shape=[token_type_vocab_size, width],
        initializer=create_initializer(initializer_range))
    # This vocab will be small so we always do one-hot here, since it is always
    # faster for a small vocabulary.
    flat_token_type_ids = tf.reshape(token_type_ids, [-1])
    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
    token_type_embeddings = tf.reshape(token_type_embeddings,
                                       [batch_size, seq_length, width])
    output += token_type_embeddings


Then, this output is normalized and a dropout is applied.

In conclusion, special tokens are defined by a convention, and the 2 main ones are [CLS] and [SEP] which delimit the 2 main types of vectors necessary for the Bert model for the question/answer process.

Note: You can define [CLS] or [SEP] with other names in the Pretrained tokenizer from HuggingFace with the sep_token and the cls_token attributes."
What is the bleu score of professional human translators?,"The original paper ""BLEU: a Method for Automatic Evaluation of Machine Translation"" contains a couple of numbers on this:

The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even ‚Äúrough‚Äù comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references.

But as their table 1 (providing the numbers compared to two references, H2 is the one mentioned in the text above) shows there is variance among human BLEU scores:

Unfortunately, the paper does not qualify the skill level of the translators."
What dataset was Stanford NER trained on?,"The original paper mentions two corpora: CoNLL 2003 (apparently here now) and the ""CMU Seminar Announcements Task"". However according to the page linked in the question the actual NER was trained on a larger combination of corpora:

Our big English NER models were trained on a mixture of CoNLL, MUC-6, MUC-7 and ACE named entity corpora, and as a result the models are fairly robust across domains.

So it might be difficult to obtain the exact original training data. However most of these corpora were compiled for some shared tasks and should be available online. There are probably more recent ones as well: a quick search ""named entity recognition shared task"" returns many hits."
How PV-DBOW works,"The paragraph vectors are trained by using the information of words in a paragraph. If we randomly sample without distinguishing paragraph as you suggest, a paragraph vector will be adjusted in vector space by other words which are not components of the paragraph. So it will map/project the paragraph vector by irrelevant syntactical and semantical information of words.

Both hierarchical softmax and negative sampling are not one of gradient-based optimizer. These methods just adjust/change the objective function to train much easier and faster.

I'm not sure but I think it‚Äôs not possible. Check this: doc2vec - How does the inference step work in PV-DBOW"
What exactly are the parameters in GPT-3's 175 billion parameters?,"The parameters in GPT-3, like any neural network, are the weights and biases of the layers.

From the following table taken from the GTP-3 paper

there are different versions of GPT-3 of various sizes. The more layers a version has the more parameters it has since it has more weights and biases. Regardless of the model version, the words it was trained on are the 300 billion tokens the caption references with what appears to be around 45TB of data scraped from the internet."
Why Heaps' Law Equation looks so different in this NLP course?,"The plot shows Heaps' Law but the formula is something different, it is Zipf's Law.

f(w)
ùëì
(
ùë§
)
 is the relative frequency (or probability) of word 
w
ùë§
. That is, given a random word, it will be 
w
ùë§
 with probability 
f(w)
ùëì
(
ùë§
)
. Therefore, if a document has 
n
ùëõ
 words, it has on average 
n√óf(w)
ùëõ
√ó
ùëì
(
ùë§
)
 occurrences of word 
w
ùë§
.

The formula can be re-written as follows:
f(w)=C(r(w)‚àíb
)
‚àíŒ±
ùëì
(
ùë§
)
=
ùê∂
(
ùëü
(
ùë§
)
‚àí
ùëè
)
‚àí
ùõº
which is a power-law distribution that shows Zipf's Law, however with a slightly different parameterization by introducing cut-off 
b
ùëè
.

r(w)
ùëü
(
ùë§
)
 denotes the rank of word 
w
ùë§
. For example, if we sort all the words in a news corpus based on their frequency, 
r('the')
ùëü
(
'the'
)
 would be 1, 
r('be')
ùëü
(
'be'
)
 would be 2, and so on,

Cut-off 
b
ùëè
 ignores highly frequent words 
r(w)‚â§b
ùëü
(
ùë§
)
‚â§
ùëè
, effectively shifting up the rank of remaining words,

C
ùê∂
 is the normalizing constant, i.e. 
C=
‚àë
‚àû
r=‚åäb‚åã+1
(r‚àíb
)
‚àíŒ±
ùê∂
=
‚àë
ùëü
=
‚åä
ùëè
‚åã
+
1
‚àû
(
ùëü
‚àí
ùëè
)
‚àí
ùõº
, which gives 
‚àë
w,r(w)>b
f(w)=1
‚àë
ùë§
,
ùëü
(
ùë§
)
>
ùëè
ùëì
(
ùë§
)
=
1
, and

Exponent 
Œ±
ùõº
 denotes the rate of drop in probability when rank increases. Higher 
Œ±
ùõº
, faster drop.

Exponent 
Œ±
ùõº
 is determined by fitting the formula to some corpus, as shown in the table. Generally, lower 
Œ±
ùõº
 (in the case of twitter), thus slower drop, means corpus has more word diversity."
word2vec: usefulness of context vectors in classification,"The practice for word2vec is to use only target (hidden) embeddings. But there are some works (for example this paper) about combining target vectors with context vectors. It's not always the case that you will achieve better results by combining embeddings.

In the GloVe paper, the authors achieved a small boost in performance by summing these vectors."
GMM in speech recoginition using HMM-GMM,"The previous answer was wrong so I removed it.

Here goes my second attempt after reading Speech and Language processing by daniel Jurafsky and James H Martin(good book to read).

The 39 features associated with an observation/acoustic is considered to have come from mixtures of multivariate gaussian.

Why Mixture of MV gaussian ? Assuming a single MV gaussian for each state(phones) is a strong assumption which might not be true.

How does HMM comes into picture with GMM in ASR: Consider an uni-variate case where a single cepstral feature(usually it is 39) is represented by a single gaussian and HMM state has a mean value and variance which generate the particular observation. To get which observation was produced by which state is a part of decoding problem.

Let me know if this is right ?"
Most Efficient Machine Learning Algorithm for Text Analysis,"The previously selected good answer is true in the sense that CNN and RNN where the bests choices the last few years for NLP (combined with unsupervised methods like word2vec, glove or wordpiece). But recent works use the attention neural network called the Transformer. See Attention Is All You Need and BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. These model now achieves state of the art performance in many NLP tasks."
Understanding the generality of the NER problem,"The problem described is not a more general version of Named Entity Recognition, it is a different problem called parsing. Parsing consists in extracting the syntactic structure of a text, usually in the goal to better capture its semantics. There are various approaches:

Shallow parsing only identifies the constituents of the sentences (based on your example this could be sufficient in your case)
Statistical parsing and in particular Dependency parsing represent the full structure of the sentence, including the links between its constituents.

There are various libraries and datasets for parsing: one of the most famous is probably the Stanford parser, but there are many others often included in NLP toolkits such as OpenNLP. The Universal Dependencies project is a vast multilingual collection of annotated text which can be used to train parsers.

Semantic Role Labeling (SRL) is a closely related task which consists in identifying the semantic relations between a predicate (verb) and its related constituents (e.g. subject, object)."
how to extract common aspects from text using deep learning?,"The problem is called by several different names: multi-aspect, semi-supervised, or hierarchical topic modeling.

One way to approach the problem is to start with a seed or anchor words (e.g., ""winter"" and ""summer"") and form clusters around those words.

It can be solved with a variety of methods, the most commonly used methods are variations of Latent Dirichlet Allocation (LDA)."
Does it make sense to use a tfidf matrix for a model which expects to see new text?,"The problem is not really ""new text"", since by definition any classification model for text is meant to be applied to some new text. The problem is out of vocabulary words (OOV): the model will not be able to represent words that it didn't see in the training data.

The most simple way (and probably the most standard way) to deal with OOV in the test data is to completely remove them before representing the text as features.

Naturally OOV words can be a serious problem, especially in data such as Twitter where vocabulary evolves fast. But this issue is not related to using TF-IDF or not: any model trained at a certain point in time can only take into account the vocabulary in the training data, it cannot guess how future words are going to behave with respect to the class. The only solution for that is to use some form of re-training, for instance semi-supervised learning."
"LSTM for text with different sentences size, but same input-output sizes","The problem is, the LSTM results, after excluding the tags that correspond to the padding portion give outputs of different size than the input.

How do you get the ""LSTM results""? Are the ""LSTM results"" extracted making a prediction for each word in the original sentence?

The LSTM is not a problem to do what you want to do. It might be an issue of your implementation. If you put a snippet of code maybe you could get some help."
Getting unexpected result while using CountVectorizer(),"The problem lies in the line:

keys = 'keys_' + str(i+1)


Here, keys becomes a string variable while I guess you would expect it to take the value of the list you defined in the first lines..

Try instead with a dictionary:

my_keys= {
""keys_1"" : ['funny', 'amusing', 'humorous', 'hilarious', 'jolly'],
""keys_2"" : ['horror', 'fear', 'shock', 'panic', 'scream'],
""keys_3"" : ['romantic', 'intimate', 'passionate', 'love', 'fond']
}
text = ('funny amusing fear passionate')

for i in range(3):
    keys = my_keys['keys_' + str(i+1)]
    cv = CountVectorizer(vocabulary = keys) 
    data = cv.fit_transform([text]).toarray()
    print(data)"
NLP Feature creation from phrase matching,"The problem with imbalance is that the optimizer can get a very good score by declaring everything 'not raised'. You need to cheat with your training data by removing that incentive. I would suggest a training set that is balanced 50/50 between the classes. Your evaluation set can still be representative, which will give you a sense of how it'll generalize."
Best way to vectorise names and addresses for similarity searching?,"The problem you are describing is commonly called record linkage, in particular probabilistic record linkage.

Clustering the embeddings would work if the different personas for the same entity frequently co-occur. Each item has to be tagged with metadata so clustering would only return the same type of info (e.g., only person names)."
Find matching text from a text column,"The problem you are suggesting is a text summarization problem. It can be of two types abstractive - understanding the text distribution and then producing a summary, extractive - extracting specific words from the text to produce a summary. Clearly, your problem falls in the latter category. Also, since you have the labels to be generated it is a supervised text summarization problem. Take a look at this paper regarding algorithms for the same. It suggests two solutions for a similar problem, you'll of course have to modify them a bit and see which one works best for you. Good luck."
n-gram Model - Why Smoothing?,"The purpose of smoothing is to prevent a language model from assigning zero probability to unseen events.

That is needed because in some cases, words can appear in the same context, but they didn't in your train set. Smoothing is a quite rough trick to make your model more generalizable and realistic. You can also see it as a tool to prevent overfitting."
Data Mining - Intent matching and classification of text,"The questions you have listed are more or less independent from each other. To extract location, you should be looking into the class of solutions called Named Entity Recognition. It is widely supported, and, for example, NLTK should have location extraction support for languages but English.

The second task, the line of business, seems cryptic to me, since you are solving the task, you have complete information on the full list of those lines, so probably writing any kind of detector (custom NER would work) is a way to go, however, if all the queries are formed as LOB + location, by solving the first one, you can subtract the location out of query to get the line of business.

From the examples of queries you have provided, I don't see any way for a human being to extract any affinity, so you should investigate it to understand if those queries actually carry such kind of information.

With (1) and (2) given, the fourth task should be trivial to count."
Twitter Sentiment Analysis: Detecting neutral tweets despite training on only Positive and Negative Classes,"The quick (and not very satisfying) answer is ""it depends"" -- specifically it depends upon what your underlying conceptual model of human emotion is and how it manifests in verbal/written behavior.

What is your characterization of neutrality in relation to positive and negative valence? Can documents be put on some sort of quantitative scale with neutral sandwiched between positive and negative? This position has linguistic support at least with simple phrases which express single valence states (the solution is {great > good > acceptable > poor > horrible}).

As @dmb poster suggested, if this is your conceptual model, then you might reasonablly argue that neutral falls between pos/neg and all you have to do is determine the optimal boundaries/cutoff. Thus, you can assign neutral to a test case even if the classifier wasn't trained on neutral cases (though you do need some way of determining these cutoffs).

But what about more complex cases? What happens when you move from the level of the phrasal unit to larger sentential and discourse level units? How would you rate the sentence ""I love apples but hate bananas""? Do the positive and negative elements cancel each other out to create a neutral? You can easily see how muddied this gets when we start to talk about real human texts. In my opinion, I don't think it is particularly meaningful to talk about a document-level emotion score. Rather, I believe people express emotion/sentiment that is directed at an individual objects ""I hate bananas"" -> hate(subj, obj) that get combined into discourse-level constructions.

So, yes, I think you could reasonable defend the use of outputting a neutral category if only pos/neg are used to train... BUT you will need to justify your use of particular cutoffs as well as consider how to deal with longer documents that express multiple (conflicting) emotions. My preferred course of action would be to have people read and label tweets as pos, neg, neutral, and mixed (then ensure inter-coder reliability) to create my training data... let the algorithm do the hard work of finding the cutoff values."
Text classification with thousands of output classes in Keras,"The ratio of number of examples to number of classes is not large. There are few classes which have high number of occurrences (from the second graph) and the distribution seems to be following power law.

In such cases, I'll advise the following strategy,

Sort your tags by number of occurrences and discard tags which occur very few number of times. This will make the problem more tractable.
You can benchmark the accuracy that you get from classical machine learning techniques. Many classical methods support multilabel output, you can check the documentation of support in scikit-learn library here.
You can do a mix of unsupervised learning and nearest neighbours approach. For example, learn doc2vec embeddings on your data including tags and suggest tags from nearest matching document for a new input. The number of documents is small by doc2vec standards and you will need careful tuning of doc2vec parameters.
With neural networks, you can use more suitable loss functions like multilabel soft margin loss.

I'd advise you start with classical techniques to first benchmark the potential accuracy."
Ratio between embedded vector dimensions and vocabulary size,"The ratio of vocabulary vs embedding length to determine the size of other layers in a neural network doesn't really matter. Word embeddings are always around 100 and 300 in length, longer embedding vectors don't add enough information and smaller ones don't represent the semantics well enough. What matters more is the network architecture, the algorithm(s) and the dataset size.

A simple way to understand this concept is that a bidirectional LSTM model with 50 neurons (nodes) followed by a fully connected layer of 70 neurons will outperform a simple MLP of 1000 neurons (nodes) connected to a embedding layer simply due to its architecture. Adding dropout will improve performance as well.

In addition, even if the vocabulary is just 300 words, using pre-trained embeddings will probably yield better results than training the embeddings directly on the dataset. The same applies to data size, a dataset with more samples will make a better classifier than a dataset with just a couple thousand samples.

In summary, it is preferable to try many architectures and cross-validate them (and/or ensemble them depending if you have a large enough dataset) with the smallest number of neurons possible and then start building up in size, depending on what computational resources you have and the speed of development you need. Large models slow down development speed whereas small models speed it up. This goes whether your vocabulary is the size of common crawl or just 300. As usual, try feature engineering (sentence length, special characters, etc.) and increase the dataset size as doing so often helps in whatever task you're trying to predict."
Why is max_features ordered by term frequency instead of inverse document frequency,"The reason is probably that using the top IDF features would mean selecting the least frequent words, in particular the ones which appear only once and are very frequent. These rare words should usually be removed because they are often due to chance and anyway are unlikely to appear again, therefore these are bad features likely to cause overfitting.

In other words, it's always better for the features to be frequent so their statistical relations with other variables (especially the target) can be estimated reliably by the algorithm. Picking the top IDF features would do the opposite: take the least reliable statistical information into account."
What are some key strengths of BERT over ELMO/ULMFiT?,"The reason you're seeing BERT and its derivatives as benchmarks is probably because it is newer than the other models mentioned and shows state-of-the-art performance on many NLP tasks. Thus, when researchers publish new models they normally want to compare them to the current leading models out there (i.e BERT). I don't know if there has been a study on the strengths of BERT compared to the other methods but looking at their differences might give some insight:


Truly Bidirectional
BERT is deeply bidirectional due to its novel masked language modeling technique. ELMo on the other hand uses an concatenation of right-to-left and left-to-right LSTMs and ULMFit uses a unidirectional LSTM. Having bidirectional context should, in theory, generate more accurate word representations.


Model Input
BERT tokenizes words into sub-words (using WordPiece) and those are then given as input to the model. ELMo uses character based input and ULMFit is word based. It's been claimed that character level language models don't perform as well as word based ones but word based models have the issue of out-of-vocabulary words. BERT's sub-words approach enjoys the best of both worlds.


Transformer vs. LSTM
At its heart BERT uses transformers whereas ELMo and ULMFit both use LSTMs. Besides the fact that these two approaches work differently, it should also be noted that using transformers enables the parallelization of training which is an important factor when working with large amounts of data.

This list goes on with things such as the corpus the model was trained on, the tasks used to train and more. So while it is true that BERT shows SOTA performance across a variety of NLP tasks, there are times where other models perform better. Therefore, when you're working on a problem it is a good idea to test a few of them a see for yourself which one suits your needs better."
Dot product for similarity in word to vector computation in NLP,"The reference points to the word2vec library (source code).

It does not use normalised vectors during training (although it indeed uses the cosine similarity metric for semantic comparisons on already trained vectors).

The reasons for using only dot product instead of cosine similarity during training can be due to:

Dot product is a variation of cosine similarity.
Length captures some semantic information in the sense that length can correlate to frequency of occurance in a given context, so using dot product only captures this information as well (although for strict similarity testing cosine metric is still used)
When vectors are normalised the two metrics coincide.
Efficiency (doing less computations)

References:

Why does the word2vec objective use the inner product (inside the softmax), but the nearest neighbors phase of it uses cosine? It seems like a mismatch.
Should I normalize word2vec's word vectors before using them?
Measuring Word Significance using Distributed Representations of Words
word2vec Parameter Learning Explained"
Why does vanilla transformer has fixed-length input?,"The restriction in the maximum length of the transformer input is due to the needed amount of memory to compute the self-attention over it.

The amount of memory needed by the self-attention in the Transformer is quadratic on the length of the input. This means that increasing the maximum length of the input, increases drastically the needed memory for self-attention. The maximum length is that which makes the model use up the whole memory of the GPU for at least one sentence (once the other elements of the model are also taken into account, like the embeddings which take a lot of memory).

Transformer-XL is certainly a way to take into account as much context as possible in language modeling (its role is analogous to truncated back-propagation through time in LSTM language models). However, the gradients are not propagated through the attention over the memory segment, only through the current segment.

There have been several architectural attempts to reduce the amount of memory needed by transformers, like using locality-constraints in the attention (Dynamic Convolutions model) or using locality-sensitive hashing (Reformer model).

There have been other implementation attempts, like gradient checkpointing(e.g. this), which is a general technique to run computations that don't fit at once in the GPU memory"
How to apply two input and one output with LR and SVM,"The simplest option in order to represent the two sentences independently of each other is to represent each of the two sentences with its own TFIDF vector of features and concatenate the two vectors. In other words you obtain 2 * N features where N is the size of the vocabulary.

But at first sight it looks like the wrong approach for the problem that you're trying to solve: LR or SVM are unlikely to capture the high-level nature of paraphrasing, especially if fed with only basic vocabulary features like this. A slightly more advanced approach would be to provide the model with features which represent the relationship between the two sentences: length, words in common, readability measure, etc."
Understanding how Long Short-Term Memory works in classification of sequences of symbols,"The solution of every problem starts with asking the right question and then look for an appropriate solution to it.

From my understanding of the problem you are trying to solve, it is a Sequence Classification problem, and Yes, RNN-LSTM is one of the approaches you can follow. I am no expert in protein sequencing, so I am just trying to provide a direction to look into.

There are two important factors you will need to consider for applying RNN-LSTM:

Tokenization i.e. how you want to split your protein sequence. One way would be to break the sequence into individual characters which in my opinion will create unnecessary noise for the algorithm to learn patterns. It is more of a domain knowledge based choice which matters here. From your example it seems that you have prior knowledge of subsequences and that should be the way to create tokens and limit the vocabulary for this problem.

Embeddings i.e. the vector representations for the protein sequence after tokenization. If it was English language you could have used pre-trained word embeddings like GloVe, but that is not the case. I would recommend you to search for any existing protein subsequence based embeddings to create vector representations for your use case. If not, then you can go ahead with one of the general frameworks for token vectorization."
(pre-trained) python package for semantic word similarity,"The spaCy Python package might work for you. It allows you to easily ""install"" large pre-trained language models, and it provides a nice high-level interface for comparing word vectors.

To install spaCy:

pip install spacy

Then you need to download a language model. I believe these models are trained on Common Crawl, which is a massive dataset. You should choose one of the medium or large models, because the small models do not ship with word vectors.

python -m spacy download en_core_web_md

Using spacy models to compute word similarity is a breeze:

import spacy

# load the language model
nlp = spacy.load('en_core_web_md')

word1 = 'cat'
word2 = 'dog'

# convert the strings to spaCy Token objects
token1 = nlp(word1)[0]
token2 = nlp(word2)[0]

# compute word similarity
token1.similarity(token2)  # returns 0.80168


Here's an example that's more similar to the one in your question:

import spacy

nlp = spacy.load('en_core_web_md')
token = lambda word: nlp(word)[0]  # shortcut to convert string to spacy.Token
score_words = lambda w1, w2: token(w1).similarity(token(w2))

score_words(""pilot"", ""airplane"")      # 0.5998
score_words(""student"", ""university"")  # 0.7238
score_words(""cat"", ""dog"")             # 0.8017
score_words(""cat"", ""airplane"")        # 0.2654
score_words(""student"", ""apple"")       # 0.0928"
How to evaluate triple extraction in NLP?,"The standard evaluation method works for this kind of task: measure precision, recall and F1-score on a manually annotated sample.

In general one can find which evaluation measure is standard for a particular task in the literature. For example this paper seems to address the topic (I didn't read it)."
How can I classify specific types of words in a document given I have the full text of the document and the labels,"The task you describe corresponds exactly to Named Entity Recognition (NER). This is a standard task for which there are many available libraries. NER is usually done with sequence labelling models such as Conditional Random Fields. Sequence labelling implies that the data is provided as a sequence which looks like this:

During    <features ...>  O
the                       O
process                   O
of                        O
protein                   O
synthesis                 O
,                         O
X_token1                  B_classX
X_token2                  I_classX
X_token3                  I_classX
was                       O
used                      O
.                         O


Here I'm using the common BIO format (Begin, Inside, Outside an entity) but there are variants. The model is trained with data annotated in this way, where there can be additional features (very often POS tag and others). Then when fresh text is provided (with the features) the model predicts the BIO tag for every token.

There has been a lot of research and resources produced for extracting specific entities in the specific context of biomedical data, so you might be interested in exploring these specific resources as well.

Medline, PMC are huge collections of biomedical abstracts/papers
There are many tools for extracting biomedical annotations based on Medline/PMC data: PubTator, cTakes, SciSpacy,etc."
Automatic labelling of text data based on predefined entities,"The task you have is called named-entity recognition. From wiki:

Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.

Since this is a common NLP task there are libraries that are made to do NER out of the box. One such library is spaCy which can do NER as well as many other NLP tasks using Python.

You will not be able to perform NER without first training a model on your custom labels/entities. You need to have some labelled data to train on, maybe you already have this or you can label it manually. SpaCy wants yo have the data labelled with location of each entity on the format:

[(""legal text here"", {""entities"": [(Start index, End index, ""Money""), 
                                   (Start index, End index, ""Judge""), 
                                   (Start index, End index, ""Tribunal""), 
                                   (Start index, End index, ""State"")]}),
(""legal text here"", {""entities"": [(Start index, End index, ""Money""), 
                                  (Start index, End index, ""Judge""), 
                                  (Start index, End index, ""Tribunal""), 
                                  (Start index, End index, ""State"")]})
...]


Example on how to training a spaCy model for NER (taken from docs):

from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding  

# training data
TRAIN_DATA =   Insert you labelled training data here  

@plac.annotations(
    model=(""Model name. Defaults to blank 'en' model."", ""option"", ""m"", str),
    output_dir=(""Optional output directory"", ""option"", ""o"", Path),
    n_iter=(""Number of training iterations"", ""option"", ""n"", int),
)
def main(model=None, output_dir=None, n_iter=100):
    """"""Load the model, set up the pipeline and train the entity recognizer.""""""
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print(""Loaded model '%s'"" % model)
    else:
        nlp = spacy.blank(""en"")  # create blank Language class
        print(""Created blank 'en' model"")

    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if ""ner"" not in nlp.pipe_names:
        ner = nlp.create_pipe(""ner"")
        nlp.add_pipe(ner, last=True)
    # otherwise, get it so we can add labels
    else:
        ner = nlp.get_pipe(""ner"")

    # add labels
    for _, annotations in TRAIN_DATA:
        for ent in annotations.get(""entities""):
            ner.add_label(ent[2])

    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != ""ner""]
    with nlp.disable_pipes(*other_pipes):  # only train NER
        # reset and initialize the weights randomly ‚Äì but only if we're
        # training a new model
        if model is None:
            nlp.begin_training()
        for itn in range(n_iter):
            random.shuffle(TRAIN_DATA)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(
                    texts,  # batch of texts
                    annotations,  # batch of annotations
                    drop=0.5,  # dropout - make it harder to memorise data
                    losses=losses,
                )
            print(""Losses"", losses)

    # test the trained model
    for text, _ in TRAIN_DATA:
        doc = nlp(text)
        print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
        print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])

    # save model to output directory
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)

        # test the saved model
        print(""Loading from"", output_dir)
        nlp2 = spacy.load(output_dir)
        for text, _ in TRAIN_DATA:
            doc = nlp2(text)
            print(""Entities"", [(ent.text, ent.label_) for ent in doc.ents])
            print(""Tokens"", [(t.text, t.ent_type_, t.ent_iob) for t in doc])


if __name__ == ""__main__"":
    plac.call(main)


Then when you have a trained model you can use it to get your entities:

doc = nlp('put legal text to test your model here')

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
What is the advantage of positional encoding over one hot encoding in a transformer model?,"The theoretical advantage should be that the network should be able to grasp the pattern from the encoding and thus generalize better for longer sentences. With one-hot position encoding, you would learn embeddings of earlier positions much more reliably than embeddings of later positions.

On the other hand paper on Convolutional Sequence to Sequence Learning published shortly before the Transformer uses one-hot encoding and learned embeddings for positions and it seems it does not make any harm there."
Tokenisation with Spacy - how to get tokens to the left/right of token,"the token.lefts and token.rights attributes return a generator of the immediate children of the word, in the syntactic dependency parse. It does not just return the tokens on the left and right of the given token.

see : https://spacy.io/api/token#rights

If you want the adjacent tokens for a doc, you can do :

for i in range(len(doc))[1:-1]:
    print(doc[i-1], doc[i+1])


It will print the adjacent tokens for all tokens of the doc, starting at the 2nd token and finishing at the penultimate one."
NLP: find the best preposition for connecting parts of a sentence,"The traditional approach for this kind of problem would be an n-gram language model. The language model is trained on a large corpus, then it's reasonably simple to calculate the most likely missing tokens for any incomplete sentence. SRILM was one of the most common toolkits, but there are probably many other libraries."
Transformer masking during training or inference?,"The trick is that you do not need masking at inference time. The purpose of masking is that you prevent the decoder state from attending to positions that correspond to tokens ""in the future"", i.e., those that will not be known at the inference time, because they will not have been generated yet.

At inference time, it is no longer a problem because there are no tokens from the future, there have not been generated yet."
Human readable format for clusters of word vectors,"The usual way is to present the top N (e.g. top 10) words for the cluster:

With distance-based clustering like K-means, the top words can be picked as the closest ones to the centroid.
With probabilistic methods such as LDA, the top words are the ones with the highest probability for the topic."
Range of values of BERT and other embeddings?,"The values depend on the activation function, but I usually see very small positive values close to 0: this is probably due to the small probabilities between tokens.

You can see the behavior of the activation functions here: https://mlfromscratch.com/activation-functions-explained/#/

Gelu is nowadays the most popular one.

On the other hand, the optimizers are also crucial in the weight values. Stochastic Gradient Descent or Adam are the most popular ones, but AdamW seems to perform even better because it has a progressive learning rate decreasing with learning iterations: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules"
Vector elements of word2vec?,"The vector size is the number of dimensions in the embedding space. Each word in the vocabulary is represented by a vector. The vector size is the same for each word. The values in the vector are different for each word.

In your example, 100 is the vector size. The number of words is far larger, typically thousands or millions."
What are the merges and vocab files used for in BERT-based models?,"The vocab file contains a mapping from vocabulary strings and indices used for embedding lookup in the model.

The merges say how to split the input string into subword units. The algorithm is as follows: At the beginning of merging, a word split into characters and then you greedily search for neighboring symbols that can be merged (i.e., are in a list of allowed merges), you stop when there is nothing to merge and this is the subword segmentation."
How to grow a list of related words based on initial keywords?,"The word2vec algorithm may be a good way to retrieve more elements for a list of similar words. It is an unsupervised ""deep learning"" algorithm that has previously been demonstrated with Wikipedia-based training data (helper scripts are provided on the Google code page).

There are currently C and Python implementations. This tutorial by Radim ≈òeh≈Ø≈ôek, the author of the Gensim topic modelling library, is an excellent place to start.

The ""single topic"" demonstration on the tutorial is a good example of retreiving similar words to a single term (try searching on 'red' or 'yellow'). It should be possible to extend this technique to find the words that have the greatest overall similarity to a set of input words."
N-grams in NLP deep learning,"Theoretically, you could use n-grams to model text sequences. But there are some good reasons why it's not often mentioned in textbooks.

Intuitively, modeling sequences is learning that after a, b, and c comes d. So if your sequence is:

""Hello, my name is Bob, what a lovely day today, how are you?""

It technically doesn't matter if a = ""Hello"", or ""H"", or ""Hello, my name is Bob"". To your model, it will just learn that after a thing, comes another thing etc.

So yes, you could use n-grams. But the problem is that you need to make sure that the way the ngrams are split is meaningful (i.e. ""Hello, my"" is a probably worse split than ""my name is"", which probably happens more often)

But don't worry, the whole point of using LSTM is that the memory part of the network will mimic using n-grams, as long as it helps with the overall performance. This means that if it needs to take more words into account to predict the next one, it will do it up to the depth of your network."
Fewer observations & larger documents vs More observations & smaller documents,"There 800 separate documents of any of the 3 labels or 3 big documents of each of the labels is the best way to go and why?

The first thing you need to think about in any ML problem is: what is an instance for the problem? In other words, what is going to be the input for which you want a prediction at the end of the process?

Imagine you train your model with 3 big documents, one of each label. Then the input for such a model is a big set of documents with the same label. So it can only predict a label for a set of documents sharing the same label. This means that somehow you need to have the labels before applying your model... difficult isn't it? :)

This is why in this case an instance must be a single document. It's the job of the learning algorithm to learn to discover the label based on the instances, and for that it needs many instances of each possible label.

On the other hand, if I do (as we usually do actually) like in the former case then the TF-IDF will be categories/labels-agnostic and I do not know this helps things.

This is where there is a confusion: the TF-IDF weights are not supposed to encode the label in any way, they represent the importance of a particular word in a document. The learning algorithm will use this information for all the words, that is it's going to learn the difference between when the word delicious has a high TF-IDF and when the word disgusting has a high TF-IDF (for instance).

Is the answer simply that this an interesting but pretty bad idea because in this way you simply massively decrease the number of the observations with which the model/algorithm is trained and so you make much harder for him to figure out how to successfully classify things?

That would be true as well, but the main issue is the one I mentioned above: you won't be able to provide the same kind of input when you apply your model on your unlabeled data."
Could one algorithm fetch keywords from texts of different natural languages?,"There are a few ways to deal with this issue. Python has a package called NLTK which contains stop word lists for several languages (including English and Norwegian). You can simply use this package, it's usage is as follows:

>>> from nltk.corpus import stopwords
>>> stop = stopwords.words('english')
>>> sentence = ""this is a foo bar sentence""
>>> print [i for i in sentence.split() if i not in stop]
  ['foo', 'bar', 'sentence']


Alternatively, a method for automatically suppressing stop words is called tf-idf; tf-idf is commonly used in search engines so that the most important words are promoted to the forefront. In your case, I would suspect you'd want to have IDF scores for both English and Norwegian and apply only the appropriate one on a language to language basis.

http://en.wikipedia.org/wiki/Tf%E2%80%93idf"
Using several documents with word2vec,"There are a number of implementations of Word2Vec, but most assume the basic unit to be 'sentences' - though they don't care what those sentences look like. If you are using something like gensim you will need each sentence in its own list, and each sentence will be a list of tokens. If you are using another package you may be reading from the disk in which case you likely need all documents concatenated with each newline representing a new sentence.

The fundamental consideration in deciding on the size of a 'sentence' (whether it's the entire note, all a patient's notes, or a single sentence) is what surrounding words should be used to train the model. The W2V model will consider all words within a certain distance of the target word when tuning its vector representation (regardless of if it's cbow or skipgram), but will not look beyond a 'sentence' boundary. So if your fundamental unit is a document not a sentence, associations will bleed across sentences (this may or may not be what you want).

As an aside, consider the nature of the documents first. Patient notes can be messy and full of automatically generated text, so it can be important to strip out or replace certain strings. Similarly, they often use whitespace and newlines rather than punctuation. You might want to consider treating existing whitespace as the end of a sentence and you might want to use a parser to replace dates, names, etc with default tokens to increase generalizability."
Extract key phrases for binary outcome,"There are a variety of techniques that you could use, depending on what you would like to do.

If your goal is to gain insight into the phrases that are being used in each group, then I'd recommend looking for the most frequent N-grams of different lengths that appear in each class. Here is a related stackoverflow question that shows how you can use nltk and sklearn to extract these.

If your goal is to predict the outcome (accept/reject) given a phrase, then I'd recommend setting this up as a binary classification problem. Since those phrases are quite short, you could start with a Bag of Words approach - the scikit-learn documentation for working with text data is a good example that guides you through the steps."
How to train a machine learning model for named entity recognition,"There are actually many libraries for training NER models.

It's useful to know that this type of model/task is called sequence labeling because it consists in predicting a label for every word, taking into account the other words close to the target word.
The standard method is Conditional Random Fields (CRF). There are various libraries, see for example this answer.
Traditionally a specific format called BIO (sometimes IOB) which stands for Begin, Inside, Outside is used as input (see a very short example). The features can involve context words through custom patterns (see the documentation of the libraries for details)."
Learning to Rank with Unlabelled Dataset,"There are different ways to look at this:

You can apply a totally unsupervised method, like computing a TDIDF vector for the query and then ranking according to its similarity (e.g. cosine) against every document. This requires no training at all, but you can't even evaluate the method.
You can use an already implemented system like ElasticSearch.
You can train a supervised ranking model with any number of samples, but obviously it's going to work much better with a large number. The first difficulty is to generate a sample of queries which is as representative as possible. The second difficulty is to find a way to select the top document(s) for every query: if done manually, the annotator needs to read 60k documents (ouch!). I'm not even going to talk about taking into account the subjectivity and potential ambiguity of a query.
You could try to do some form of semi-supervised learning or active learning. You could progressively refine the model by using user feedback for instance, if this works for your use case."
Are there any open-source text annotation for multi label classification tools?,"There are many manual text annotation tools available, but you will probably have to search around in order to find the one which suits your precise needs.

Here are a few pointers:

Gate
Text annotation tools
Doccano
a recent review"
"Classification of ""good"" and ""bad"" sentences","There are many methods . But for starters you can try the below method. ref SO post here

import re
if re.search('[^a-zA-Z\s:]', ""Watch our latest webinar about flu vaccine""):
     print(""bad"") 
else :
    print(""good"")



Later you can try sentiment analysis for positive and negative words.

from textblob import TextBlob
TextBlob(""Watch our latest webinar about flu vaccine"").sentiment


From here you can look at removing stop words and other techniques to clean up your word corpus."
Training pipelines where featurization/NLP is more expensive than backprop,"There are many options to spend this up:

Get a better CPU.
Distribute the process across a cluster since each document is independent.
Reduce the size of the vocabulary. If only the top-n most popular words are used, it greats reduces the size of the data.
Reduce the size of the embedding space.
Switch to doc2vec so the document themselves are a learned embedding."
Using word embeddings as features in classification algorithms?,"There are many options. Here are a couple of suggestions:

Learn a weighted average of the vectors based on the classifier task. ""Task-Optimized Word Embeddings for Text Classification Representations"" by Grupta et al. goes into detail.

Input each word embedding separately into the classifier.

Train a new embedding model that embeds the entire document (e.g., doc2vec)."
Is there a way to train Doc2Vec on a corpus of docs and be able to take a novel doc and see how similar it is to the trained corpus?,"There are many possible approaches:

using simple similarity measures (e.g. cosine) to compare the new document against every training document.
train a binary classifier to distinguish the reference documents from ""anything else"". Requires negative examples, it's usually difficult to have a representative sample.
Use one-class classification, i.e. train a model using only the reference documents. The model tries to represent this class of documents and considers anything else as negative.
Could even consider it as a regression problem, i.e. score documents by how similar they are from the reference documents."
NLP : Rules for chunking Verb Phrases,"There are many ways to perform chunking/phrase structure parsing. The most common methods are:

Rule-based
Statistical models
Deep Learning

Fixed rules are simple and fast. But they make many mistakes given the complexity of human language. The other methods are typically more useful because they generate probabilistic rules.

Context-free grammar (CFG) is a set of rules for the production of formal language and can be used for parsing."
Text Similarities: which nlp methods to use?,"There are many ways to see how texts are similar, but this will depend on your use case.

Semantics

Nowadays, word embeddings are getting popular. Like it was suggested in the comments, you could use Doc2Vec to transform your sentence into a vector and calculate the cosine distance from each sentence.

The idea with these learned embeddings is that you kind of encode the meaning of the sentence by looking how words are used in context. So, to sentences would be similar if words are used in the same manner in a context.

However, this could potentially be difficult to interpret.

For example:

Good tattoo shop. Clean space.
Good pizza restaurant. Large space.
Terrible tattoo shop. Dirty space.


Which of these are semantically close? It will depend a lot on your training and your own judgment to tell if the results are useful to you.

Sentiment

Maybe you are interested in saying that sentences are similar if their sentiment is positive or negative (or on some other scale).

For example:

Suppose you can have sentiment on a scale 
[‚àí1,1]
[
‚àí
1
,
1
]
, where 
‚àí1
‚àí
1
 is negative, 
0
0
 is neutral and 
1
1
 is positive.

Good tattoo shop. Clean space.
    Sentiment = 0.7

Good pizza restaurant. Large space.
    Sentiment = 0.75

Terrible tattoo shop. Dirty space.
    Sentiment = -0.9


There are many resources online on how to do this. Here is an answer with some links.

BoW

The bag-of-words is a very simple approach, but depending on what you are doing, it could do the trick. Suppose you want to say that sentences are similar if they are about a business in the same industry.

You can make a simple dictionary for every industry:

word_per_industry = {
    'restaurants' = ['restaurant', 'food', 'chef', 'dish', 'salad', 'lunch'],
    'tatto_shop' = ['clean', 'dirty', 'art', 'sterlized']
    .
    .
    .
}


Then, per sentences, you can count which business has more words in the sentence, and if sentences are from the same industry they are similar. Of course, you could also make lists based on another characteristic, not necessarily on industries."
Selecting most relevant word from lists of candidate words,"There are many ways you could approach this problem

Word embeddings

If you have word embeddings at hand, you can look at the distance between the tags and the bucket and pick the one with the smallest distance.

Frequentist approach

You could simply look at the frequency of a bucket/tag pair and choose this. Likely not the best model, but might already go a long way.

Recommender system

Given a bucket, your goal is to recommend the best tag. You can use collaborative filtering or neural approaches to train a recommender. I feel this could work well especially if the data is sparse (i.e. lots of different tags, lots of buckets).

The caveat I would see with this approach is that you would technically always compare all tags, which only works if tag A is always better than tag B regardless of which tags are proposed to the user.

Ranking problem

You could look at it as a ranking problem, I recommend reading this blog to have a better idea of how you can train such model.

Classification problem

This becomes a classification problem if you turn your problem into the following: given a bucket, and two tags (A & B), return 0 if tag A is preferred, 1 if tag B is preferred. You can create your training data as every combination of two tags from your data, times 2 (swap A and B).

The caveat is that given N tags, you might need to do a round-robin or tournament approach to know which tag is the winner, due to the pairwise nature.

Recurrent/Convolutional network

If you want to implicitly deal with the variable-length nature of the problem, you could pass your tags as a sequence. Since your tags have no particular order, this creates a different input for each permutation of the tags. During training, this provides more data points, and during inference, this could be used to create an ensemble (i.e. predict a tag for each permutation and do majority voting).

If you believe that it matters in which order the tags are presented to the user, then deal with the sequence in the order it is in your data.

Your LSTM/CNN would essentially learn to output a single score for each item, such that the item with the highest score is the desired one."
Are there any tools to make text labeling faster?,"There are many. A couple of the most popular:

ML-Annotate - Supports binary, multi-label and multi-class labeling.
TagEditor - A Windows application that uses spaCy"
Classification of Conversations in Text,"There are multiple techniques that help with the problem you sketch, the applicability of which usually depends on the classification technique and the corpus. But I get the idea you would be helped by some practical examples. So let's go over some of them. Feel free to comment if I tread over familiar grounds, or you want me to elaborate on some of them. Or where to start experimenting with them

Stopping A simple technique is applying a stoplist: A list of common words that should be removed. There are pre-packaged lists, but most packages allow you to provide your own.
TF/IDF A technique that transforms your features by weighing words by their term frequency (how often do they occur in the document) divided by the document frequency (how often does the word occur in other documents. This way frequent words are made less relevant to the document
POS Many packages offer you a part-of-sentence-tagger, that will tag the words by their grammatical function (Verb for instance). You can leverage that in the tokenization step to filter out words (usually you'll look for verbs and nouns). Some vectorizers can do this straight out. (This could also be done with NER)
Stemming transforms inflections of words (eg: train/trains) to a stem. This might make some of your words a little more relevant by upping the chance a pair of them collides
Restricting your vectorizer: Most packages sport a vectorizer that you can instruct to either look for a minimum/maximum document count (ignore words that either occur in to many different documents, or to little different documents), or to cap the amount of features (words). Capping the amount of words usually selects for most frequently used words.
Encoding word/token based features to more semantic features: Word2Vec, but also older techniques such as LDA/LSI.
Picking the classification algorithm: Some algorithms are very capable of handeling large feature spaces (Naive Bayes for instance), some algorithms learn to transform the feature space to find better ways to weigh the features.

Packages such as Sklearn, NLTK and Gensim offer most of these techniques.

Let me know if this was helpful"
Existing pre-trained NLP models to detect if a text input is a question,"There are multiple threads on Stack Overflow talking about this. Couple of examples can be found below:

Determine if a sentence is an inquiry

NLTK. Detecting whether a sentence is Interogative or Not?

You can also take a look at Natty. It's a bot on Stack Overflow, which reviews answers to old questions. You can see how effective it is in terms of detecting questions (every bot needs training, and Natty has been trained for quite a while). However it is written in Java, it may give you some ideas. Like detecting keywords alongside checking for presence of question mark."
Which NLP library has the most mature Chinese language models? [closed],"There are no common libraries that support high quality named entity recognition for Chinese.

Other options include Information-Extraction-Chinese on GitHub or adapting a paper with code."
Good-Turing Smoothing Intuition,"There are no unseen item types in the given data, by definition. 3 is the count of items seen once, and they are already included in the denominator 18. If the next item were previously unseen, it would become seen once when it appears. Since 3-of-18 examples were seen-once items, this is an estimate of the probability that the next item will be seen-once too on its first appearance.

It is certainly a heuristic. There is no way to know whether there are 0 or 1000 other types out there."
What machine learning algorithms to use for unsupervised POS tagging?,"There are no unsupervised methods to train a POS-Tagger that have similar performance to human annotations or supervised methods.

The current state-of-the-art supervised methods for training POS-Tagger are Long short-term memory (LSTM) neural networks."
Natural Language Processing (NLP) in .net environment,There are plenty of NLP libraries for .Net if you Google that. Please be more specific as to what you want to accomplish when they query the DB.
How to classify a set of words into one of the given labels,"There are probably many variants but here are two simple approches:

Using pretrained word embeddings, you can calculate the semantic similarity between two words. For example you could use cosine to measure the similarity between the vector of a target word (e.g. ""calm"") and every word in the set (e.g. ""cloud""). Then the mean across words in the set gives how much the set is associated to the target, and you can pick the target which has the max similarity.
Using WordNet to directly obtain a semantic distance/similarity between words. The method is similar to the one above.

Note that there are many improvements that can be done to these basic ideas, for example you could use a predefined set of words related to ""calm"" instead of just the word ""calm"" (you can get the most similar words from WordNet for instance). There also many options for the aggregation across the set of words."
Good NLP model for computationally cheap predictions that can reasonably approximate language model given large training data set,"There are several smaller BERT models, including bert-tiny. Bert-tiny is a distillation of the full BERT model."
how to get the Polysemes of a word in wordnet or any other api?,"There are several third-party Java APIs for WordNet listed here: http://wordnet.princeton.edu/wordnet/related-projects/#Java

In the past, I've used JWNL the most: http://sourceforge.net/projects/jwordnet/

The documentation for JWNL isn't great, but it should provide the functionality you need."
How to create a training set for sequence labelling,"There are several tools for this you could check the Stanford Simple Annotation Tool, also Brat has online demos. For a comprehensive list you could check this question on Quora"
Package/function to convert the word to one hot vector in Python NLP,"There are several ways to convert words to one hot encoded vectors. Since I do not know the data structure you store your data. I assume it is going to be a list

from keras.preprocessing.text import Tokenizer
samples = ['The', 'dog','mouse','elephant']
tokenizer = Tokenizer(num_words=len(samples))


This builds the word index

tokenizer.fit_on_texts(samples)


one hot representation

one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')


By changing the mode from binary to 'tfidf' or 'count', you can make a matrix of any type, apart from one hot.

You can achieve the same result using other packages like sklearn. But it does involve a bit more lines of code."
Comparing text corpora sentence-wise in Python,"There are several ways to do this, but, assuming the lists are the same length and the sentences you wish to compare are in the same indices int their respective lists, you can handle this with a simple list comprehension:

diffs = [(s1[i], s2[i]) for i in range(len(s1)) if s1[i] != s2[i]]"
Use embeddings to find similarity between documents,"There are several ways you can obtain document embeddings. If you want to obtain a vector of a document that is not part of the trained doc2vec model, gensim provides a method called infer_vector which allows to you map embeddings.

You can also use bert-as-service to generate sentence level embeddings. I would recommend using Google's Universal Sentence Encoder (USE) model to generate sentence embeddings if your goal is to find some sort of similarity between sentences or documents. There are ways you can combine sentence level embeddings to document level, the first step to try would be to take the mean, or you could generate sentence embeddings for a sliding window over the document and take the mean of that.

The reason I recommend USE over BERT is that, USE was trained specially for sentence similarity tasks whereas BERT, even though can be applied to any NLP task was original trained to predict words in a sentence or complete a sentence. You might find this link helpful, it draws a great comparison between USE and BERT, and why it is important to choose a model based on task."
what is difference between set() and word_tokenize()?,"There are two differences between word_tokenize and set.

Word_tokenize

Returns a list (try print(type(word_tok)))
Returns all the tokens, regardless if there are duplicates

Set

Returns a set (try print(type(set_all)))
Returns all the unique tokens

Try this instead

sentence = 'jainmiah jainmiah jainmiah I love you but you are not bothering about my request, please yaar consider me for the sake'

word_tok = word_tokenize(sentence)
print(word_tok)

set_all = set(word_tokenize(sentence))
print(set_all)"
How do you handle the free-text fields in tabular data in ML/DL?,"There are various text representation techniques, ranging from bag-of-words methods like TFIDF to embeddings. These techniques are used to build a fixed-length representation of any input text.

If the text is the only input in an instance, as in text classification for example, then this representation is directly usable as the vector of features values.
If there are other features or several distinct texts which must be represented separately, then the vector and the other features can be concatenated.

In general one should be careful to prevent the resulting number of features to become too high. For example, it's very common to ignore the least frequent words in the case of a bag-of-words representation."
"Incorrect Text Classification, But Accurate Model. Do I Perform Manual Text Classification For A Data Set?","There can be two distinct reasons to use instances annotated with the gold-standard class, i.e. the true answer for the target application:

In order to perform proper evaluation your test set must contain the gold-standard labels. The principle of evaluation is to measure by how much the predictions deviate from the truth, but without the truth the performance that you obtain on the test set is meaningless for the task that you are doing.
In order to train a supervised or semi-supervised model, the training set must contain the gold-standard labels. Semi-supervised methods offer some options to adapt a training set to a different task.

You can't rely on a model if you can't evaluate it at least on a small sample, so yes you probably need to manually annotate a subset of the data. It's only after that you can start thinking about how to improve performance."
Will repeatedly fine-tuning on new data cause overfitting?,"There could be an overfitting issue indeed. Models shouldn't constantly increase their accuracy with additional trainings, and it could result as a bad generalization, i.e. a failure in classifying new combinations.

I don't know if your model uses neural networks, but if it is so, there are several functions to forget/reset a small part of the trained neurons, so that it avoids overfitting in new trainings. The Dropout function is the most common one, but you could have similar functions in error calculation functions like the Adam Optimization.

If there is no such functions in your model, a solution could to restart from scratch the training including the new data. If the result is similar to model A, it could confirm that model B and C were overfitting.

Finally, overfitting results highly depends on data, Model C could also be correct without overfitting, more details would be necessary to check this out. But generally speaking if you are around 90% or more, you are in an overfitting scenario."
Why are words represented by frequency counts before embedding?,"There is a general principle in linguistics and consequently in NLP: the meaning of a word is represented by the context of the word, i.e. the words around it. [edit] In NLP this principle is the basis of distributional semantics, which is used in every NLP application involving semantics (almost of them).

This means that statistically the meaning of a word can be represented by a distribution of frequencies/probabilities over the vocabulary of all its possible context words. This principle is generalized to a full text: the meaning of the text is represented as the frequency distribution of the words it contains.

Thus it's very meaningful to use the word frequency: it represents the ""importance"" of the word in the text and taking together the ""importance"" of all the words gives a representation of the meaning of the text."
"Some answers given by ChatGPT are just beyond ridiculous, what could be the reasons?","There is a major misunderstanding in the general public and especially in the news about what ChatGPT really does. This misunderstanding is not really new, there has been confusion about ML techniques for a long time.

Imho a large part of the issue comes from the term ""Artificial Intelligence"", which covers everything in this area supposed to represent some form of intelligence. Clearly most people would like to believe that there is some real intelligence at play (sci-fi, etc.). The fact that many companies or institutions have an interest in keeping the ambiguity alive (e.g. ""smart cars"", ""smart home"", etc.) and that some experts support it greatly contributes to the confusion.

But as far as I know, from the point of view of ML experts, a system like ChatGPT is more like a highly complex calculator than an intelligence. Its calculations cannot be simulated manually (well technically they can, but it would take a lot of people and a very long time) so it gives the impression of something smart. It's also worth noting that the evolution of ML has been to give less and less direct human supervision to the models, i.e. they have more autonomy in which way and what they learn.

What this kind of system is trained to do is to simulate giving an answer. The more the answer looks real, the better. It is not trained to give a correct answer or give any importance to the truth, because this is not easily measurable. The Chinese Room argument explains why such a system does not comprehend anything of what it says, it just applies a lot of complex rules/calculations based on its training. One could say that the goal of a system like ChatGPT is to create the illusion that there is some intelligence behind it, like the goal of a magician is to make you believe whatever they are showing.

As a consequence, it is well known that ChatGPT answers are meaningless in general: see for instance this article or the ban on StackExchange."
"How to recognize a two part term when the space is removed? (""bigdata"" and ""big data"")","There is a nice implementation of this in gensim: http://radimrehurek.com/gensim/models/phrases.html

Basically, it uses a data-driven approach to detect phrases, ie. common collocations. So if you feed the Phrase class a bunch of sentences, and the phrase ""big data"" comes up a lot, then the class will learn to combine ""big data"" into a single token ""big_data"". There is a more complete tutorial-style blog post about it here: http://www.markhneedham.com/blog/2015/02/12/pythongensim-creating-bigrams-over-how-i-met-your-mother-transcripts/"
Reducing the dimensionality of word embeddings,"There is a paper on this subject calle

Simple and Effective Dimensionality Reduction for Word Embeddings, Vikas Raunak

You can read it here

You can also find the implementation here

In my opinion it works quite well"
Resolving time in NLP,"There is a Python library called dateparser that will accept a wide variety of formats, including relative dates like ""next Tuesday"", and return exact datetime representation."
Machine learning - Algorithm suggestion for my problem using NLP,"There is a really good video about this topic from a PyCon in 2016. There is a pretty in-depth description on how to vectorize your sentences as well as make predictions based on those vectors.

I think that would greatly help you out. That is what I used when I was learning about how to perform sentiment analysis."
Why not rule-based semantic role labelling?,"There is a subtle but important difference between ""semantic role"" and ""grammatical role"" (I think there's a specific term for the latter but I forgot it).

Grammatical role is strictly about syntax. For example in the sentence ""John sent a letter to Mary"":

""John"" is subject
""a letter"" is object
""Mary"" is an indirect object

This is what a syntactic parser (typically dependency parser) would normally identify.

By contrast semantic role is mostly about the semantics. We could describe the semantic of the above sentence like this:

The predicate is ""to send""
This predicate can have 3 arguments:
the sender is ""John""
the object is ""a letter""
the receiver is ""Mary"".

Typically one needs a specific resource like PropBank in order to know what are the expected and optional arguments specific to a predicate.

So far the difference might look thin, but it will become clearer with this new example: ""A letter was sent to Mary by John"". The grammatical roles become:

""a letter"" is subject

""Mary"" is indirect object

""by John"" is some kind of dependent clause (I forgot what this is called but it's definitely not subject).

Whereas the semantic roles are exactly the same as previously, because the semantic didn't change.

So in general semantic role labelling is harder than simple identification of syntactic roles. This is why specific resources and methods are developed for it.

Disclaimer: my knowledge on the topic is from 10 years ago, I'm not aware of the recent developments in this domain. I'm under the impression that DL methods bypass this step for many applications by producing end to-end systems, but I'm not sure at all about this."
Why did Logistic regression perform better than svm? [closed],"There is a theorem in machine learning literature which is called ‚ÄúNo free lunch theorem‚Äù. The essence of NFL is that there is no universal model which performs best for every problem and every dataset. So, according to NFL, you can not expect SVM to outperferm logistic regression in all situations and contexts. If your classes were linearly separable SVM would be perfect with 100% accuracy but otherwise you shouldn‚Äôt expect it to necessarily outperform ligistic regression. So, whether SVM or ligistic regression are better choice highly depends on the problem and on the available dataset."
Choosing the size of Character Embedding for Language Generation models,"There is a theoretical lower bound for embedding dimension

I would urge you to read this paper, but the gist of it is dimension could be chosen based on corpus statistics

GLOVE paper discussed embedding, check page 7 for graphs. What I want to say with this reference is that you can treat it as hyperparameter and find your optimal value.

EDIT: Here is my personal/borrowed from google rule of thumb. Embedding vector dimension should be the 4th root of the number of categories is start with that, and then I play around it. Read this toward the end when they explain their embedding. Why COULD (it must not) make sence: What is BOW rather than one hot encoding of your n-grams.

Does it make sence to make it larger? it depends. On one hand you are right if we make it too big we loose the distributed representation property of the word embedding matrix, on the other hand it works in praxis."
From where does BERT get the tokens it predicts?,"There is a token vocabulary, that is, the set of all possible tokens that can be handled by BERT. You can find the vocabulary used by one of the variants of BERT (BERT-base-uncased) here.

You can see that it contains one token per line, with a total of 30522 tokens. The softmax is computed over them.

The token granularity in the BERT vocabulary is subwords. This means that each token does not represent a complete word, but just a piece of word. Before feeding text as input to BERT, it is needed to segment it into subwords according to the subword vocabulary mentioned before. Having a subword vocabulary instead of a word-level vocabulary is what makes it possible for BERT (and any other text generation subword model) to only need a ""small"" vocabulary to be able to represent any string (within the character set seen in the training data)."
Open source NLP annotation tool/library supports active learning,"There is a tool Acharya which does this, available here (https://github.com/astutic/Acharya).

You can upload your dataset and then add any algo to train a custom model and use it for further annotations.

Their documentation regarding adding an algorithm is a bit confusing but adding it is pretty straight forward, to begin with you can simply copy paste a configuration template which they provide into the Add algo section, this config template which trains a spacy ner model and is available here:

https://github.com/astutic/acharya-spacy/blob/setup/algo_configuration.yaml"
"Extracting name, date and total from a set of heterogeneous receipts","There is already an ML engine that does these extractions, here is a general process layout: 

Hre is the orginal paper describing the architecture, features, approaches etc. read it up, instead of me copying it here. cloudscan"
Which libraries in Python are there in NLP to tokenize the Hindi sentence?,"There is an Indic NLP library developed by iitB for Hindi texts. you can check out the below links

https://www.cse.iitb.ac.in/~anoopk/pages/softwares.html

https://github.com/anoopkunchukuttan/indic_nlp_library"
Stemmer or dictionary?,"There is another reason: words which don't appear in the dictionary. Of course a dictionary approach will correctly stem all the forms which are known in the dictionary, and depending on the language this may indeed lead to better accuracy. However the dictionary approach cannot do anything about unknown words, whereas a generic stemmer can try to apply its generic rules. This can be particularly important with texts which are either very domain-specific (e.g. medicine), which often contain technical words which are not in a general dictionary, or recent user-generated texts such as social media posts where people may use neologisms or words borrowed and sometimes transformed from another language."
How to evaluate the quality of speech-to-text data without access to the true labels?,"There is at least one way:

Create/Acquire a grammar model for the language spoken (there are several such models for various languages used in NLP)
Test the transcripts for beign grammaticaly/syntacticaly correct.
This assesment will at least rule out gibberish and most of transcripts that do not correspond to valid sentences of the language spoken"
Stemmer/lemmatizer for Polish language,"There is bunch of lemmatization solutions for polish language. One of the best implementation is in polish morphosyntactic analyser, which you can download here.

It has bindings to python, but you have to install them manually. It is ""morphosyntactic analyser"" which means, that you get all possible lemmas for a given word. If you want also disambiguation, you can use this tool which is provided as docker container.

My team is currently working on implementing polish language support for Spacy, and we will cover lemmatization, so tool with good integration and support will be available soon."
How to choose solution - Neural Neworks or Scikit-Learn/Numpy/Pandas?,"There is no magic solution. You will have to try. The rule of thumb I would say is to try firstly classifiers which are less costly to train and deploy (i.e. Scikit-learn) then give it a try with NNs.

Anyways, for your problem, you may be interested to try the Genism library for topic modelling to extract what kind of topics are mentioned in an email.

Then spaCy is a great tool for NLP production tasks, there should be some text classification walkthroughs using it.

These are only a couple of ideas I am sure there are tons more. I believe the key thing is to understand which key indicators, features, words are responsible for an email to be labelled of a particular class."
How to train neural word embeddings?,"There is one answer to know: try both methods and take the one that gives the best result. I would say that in general pre-trained embeddings usually gives better results. You can also start with pre-treained embeddings as initial conditions and let the embeddings train maybe with a smaller learning rate.

In any case, the current state of the art for text classification is ULMFIT (https://arxiv.org/abs/1801.06146), which actually doesn't do any of this. It pre-trains embedding and RNN with a language model in the wikipedia and in the target text and then fine tunes the whole model with the target text."
Word2Vec - CBOW and Skip-Grams,"There is only one neural network to train in word2vec. CBOW, continuous-bag-of-words', and skip-gram are two different methods of constructing the training data for the neural network. You have to pick one of those two training methods."
Determining number of clusters in high dimensions,"There isn't a correct way to approach this problem. One common way is what you are doing, i.e. check for various values of 
k
ùëò
 and have a heuristic tell me the best value. Some such methods are are the elbow, silhouette and the gap statistic that you're using. Determining the number of clusters via such a method is perfectly valid; in fact that's what they're for.

Another approach would be to try hierarchical clustering on the data and see which level leads to the maximum difference in variance."
Why categorical cross entropy loss is not correlated with NLP scores?,"There may be no mathematical linkage between categorical cross entropy and BLEU. BLEU if i remember correctly is probably a distance based measure of computer output to human judgement - there's no loss variable anywhere. This is unlike the perplexity score which IS derivable from the categorical cross entropy.

Loss function really depends on what you're comparing against your labels. Categorical cross entropy gives a way to compare the distributions of the prediction and truth labels and calculate an error based on the 'distance' between the distributions. Categorical cross entropy really works well to model the distributions of discrete vector labels, common in NLP."
group the similar words,"There will not be any pre-trained models to cluster these words.

In fact, in order to build your own clustering model you will need more metadata about each observation/word in your array.

At the moment any model would only be able to ""see"" the name of the package/software in your array. So the best you could hope for is a model that clusters these words based on their spellings.

Now let's pretend you find a brief description of each software, now you could do a bit more. With this longer text you could use supervised or unsupervised methods to cluster these softwares into groups (see topic models, k-means etc) based upon similar words in the descriptions.

Long story short, there is not a pre-built model to do this, and to build one yourself you're going to need more information about each observation."
What are some standard ways of computing the distance between documents?,"There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)

Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.

Cosine Distance - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.
Levenshtein Distance - Also known as edit distance, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing
LSA - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called topic modeling. LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations
LDA - Is also a technique used for topic modeling, but it's different from LSA in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from LDA are better for modeling document similarity than LSA, but not quite as good for learning how to discriminate strongly between topics.
Pachinko Allocation - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of LDA, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by
word2vec - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as Count Vectorizers and TF-IDF. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.
doc2vec - Also known as paragraph vectors, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The gensim library in python has an implementation of word2vec that is straightforward enough that it can pretty reasonably be leveraged to build doc2vec, but make sure to keep the license in mind if you want to go down this route

Hope that helps, let me know if you've got any questions."
Calculate cosine similarity in Apache Spark,"There's a related example to your problem in the Spark repo here. The strategy is to represent the documents as a RowMatrix and then use its columnSimilarities() method. That will get you a matrix of all the cosine similarities. Extract the row which corresponds to your query document and sort. That will give the indices of the most-similar documents.

Depending on your application, all of this work can be done pre-query."
How to find appropliate algorithm to bulid a model for natural language based two data [closed],"There's no algorithm intended specifically for this task, you need to design the process yourself (like for most tasks btw).

Given that the goal would be to use a person's name as an indication, I'd suggest you represent a name as a vector of characters n-grams in the features.

Example with bigrams (
n=2
ùëõ
=
2
):

""Braund"" = [ #B, Br, ra, au, un, nd, d# ]


Intuitively the goal is for the model to find the sequences of letters which are more specific to a nationality. You could try with unigrams, bigrams or trigrams (the higher 
n
ùëõ
, the more data you need for training).

Once the names are represented as features this way, you can train any type of supervised model, for example Decision Tree or Naive Bayes."
What are some best Text Representation techniques in NLP,"There's no simple answer to this question. As far as I know in general the choice depends mostly on the type of classification:

Bag of Words (usually with tf-idf weights) is a simple but quite efficient representation for classification based on the text topic or similar, assuming the classes are reasonably distinct from each other.
Word embeddings are a more advanced option for semantic-based classification. They can handle more subtle semantic relations but require being trained on a large training corpus. Using pre-defined embeddings can be a solution but then there's the risk that the original training data isn't perfectly suitable for the dataset.
N-grams models can be used in many different ways but are often chosen when the classification involves syntax and/or writing style. Note that the higher the value 
n
ùëõ
, the larger the training corpus needs to be, this can also be taken into account in the choice.

I might have around 40 categories and then around a same number of sub-categories upto 4 levels.

It depends on the data but 40 classes is already a very challenging classification task. For the sake of simplicity let's assume a uniform distribution over classes: a random baseline accuracy would be 1/40 = 2.5%. Of course it depends on the data and a good classifier will do better than that, but don't expect too much...

Now 4 levels of 40 sub-categories means 40^4 = 2.5 millions classes! Even assuming you have enough data (say around 10 instances by class in average, that is 25 millions instances!), it's very unlikely that a classifier will be able to predict anything useful from such a huge amount of classes."
My custom stop-words list using tf-idf,"There's no standard definition of stop-word, but in general stop words are very frequent words which don't contribute to the meaning of the text, like determiners, pronouns, etc. Importantly stop-word is a property which applies to unique words in the vocabulary. For example if the word 
w
ùë§
 is considered as a stop-word then this applies to all the occurrences of 
w
ùë§
 in the text, not only to some of them.
On the contrary TFIDF applies to the words in the sentences/documents, so the same word 
w
ùë§
 may have a different TFIDF value in different sentences/documents:
IDF is a property at the vocabulary level, i.e. all the occurrences of 
w
ùë§
 have the same IDF.
TF is specific to the sentence/document. If 
w
ùë§
 appears 3 times more often in document A than in document B, then it has 3 times higher TFIDF value in A than in B.

This is why it doesn't really make sense to consider the TFIDF value to select stop-words: the former is specific to a sentence/document but not the second. You could use the IDF part only, but there's no difference with just using the document frequency, and practically it would give the same results as using the overall frequency."
what actually word embedding dimensions values represent?,"These columns are actually arbitrary, they do not represent anything for humans. However, it does not mean they are useless, quite the opposite - computers can extract features from this highly dimensional space more easily, such as in neural networks.

To extract information which is useful for humans, I recommend to have a look at examples in original paper by Tomas Mikolov:

We can have a look at common capital city. Athens is to Greece (in sense that they have similar relationship) as is Oslo to Norway. We can use afterwards vectors obtained using GloVe and construct

v(Athens)‚àív(Greece)+v(Norway)
ùë£
(
ùê¥
ùë°
‚Ñé
ùëí
ùëõ
ùë†
)
‚àí
ùë£
(
ùê∫
ùëü
ùëí
ùëí
ùëê
ùëí
)
+
ùë£
(
ùëÅ
ùëú
ùëü
ùë§
ùëé
ùë¶
)

and this vector in 'globally' (meaning that trained on universal dataset, such as wikipedia) trained model would be closest to 
v(Oslo)
ùë£
(
ùëÇ
ùë†
ùëô
ùëú
)
."
"How are Q, K, and V Vectors Trained in a Transformer Self-Attention?","These matrices are not learned parameters but are a result of previous (yet parameterized) computations. In self-attentive layers, are all three of them the same, they are the outputs of the previous layers. In encoder-decoder attention, the queries are decoder states from the previous layer, keys and values and the encoder states.

In Equation 1 of the Attention is all you need paper, these are just parameters that come from outside:

It just says, what do you do, if get some queries, keys, and values from somewhere outside. This also the case of the unnumbered equation on top of page 5. Here, you only project the keys, queries, and values for the heads of multiple attentions.

Here 
W
Q
i
ùëä
ùëñ
ùëÑ
, 
W
K
i
ùëä
ùëñ
ùêæ
, and 
W
V
i
ùëä
ùëñ
ùëâ
 are learnable parameters and they learned by the standard back-propagation algorithm.

Note that although keys and values (and queries in the self-attention) are equal only at the input of the 
Multihead
Multihead
 function. The 
Attention
Attention
 function already gets the projected states."
How to process the hyphenated english words for any nlp problem?,"They all sound like interesting approaches. The first one is better I think because it allows for unseen hyphenated words to be somewhat understood (as e.g. well + known ~= well-known).

For a tfidf BOW model, you might get good performance from any of the above.

For a model that is sensitive to word order I would certainly go with the first option and might tokenise the text so that I had a token to represent the hyphen too."
"If i use use BERT embeddings for if cosine(sent1,sent2) > 0.9, then is it fair to assume s1 and s2 are similar","They might or might not be similar, the embeddings extracted by mean pooling the BERT output usually have high cosine similarity even though the input sentences are completely different.

Bert embeddings are not meant for sentence similarity task(SST), but there is some research combining Bert and SST. Here are those resources,

SBERT paper: https://arxiv.org/abs/1908.10084

SBERT implementation: https://github.com/UKPLab/sentence-transformers"
"Of all the books ever published in English, what percentage are available for NLP training?","Things are much more complex than this in reality.

Obviously Wikipedia and a few other datasets are used mostly because they are quite large and easily available and free. But there is a huge variety of corpora in various languages and for various tasks. There are several semi-commercial sites, for example the Linguistic Data Consortium or ELDA.

So as mentioned earlier, of course there is no such dataset of all books in the English language:

It's impossible, there are probably hundreds of new books in English published every minute.
It would be extremely expensive, since it basically contains all the content of all the libraries in the world (btw if this corpus was free, all the libraries would be bankrupt).

The closest to this is Project Gutenberg, which compiles a lot of books which fell in the public domain (so not recent ones).

But anyway it wouldn't be as useful as you imagine, because things are more complex because it depends what the model is for: one doesn't train a model on literature for suggesting options in corporate emails, for example. There are two main 'types' to take into account: the genre (e.g. book, email, social media, letter, scientific paper, news ...) and the domain (legal, biomedical, political, ...).

There are also a lot of other parameters to take into account:

When the text is written/published. Shakespeare's English is not exactly the same as the one spoken today. New words/terms appear every day.
Regional dialect: UK, US, Australia... There is even a new ""EU English"" recently.
Quality"
Output a word instead of a vector after word embedding?,This answer describes how you go from a vector in the embedding space back to the the most similar class (e.g. word or character).
Does BERT has any advantage over GPT3?,"This article on Medium introduces GPT-3 makes some comparisons with BERT.

Specifically, section 4 examines how GPT-3 and BERT differ and mentions that: ""On the Architecture dimension, BERT still holds the edge. It‚Äô s trained-on challenges which are better able to capture the latent relationship between text in different problem contexts.""

Also, in section 6 from the article, author lists areas where GPT-3 struggles. It may be that BERT and other bi-directional encoder/transformers may do better, although I have no data/references to support this yet."
Autocomplete with deep learning,"This can be easily worked out using a standard Seq2Seq word-level model without any modifications in the training process. During inference, in the decoder, just mask the vocabulary based on the partial word input which has been provided.

By masking, I mean to assign a very high score (inf) to words which don't start with the partial word input, thus constraining the decoder to predict words which only start with the partial word input."
"Is there any NLP library or package which can help in adding comma, punctuation, newlines appropriately to text?","This can be solved with ""text segmentation"". NLP libraries have code for breaking given text into :

Sentences
Phrases
Words

With this, you can break text into sentences and insert . or ? for each sentence. Similarly, dependency tree will help with inserting some punctuation marks (not all).

Example (breaking text into sentences):

import spacy
nlp = spacy.load('en_core_web_sm')
text = ""I was expecting a surplus of cute close-ups but Burton does surprisingly little to win us over He's never been big on treacle but a bit more warmth in this chilly movie which barely follows the outline of the 1941 original would have gone a long way""
text_sentences = nlp(text)
for sentence in text_sentences.sents:
    print(sentence.text)


Output is :

I was expecting a surplus of cute close-ups but Burton does surprisingly little to win us over

and

He's never been big on treacle but a bit more warmth in this chilly movie which barely follows the outline of the 1941 original would have gone a long way

More details : https://spacy.io/usage/linguistic-features"
How to figure out if two sentences have the same meaning with AI?,"This corresponds to an NLP task called paraphrase detection. It's an active area of research, as far as I know there's no ready-to-use system able to perform this task very well, but there are probably a good few methods and prototypes around. A quick search gives these links for example:

https://aclweb.org/aclwiki/Paraphrase_Identification_(State_of_the_art)
https://arxiv.org/abs/1712.02820"
What approach should I take for my product classification ML model with user feedback for improving result accuracy?,"This could be framed, as a first approximation, as a supervised learning classifier, where, based on the input texts (both name and description), you can build a series of features to build your classification model.

One option is:

tokenize (split into words) your texts (both name and descriptions)
filter some not useful (presumably) words like preprositions and other so called stop words (look at libraries like nltk for language processing
select the most frequent words of your bag-of-words based on all of the categories you have until now; this is something you can find out by looking at a frequency bar plot for your entire words dataset
find the frequency of ocurrence of each word in each of your name-description sample, where each row of your dataset could be something like:
kitchen	bathroom	storage	cooking	microwave	oven	...	CATEGORY_label
0	0	0	0	2	1	...	1
1	0	1	0	0	0	...	3
...							

where label 1 is your kitchen appliances category and so on...

This would end up in a multi-class classifier, since you are trying to classify among several possible categories.

As many new entries as you end having, more key words you will have for each category.

This is the easiest approach (based on words counting) you could begin with, since, for natural language processing, you can go on lagter with other approaches: TF-IDF instead of just counting words, and other more sophisticated like word embeddings"
Do generative model produce varying outputs for same input,"This depends entirely on the specific model. There are generative models, like most Generative Adversarial Networks (GANs) that receive a random number and generate data. There are other generative models that generate a probability distribution over the output space (e.g. text generation models), and therefore whether the model generates data deterministically depends on the inference procedure (e.g. greedy, sampling, beam search).

If you want your model to generate outputs deterministically, you just select a model and inference method that assures that.

In your example, you may have a normal seq2seq model (e.g. Transformer) and use beam search for decoding, and the outputs will be the same given the same input."
Is backpropagation applied every layer the same?,"This depends on how you configure the training process:

You can, for instance, freeze the pretrained layers; this implies that only the not pretrained layers will be updated.

You can also set different learning rates to different layers, so that the pretrained layers are assigned a very small learning rate that allows them to be updated but not too fast.

Therefore, backpropagation is the same for all layers but the weight update strategy can be different."
Predicting similarity between nouns like university names and tech companies?,This is a case of entity resolution for which a standard method is not available. You will have to write your own method also using abbreviation resolution. The python Dedupe package has some distance metrics which you could use to calculate the similarities.
"What is the logic/algorithm behind 'did you mean' suggestion by search engines, command suggestion in command prompt like git?","This is a combination of text similarity measures and a large database of popular queries.

It's quite easy in the case of small closed sets, like git commands: there are only a few possible commands, so the whole matrix of similarities can be predefined or even calculated on the fly with the query. The most similar option(s) are then proposed.

In the case of major search engines they certainly have a massive database of frequent queries, and possibly predefined groups of similar queries to make the suggestions appear quickly.

How do one usually backtrack the algorithm behind an application from usage?

It's simply impossible, unless you have access to the code (git is open-source for example) you have to redesign the whole thing."
NLP logistic regression,"This is a completely plausible model. You have five features (probably one-hot encoded) and then a categorical outcome. This is a reasonable place to use a (multinomial) logistic regression.

Depending on how important those first five words are, though, you might not achieve high performance. More complicated models from deep learning are able to capture more information from the sentences, including words past the fifth word (which your approach misses) and the order of words (which your approach does get, at least to some extent). For instance, compare these two sentences that contain the exact same words

The blue suit has black buttons.

The black suit has blue buttons.

Those have different meanings, yet your model would miss that fact."
Is there any text similarity databse available for phrases?,"This is a difficult problem, but definitely worth exploring.

An interesting resource to look into is DBpedia. It aims to extract structured information from the Wikipedia project. It is available under a free license (CC-BY-SA).

You can conveniently explore the project online, e.g.:

IBM
Beam Suntory

Note that you are restricted to the extensive but ending knowledge on Wikipedia, for example Synergy Telecom/SynTel seems not to have an entry. Your creativity would be required to overcome this limitation."
Is there a process flow to follow for text analytics?,"This is a great place to start! While not catalogued in a ""process flow"", Daniel Jurafsky's book, ""Speech and Language Processing"" talks through the various calculations and steps related to analyzing text that you will find useful.

The reason I say that a process flow is not provided is because Jurafsky - in great detail - explains the pros and cons of particular methods applied throughout a pipeline, and how this could change results. As an example, when calculating perplexity (an inverse metric that quantifies how well a language model can predict the next word in a statement), you should capture beginnings, ends, and stop words of statements - as opposed to other methods that require the removal of stop words."
What is the concept of Normalized Mutual Information in the evaluation of Clustering?,"This is a method for evaluation of two clusterings in the presence of class labels so it is not proper for real clustering problems in which class labels are not available.

Imagine you have class labels and you want to evaluate a clustering or (compare two clusterings). The most natural idea is to use Purity score. It simply checks labels with clusters and the best case is, of course, when each cluster contains only and only one class label. This score, however seemingly natural, has a drawback. If you consider each cluster having only one data point, then Purity is maximized! So there should be an awareness about the number of clusters when calculating the purity score.

The next idea is calculating the Mutual Information. Mutual Information considers two splits: (1) split according to clusters and (2) split according to class labels. Then it tells you how these two splittings agree each other (how much information they share about each other or how can you know about one of them if you know the other one). Like purity, MI also gets bigger when the number of clusters is large.

Then comes NMI which is bias-corrected for the phenomenon explained above and also normalizes the score between 
0
0
 and 
1
1
 (MI does not have an upper bound).

NOTE: I think your question was answered in first line. If you want to evaluate clustering you are not looking for external measures where labels are needed. I just explained a bit for sake of completeness of answer."
Which (naive) NLP method for correlating human messages in chatrooms?,"This is a stylometry task, more exactly some form of author verification/identification task. In case you want to dig deeper, the PAN workshop series is a good source of datasets and methods.

About your method:

Your intuition is correct about selecting the most frequent words, in particular stop words: writing style is better characterized by the patterns in frequent grammatical constructs than by the choice of content words. However depending on the size of the data you might want to be more flexible about the number of words: if you have enough data, you should probably take more than the top 100 frequent words. If you don't, well... it might not work very well. Note also that some successful methods don't use the top frequent words but rather a range of middle frequency words.
Using TFIDF is controversial for style identification. It's often done simply with the term frequency instead.

Other features you could consider:

words n-grams, typically bigrams or even trigrams if there is enough data.
characters n-grams (usually trigrams) have the surprising property to be very robust features for style detection. If you use these do not apply any tokenizer or remove punctuation signs."
NLP problem : Choosing the optimal parser for extracting quantitative values from text,"This is an NER problem. Rather than you splitting your sentence to words and finding the right word from dict, I suggest you use an NER (may be spacy NER mentioned by @jindrich).

This NER will point out right block of information from the text your sentences.

Once you get an Entity then you can parse its value. If it is quantitative then it is easy to parse (with simple preprocessing) and if it is qualitative then you may have to convert string to numbers like one to 1 . There are free libraries for that."
Generate new sentences based on keywords,"This is area of NLG . You can use template based text generation techniques, wherein you have defined structure of output text and fill in required blank areas based on keywords. This technique is used in reports generation. An example is narrative science company.

Other approach can be to use OpenAI GPT . Example is Generate Text using OpenAIGPT2 in Python . You may have to tweak the code as per your requirement.

Paraphrasing can be another technique. An example of paraphrasing is - https://github.com/vsuthichai/paraphraser"
Creating Domain specific Question Answering Systems,"This is broad question, but here is the general approach. There are several sub-problems involved in a typical QA system, and each may be solved using different approaches:

Question Classification - What type of question is the user asking? This can be posed as a classification problem, assuming you have labels.
Parsing the question - Multiple NLP techniques would be required here
Converting question to canonical form - Every question would be converted into a canonical or structured format (which can be used to query). This is usually the most difficult part of the process. This is usually referred to as Semantic Parsing. One approach is to get variants of phrases and then map it to a single canonical form using some similarity metric. You can start with this paper.

If you can get from a natural language text to a canonical form, you are essentially done. Storing data and retrieval are trivial aspects which could be done in numerous ways. Coming to the question of domains, it is always going to be a trade-off between accuracy for one domain vs a general QA system (which is way too difficult to crack). A single model for every domain will most likely give you better results at the cost of extra effort."
Contributions of each feature in classification?,"This is called feature ranking, which is closely related to feature selection.

feature ranking = determining the importance of any individual feature
feature selection = selecting a subset of relevant features for use in model construction.

So if you are able to ranked features, you can use it to select features, and if you can select a subset of useful features, you've done at least a partial ranking by removing the useless ones.

This Wikipedia page and this Quora post should give some ideas. The distinction filter methods vs. wrapper based methods vs. embedded methods is the most common one.

One straightforward approximate way is to use feature importance with forests of trees:

Other common ways:

recursive feature elimination.
stepwise regression (or LARS Lasso).

If you use scikit-learn, check out module-sklearn.feature_selection. I'd guess Weka has some similar functions."
Determine the most important documents for supervised learning,"This is commonly called active learning.

The most common active learning approaches for text classification are random, data-based, model-based, and predication-based."
"Gensim doc2vec error: KeyError: ""word 'senseless' not in vocabulary""","This is Doc2Vec not Word2Vec, so I don't think you you don't give a word to most_similar(). So instead of:

model_dbow1.most_similar('senseless')


I think you would do:

model_dbow1.most_similar('55001')


Or, alternatively if you did want to search for a one-word sentence:

vector = model_dbow1.infer_vector([""senseless""])
model_dbow1.most_similar([vector])


(The above is a bit of guesswork, based on the online docs for Doc2Vec, and some tutorials (e.g. this and e.g. Sentence similarity using Doc2vec). Where possible give a fully reproducible example that we can test against.)"
How to do sentence segmentation without loosing sentence's subject?,"This is not really sentence segmentation because the input is already a single sentence.

This is close to relation extraction but I don't know if it's applicable here. It's clear that there needs to be at least some minimal semantic analysis done in order to identify the subject. This could be done with dependency parsing, possible followed by semantic role labeling.

Note that things can get complicated if the sentence is in the passive form or in various other cases of paraphrase, e.g. ""the top performer last year was Joe""."
Is it possible to fine-tuning BERT by training it on multiple datasets? (Each dataset having it's own purpose),"This is possible but the BERT model will lose its purpose. Each NLP task will have its optimal loss value. If many tasks are fine-tuned on the same model, the optimal loss function for all the tasks will not be reached."
NLP - How to detect sentence validity,"This is related to language modeling.

A language model can predict the likelihood of a sentence after being trained on a large corpus of sentences. A language model does not deal with the semantics of the sentence, it can only assess how plausible the sentence is from a statistical point of view."
Transformer model: Why are word embeddings scaled before adding positional encodings?,"This is specified in the original Transformer paper, at the end of section 3.4:

Transcription:

3.4 Embeddings and Softmax

Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension ùëëmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by ‚àöùëëmodel

This aspect is not justified by the authors, either on the paper or anywhere else. It was specifically asked as an issue in the original implementation by Google with no response.

Other implementations of the Transformer have also wondered if this was actually needed (see this, this and this).

Some hypothesithed arguments (source) are:

It is for the sharing weight between the decoder embedding and the decoder pre-softmax linear weights.
It is not actually needed.
It is to make the positional encoding relatively smaller. This means the original meaning in the embedding vector won‚Äôt be lost when we add them together.

For reference, there are other StackExchange questions discussing this (see this and this)."
How does machine learning algorithms process text?,"this is the question of text representation: how text can be be converted (simplified) into numerical features, in a way which preserves the meaning as much a possible and makes it usable for ML.

Nowadays there are two main types of text representation: the traditional one based on one hot encoding, and the recent one based on word embeddings. Note that there are many variants of those, and even other types.

I think the most intuitive explanation is to study the traditional representation: in its most basic form, every word in the full corpus is assigned a fixed index 
i
ùëñ
, and every document (or sentence) is represented as a vector in which every position 
i
ùëñ
 has value 1 if and only if the corresponding word 
w
i
ùë§
ùëñ
 is present in the document. This way the learning algorithm can create conditions like ""if word w_i belongs to the document then predict class X"" for instance. The rest is the usual learning process: the algorithm finds the statistical patterns which connect the features/words to the classes and produces a model which exploits those patterns.

Word embeddings offer a more subtle but also more complex representation of the meaning of a word. Every dimension represents some specific kind of semantic information, but it is cannot be interpreted directly."
Why I would use TF-IDF after Bag-of-Words (CountVectorizer)?,"This is the standard TF-IDF feature extraction: you transform the document counts. It just looks odd to separate the two steps like this. sklearn provides both TfidfTransformer and TfidfVectorizer; note the documentation of the latter:

Equivalent to CountVectorizer followed by TfidfTransformer."
encoding of text data in NLP,"This kind of problem is called record linkage (or sometimes entity matching or other variants). The task consists in finding among a list of strings representing entities (persons or organizations) those which represent the same actual entity.

There are two main approaches (which can be combined):

String similarity matching methods. See for example this question or this one. Note that in case the list of companies is large, there can also be an efficiency issue: see this question.
Databases or third-party resources. See for example this related question."
Newbie questions: real-time clustering of messages,"This looks like a problem of topic modelling: unsupervised clustering based distributional semantics.

For the issue of the unknown number of clusters, HDP is certainly a good option.
For analyzing the changing behaviour, there are dynamic topic models which can represent this, in particular D-LDA, DETM."
Phrase/Token labeling,"This looks like a sequence labelling problem, the most common such problem in NLP being Named Entity Recognition (NER).

You'll find a lot of libraries and tutorials about NER. It can be done with Conditional Random Fields but there are also neural methods nowadays.

Assuming your problem is not about standard entities (like persons names, organizations, locations), you'll need to train a custom NER-like model. To do that you will need a large amount of data annotated for your specific task."
"""Object"" Detection in Textual Data","This looks quite similar to Named Entity Recognition (NER), which is traditionally done with a sequence labeling model such as Conditional Random Fields. Normally NER is used when:

The list of possible entities is not predefined: the training data might contain ""Mr James Smith"" but the test data could contain ""Mr John Doe"". In other words, the classes are open.
It is assumed that the context of the text can help the model predict an entity. For example in a sentence like ""Today X said that ..."", the word ""said"" after X should help the model predict that X is either a person or an organization, but it cannot be a location."
What is the best way to use word2vec for bilingual text similarity?,"This paper from Amazon explains how you can use aligned bilingual word embeddings to generate a similarity score between two sentences of different languages. Used movie subtitles in four language pairs (English to German, French, Portuguese and Spanish) to show the efficiency of their system.

""Unsupervised Quality Estimation Without Reference Corpus for Subtitle Machine Translation Using Word Embeddings"""
Fine-tuning NLP models,"This paper might be useful....

https://arxiv.org/abs/1801.06146"
Check If Answer for a Question is Correct by Similarity,"This problem can be solved better if you also include the background text from where the questions have been picked up. Then, firstly train word embeddings on the background text. Further, generate sentence compositionality of the questions as well as their answers using a Recursive Neural Network or some variant. After all these steps you can compute the similarity between compositions of the new answer and given answers to know if it is correct or not."
Binary document classification using keywords for a very small dataset,"This problem is called text classification (it belongs to the more general case of document classification). There are plenty of resources online about this, e.g. here, here or here. There are also a lot of research papers on the topic.

General text classification consists in two steps:

Represent the text as features
Train a classification model

The first step is specific to text, as opposed to the second step which is general ML. There are plenty of options to represent text as features, from traditional bag of words representation to word embeddings. In this question I explained the principle of the traditional BoW representation."
How to group chat messages by topic?,"This problem is related to the following standard problems:

topic classification/modeling, which ranges from simple supervised document classification to unsupervised assignment of topics distributions to every document (the advanced option, with Latent Dirichlet Analysis and variants)
sequence labeling, a supervised task which predicts classes for every instance in a sequence, taking into account the order of the instances (e.g. it can leverage the fact that the class of instance 
n
ùëõ
 is influenced by the class of instance 
n‚àí1
ùëõ
‚àí
1
).
text segmentation, more precisely topic segmentation in this case.

I think that one can find a lot of good implementations for the first two problems, which are very common. However adapting these to your case and/or combining them will probably be more complex.

If you have a small set of topics and you have (or can have) a reasonable sample of messages annotated with these topics, I would suggest starting with sequence labeling with Conditional Random Fields. I think this could give good results given the sequential nature of chat conversations."
approach to classify text with natural language processing methods,"This problem seems like a multi-class multi-label problem. The questioner seems to be comfortable in building a detailed ontology. These lead the author to propose the following approach. Please note that a detailed explanation of this can be found in an article here.

Steps to solve the problem:

Build a taxonomy file as a csv file as given below. Please note, the column headings should be identical to whats given below. 
Put all your content in another csv file that looks like below. Please note, the column headings should be identical to whats given below. 
In the following python code, please enter the path to content in the path to df and path to taxonomy in the path to df_tx. These steps are present near the comment import data for mapping. Add another path value for the output towards the end of the code.

Run the python code below. Please note that this code runs fine on Python 2.7 in Windows 10 machine. Please iron out any technical issues yourself as the author may not be of much help for such issues.

#Invoke Libraries
import pandas as pd
import numpy as np
import re

#import data for mapping
df = pd.read_csv(""path to content csv"");
df_tx = pd.read_csv(""path to taxonomy csv"");

#Build functions
#function that identifies taxonomy words ending with (*) and treats it as a wild character
def asterix_handler(asterixw, lookupw):
    mtch = ""F""
    for word in asterixw:
        for lword in lookupw:
            if(word[-1:]==""*""):
                if(bool(re.search(""^""+ word[:-1],lword))==True):
                    mtch = ""T""
                    break
    return(mtch)

#function that removes all punctuations. helps in creation of set of words
def remov_punct(withpunct):
    punctuations = '''!()-[]{};:'""\,<>./?@#$%^&*_~'''
    without_punct = """"
    char = 'nan'
    for char in withpunct:
        if char not in punctuations:
            without_punct = without_punct + char
    return(without_punct)

#function to remove just the quotes(""""). This is for the taxonomy
def remov_quote(withquote):
    quote = '""'
    without_quote = """"
    char = 'nan'
    for char in withquote:
        if char not in quote:
            without_quote = without_quote + char
    return(without_quote) 

#split each document by sentences and append one below the other for sentence level categorization and sentiment mapping
sentence_data = pd.DataFrame(columns=['slno','text'])
for d in range(len(df)):    
    doc = (df.iloc[d,1].split('.'))
    for s in ((doc)):        
        temp = {'slno': [df['slno'][d]], 'text': [s]}
        sentence_data =  pd.concat([sentence_data,pd.DataFrame(temp)])
        temp = """"

#drop empty text rows and export data
sentence_data['text'].replace('',np.nan,inplace=True);      
sentence_data.dropna(subset=['text'], inplace=True);  

data = sentence_data
cat2list = list(set(df_tx['Category2']))
data['Category'] = 0
mapped_data = pd.DataFrame(columns = ['slno','text','Category']);
temp=pd.DataFrame()

for k in range(len(data)):        
    comment = remov_punct(data.iloc[k,1])
    data_words = [str(x.strip()).lower() for x in str(comment).split()]
    data_words = filter(None, data_words)
    output = []

    for l in range(len(df_tx)):
        key_flag = False
        and_flag = False
        not_flag = False
        if (str(df_tx['Keywords'][l])!='nan'):
            kw_clean = (remov_quote(df_tx['Keywords'][l]))
        if (str(df_tx['AndWords'][l])!='nan'):
            aw_clean = (remov_quote(df_tx['AndWords'][l]))
        else:
            aw_clean = df_tx['AndWords'][l]
        if (str(df_tx['NotWords'][l])!='nan'):
            nw_clean = remov_quote(df_tx['NotWords'][l])
        else:
            nw_clean = df_tx['NotWords'][l]
        Key_words = 'nan'
        and_words = 'nan'
        and_words2 = 'nan'
        not_words = 'nan'
        not_words2 = 'nan'

        if(str(kw_clean)!='nan'):
            key_words = [str(x.strip()).lower() for x in kw_clean.split(',')]
            key_words2 = set(w.lower() for w in key_words)

        if(str(aw_clean)!='nan'):
            and_words = [str(x.strip()).lower() for x in aw_clean.split(',')]
            and_words2 = set(w.lower() for w in and_words)

        if(str(nw_clean)!= 'nan'):
            not_words = [str(x.strip()).lower() for x in nw_clean.split(',')]
            not_words2 = set(w.lower() for w in not_words)

        if(str(kw_clean) == 'nan'):
            key_flag = False        
        else:
            if set(data_words) & key_words2:
                key_flag = True
            elif(bool(re.search('""',df_tx['Keywords'][l]))==True and quote_handler(key_words, comment) == 'T'):
                key_flag = True            
            elif(asterix_handler(key_words2, data_words)=='T'):                
                    key_flag = True   

        if(str(aw_clean)=='nan'):
            and_flag = True
        else:
            if set(data_words) & and_words2:
                and_flag = True
            elif(bool(re.search('""',df_tx['AndWords'][l]))==True and quote_handler(and_words, comment) == 'T'):
                and_flag = True            
            elif(asterix_handler(and_words2, data_words)=='T'):
                and_flag = True

        if(str(nw_clean) == 'nan'):
            not_flag = False
        else:
            if set(data_words) & not_words2:
                not_flag = True
            elif(bool(re.search('""',df_tx['NotWords'][l]))==True and quote_handler(not_words, comment) == 'T'):
                not_flag = True            
            elif(asterix_handler(not_words2, data_words)=='T'):
                not_flag = True

        if(key_flag == True and and_flag == True and not_flag == False):
            output.append(str(df_tx['Category2'][l]))            
            temp = {'slno': [data.iloc[k,0]], 'text': [data.iloc[k,1].strip()], 'Category': [df_tx['Category2'][l]]}
            mapped_data = pd.concat([mapped_data,pd.DataFrame(temp)], sort = False)

#output mapped data
mapped_data = mapped_data[['slno', 'text', 'Category']]   

mapped_data.to_csv(""Path here/mapped_data.csv"",index = False)               




Final output looks like this:"
NLP methods specific to a language?,"This question is quite open, but nonetheless, here are some:

lemmatization/stemming only makes sense in languages where there is a lemma/stem in the word. Some languages like Chinese have no morphological variations (apart from some arguable cases like the explicit plural ‰ª¨), and therefore lemmatization and stemming are not applied in Chinese.

Word-based vocabularies are used to represent text in many NLP systems. However, in agglutinative and polysynthetic languages, using word-level vocabularies is crazy, because you can put together a lot of affixes and form a new word, therefore, a prior segmentation of the words is needed.

In some languages like Chinese and Japanese, there are no spaces between words. Therefore, in order to apply almost any NLP, you need a preprocessing step to segment text into words."
Combine datasets of different domains to ehance generalizibility,"This sounds reasonable indeed, but I would suggest to verify this experimentally:

Since you have access to multiple heterogeneous datasets, I think a good way to evaluate the ability of the model to generalize would be to train on all the datasets but one, and then evaluate on the remaining dataset. Then preferably repeat with every dataset as test set in order to account for chance (similarly to cross-validation). To test whether the hypothesis works, you should also train baseline models using only one dataset and compare their performance on the same test sets."
Python package to assess text coherence,"This task looks similar to what is called text segmentation, in particular topic segmentation. I don't know any python package to do it but apparently Google gives a good few results for ""semantic text segmentation python"" (I'm not sure that this is the best phrase, you might want to try variations).

Note: this is still an active NLP research topic as far as I know. I don't know how fast reliable python packages are written and maybe there are some for this, but I wouldn't be too surprised if there were only research prototypes available at this stage."
An universal sentence encoder for a specific language?,"This Universal Sentence Encoder that you link is trained specifically on English data, so it's going to work very poorly on any other language (to be clear, it's likely to produce garbage).

Unfortunately it's quite unlikely that you'll find a similar pre-trained model for Macedonian. You would have to train your own model from Macedonian data, and you need a really large amount. Btw that's the main reason why these pre-trained models are often trained on English only, since there's a lot of English text available. In case you want to try this, there is a Macedonian corpus as part of the Universal Dependencies project."
Question and Answer Chatbot for Customer Support,"This was intended as a comment.

I think this question is not receiving enough attention because it is too difficult to answer without trying several approaches first.

Your third idea is a nice one, but LSA is most likely not going to be able to help you choose clusters with the necessary granularity. For example, you may get a cluster of ""broken things"" but not to the specificity that is required to be helpful to the customer. A similar approach would be using Hierarchical Dirichlet Processes, but again, you may not get the clusters that you want.

I wouldn't be so eager to try your fourth option because it requires a lot of work and I'm not sure you're going to get great results unless your customer support deals with very well-defined questions and in that case, you're going to have to build your own ontology from scratch.

In your situation, I would be curious if I can get information about the inner workings of Watson (obviously, the purpose is only to have a general idea of some steps that are sensible enough to try.) Here is a descriptive image that could be helpful from Wikipedia:

The main idea is to segment questions into its components (using a regular chunking), then come up with several possibilities (hypothesis generation) of what the question might refer to (maybe this step can be implemented as a clustering). The third stage is rating the previous hypothesis based on available information. Finally, you choose the more likely hypothesis. There is a feedback involved in this process using sample questions and their answers. It would be interesting to see if the rating stage can be modeled in terms of a Bayesian or an incremental approach to allow for a feedback."
Convolutional Network for Text Classification,"This wildml blog post has a very clear explanation of how to use 1D convolution on text. And Debo, DS at x.ai, provided some example Keras code to classify text using a character-based model (input documents are sequences of one-hot encoded characters rather than words or POS tags):

from keras.models import Model
from keras.layers import Input, Dense, Dropout, Flatten
from keras.layers.convolutional import Convolution1D, MaxPooling1D

inputs = Input(shape=(maxlen, vocab_size), name='input', dtype='float32')
conv = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[0],
                     border_mode='valid', activation='relu',
                     input_shape=(maxlen, vocab_size))(inputs)
conv = MaxPooling1D(pool_length=3)(conv)
conv1 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[1],
                      border_mode='valid', activation='relu')(conv)
conv1 = MaxPooling1D(pool_length=3)(conv1)
conv2 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[2],
                      border_mode='valid', activation='relu')(conv1)
conv3 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[3],
                      border_mode='valid', activation='relu')(conv2)
conv4 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[4],
                      border_mode='valid', activation='relu')(conv3)
conv5 = Convolution1D(nb_filter=nb_filter, filter_length=filter_kernels[5],
                      border_mode='valid', activation='relu')(conv4)
conv5 = MaxPooling1D(pool_length=3)(conv5)
conv5 = Flatten()(conv5)
z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(conv5))
z = Dropout(0.5)(Dense(dense_outputs, activation='relu')(z))

pred = Dense(n_out, activation='softmax', name='output')(z)   
model = Model(input=inputs, output=pred) 
model.compile(loss='categorical_crossentropy', optimizer='rmsprop',
              metrics=['accuracy'])


The last 3 lines are important. You can't use softmax on your output and you can't use 'categorical_crossentropy' for multi-label tagging (your problem). Your text tagging problem should be broken down into multiple binary classification problems, or you need to use a different loss function like 'binary_crossentropy'. And for binary_crossentropy, use a sigmoid activation function rather than softmax on the output. See this SO answer for details on multi-label tagging problems in keras and TF.

If you want a more thorough explanation, check out Chapter 7 in my book, NLP In Action."
"Data Annotation: ""labeling"" target vs features","Those labels are not primarily for features, those labels are primarily for targets. Person, location, and event for targets for named-entity recognition (NER)."
Text embeddings and data splitting,"TL;DR

If you are training the document-embedding model, then split the data before you convert the text into embeddings.

If you are using a pre-trained document-embedding model, then it won't matter and it is pre-processing step that it doesn't matter when you execute it.

Pipeline when training your own document-embedding model
Split your text data into train/validate/test sets.
Use your train set to train the document-embedding model.
Use your trained document-embedding model to convert train and validation sets to train your other model (e.g. classification model).
Test your final model by using your trained document-embedding model to convert the test set and test the trained final (classification) model."
Sum vs mean of word-embeddings for sentence similarity,"TL;DR

You are better off averaging the vectors.

Average vs sum

Averaging the word vectors is a pretty known approach to get sentence level vectors. Some people may even call that ""Sentence2Vec"". Doing this, can give you a pretty good dimension space. If you have multiple sentences like that, you can even calculate their similarity with a cosine distance.

If you sum the values, you are not guaranteed to have the sentence vectors in the same magnitude in the vector space. Sentences that have many words will have very high values, where as sentences with few words with have low values. I cannot think of a use-case where this outcome is desirable since the semantical value of the embeddings will be very much dependand on the lenght of the sentence, but there may be sentences that are long with a very similar meaning of a short sentence.

Example
Sentence 1 = ""I love dogs.""
Sentence 2 = ""My favourite animal in the whole wide world are men's best friend, dogs!""



Since you may want these two sentence above to fall closely in the vector space, you need to average the word embeddings.

Doc2Vec

Another approach is to use Doc2Vec which doesn't average word embeddings, but rather treats full sentences (or paragraphs) as a single entity and therefore a single embeddings for it is created."
"Extractive text summarization, as a classification problem using deep networks","TL;DR:

Is something like this feasible? (I know that nothing can be said for sure in data science unless tried out, but is it worth the shot?)

Yes

What kind of features should I feed into the classifier? I already know of word frequency distribution(TF-IDF), sentence position, co-occurance stats, but will these be enough? What additional features should I consider? Or should I consider directly feeding in word vectors?

Feed in word vectors. Deep learning has it's way of figuring out good features like tf-idf,co-occurence etc.

Longer Version:

Input:

Convert all the words to word vectors with a word embedding algorithm like Word2Vec or Glove. This is usually a good idea since this allows you to represent words in a better semantic (dog, cat, cow,etc will be close) and syntactic (November,December,etc will be close) sense.

(Using tf-idf as a feature hasn't worked for me. I used it to supervise extractive summarization with a neural net.)

How to manage inputs:

This depends very much on how your data is. (Please let me know which dataset) I assume you have something like [sentence1, sentence2,...] as inputs and an output like [True, False, ...] which implies sent1 is in the summary and sent2 isn't.

If your data isn't in this form, you should convert it into this form.

You haven't specified whether it's Unsupervised or Supervised.

for Supervised:

Use an LSTM. Feed the LSTM the sentence word by word. Once the sentence has ended (marked by a token say or a full stop) make the LSTM make a prediction whether the sentence should be in the summary or not. Train it based on this.

This is kind of similar to how you would do sentiment analysis: feed in the sentence and then ask if the sentiment is positive or negative.

You can experiment with stacking LSTMs or changing hyper parameters to get better results.

for Unsupervied:

This is a bit difficult. You could convert the sentences to vectors (Sent2Vec) and hope that important sentences cluster together because they contain important words.

Or you could train an RNN to predict the next character and hope that one of the neurons learns to predict important words/sentences as done by OpenAI's unsupervised sentiment neuron which learnt to predict the sentiment.

The general ""Deep"" architecture followed these days is : Embed, encode, attend, predict

Check out the links I have posted (in blue), they might work for you.

Note: If possible, please mention your dataset and also how you managed to make abstractive summaries (considering that it is a lot harder than extractive)"
"how can i leverage NLP features like SRL, LSA, POS, NER, entity type , relation type with deep learning to find semantic similarity of texts?","TL;DR:

Represent words as word vectors. Then add extra dimensions to the word vectors. In these extra dimensions, include POS, NER, etc. features in a numeric form.

Longer Version:

Say you have a word2vec/Glove model with each word represented by a 100 dimensional vector. Additionally, you have features like POS, NER, etc. for each word. Instead of representing each word with just 100 dimensions, represent it with 100 + n dimensions and fill these n slots with the nlp features.

This will give the vectors an additional meaning which you have defined manually.

Suggestions

You could benefit from using other models like Doc2Vec/Sent2Vec which can represent a whole sentence as vector. With these vectors you could query similarity of sentences.

So say you've trained it on your data, just query :

model.most_similar_cosmul(SENT_42)

where SENT_42 represents ""to whom i need to contact after i resign ?"", you will get a list of sentences similar to it.

I suggest you use gensim's doc2vec model in python."
"Word2Vec: Why do some dimensions of an embedding have an interpretation, and why does addition/subtraction of embedding vectors work?","TL;DR: A theoretical/mathematical explanation for why word2vec/GloVe embeddings of analogies appear to form parallelograms, and so can be ""solved"" by adding/subtracting embeddings, is given here, as summarised in this blog. More explanation of w2v is given here.

The dimensions of word2vec (or GloVe, etc) word embeddings are not directly interpretable, but capture correlations in word statistics, which reflect meaningful semantics (e.g. similarity), so some dimensions may happen to be interpretable.

The embedding of a word is effectively a low-rank projection of the co-occurrence statistics of that word with all other words (like what you would get from PCA/SVD - but that would require an unweighted least square loss function).

That projection in word2vec is probability weighted and non-linear, making it difficult to interpret what any dimension ""means"". Also, if the embedding matrix 
W
ùëä
 (all embeddings 
w
i
ùë§
ùëñ
 stacked together) is rotated by any rotation matrix 
R
ùëÖ
, and 
R
‚àí1
ùëÖ
‚àí
1
 applied to the other embedding matrix 
C
ùê∂
, the transformed embeddings perform identically. So there isn't a unique solution, but an equivalence class of solutions, meaning the values in embeddings aren't necessarily meaningful in their own right, only when considered relative to each other.

The theoretical explanation of analogies is too long to summarise here, but it boils down to word embeddings capturing log probabilities, so adding embeddings is equivalent to multiplying probabilities and so is meaningful. I gather it's bad form to include link explanations, but the two linked research papers should last in perpetuity."
What size language model can you train on a GPU with x GB of memory?,"Tldr; I‚Äôve seen a good rule-of-thumb is about 14-18x times the model size for memory limits, so for a 10GB card, training your model would max out memory at roughly 540M parameters.

There is some really good information here: https://huggingface.co/docs/transformers/perf_train_gpu_one#anatomy-of-models-memory

Note that there are a ton of caveats, depending on framework, mixed precision, model size, batch sizes, gradient checkpointing, and so on. Just to summarize the above, rough memory requirements are: Model weights

4 bytes * number of parameters for fp32 training
6 bytes * number of params for mixed precision training.

Optimizer States

8 bytes * number of parameters for normal AdamW (maintains 2 states)
2 bytes * number of parameters for 8-bit AdamW optimizers like

bitsandbytes

4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state) Gradients:
4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32) Other: Temporary memory, functionality specific memory, forward activations, and so on."
Algorithms that can determine whether a string is an English sentence?,"To account for all your samples, first check if the text is English at all (solution as others hinted).

If yes, then there is a question what makes a 'complete English sentence'.

From your (two) samples, one can deduce that there should be a subject and a verb in finite form in relation with it. To raise attention to some 'edge cases':

""Go!""

""Done!""

To check subject-verb existence, you can try to parse - determine syntactic structure of the text but as this is still a tricky task, you can use part of speech tagging (actually morhological analysis plus disambiguation but PoS tagging made it's way as a name for the task). E.g. SpaCy does this https://spacy.io/usage/linguistic-features (It does dependency parsing too actually but error rate would be higher than using 'just' PoS tagging).

Then you can check in the output if there is a verb in a finite form - VBD, VBP in VBZ in SpaCy tag set (https://spacy.io/api/annotation#pos-tagging)."
Bert Fine Tuning with additional features,"To add additional features using BERT, one way is to use the existing WordPiece vocab and run pre-training for more steps on the additional data, and it should learn the compositionality. The WordPiece vocabulary can be basically used to create additional features that didn't already exist before.

Another approach to include additional features would be to add more vocab while training. Following approaches are possible:

Just replace the ""[unusedX]"" tokens with your vocabulary. Since these were not used they are effectively randomly initialized.
Append new vocabulary words to the end of the vocab file, and update the vocab_size parameter in bert_config.json. Later, write a script which generates a new checkpoint that is identical to the pre-trained checkpoint, but with a bigger vocab where the new embeddings are randomly initialized (for initialized we used tf.truncated_normal_initializer(stddev=0.02)). This will likely require mucking around with some tf.concat() and tf.assign() calls.

Please note that I haven't tried any of these approaches myself."
"How to calculate Accuracy, Precision, Recall and F1 score based on predict_proba matrix?","To compute performance metrics like precision, recall and F1 score you need to compare two things with each other:

the predictions of your model for your evaluation set (in what follows, I'll call them y_pred)
the true classes of your evaluation set (in what follows, y_true).

From what you write, you have obtained just the predictions of your model, and that's what you have in y_pred. You have constructed y_pred so that each of its components is equal to the class that is assigned the maximum probability by your model. All fine here!

The key ingredient that you are missing is the array of true classes (in your question, you called them ""tags"") associated to your evaluation examples. You totally need this information to understand if the predictions of your model are correct (or not). You should be able to construct an array y_true containing the true classes/tags of your examples from...knowing the actual ones. For example, if your 1st text belongs to class 3, your 2nd text belongs to class 1, your third text belongs to class 2, your y_true will be an array like

y_true = np.array([3, 1, 2, # ... the rest of components ])


Now, to compute accuracy, precision, and recall, you need to compare y_true and y_pred. If they coincide, congratulations: that means that your algorithm works perfectly on your evaluation set! In general though not all the components of y_pred will coincide with y_true. To quantify agreement/discrepancies you can use metrics like accuracy, precision, etc. You can code them yourself, but the scikit-learn library comes with functions for the purpose.

For instance you can easily compute accuracy, precision, recall and F1 score, even the confusion matrix for your problem with the code

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# print accuracy
print(""Accuracy: "", accuracy_score(y_true, y_pred))

# print precision, recall, F1-score per each class/tag
print(classification_report(y_true, y_pred))

# print confusion matrix, check documentation for sorting rows/columns
print(confusion_matrix(y_true, y_pred))


P.S. Note that in what I did I didn't use your y_pred_one_hot (which is not a confusion matrix!), and that the precision is not zero (it may be, but you have to compute it using y_true)!

P.S. Beware using predict_proba with Naive Bayes as output probabilities may not be calibrated."
Spacy word embeddings for sentence,"To construct sentence embeddings Spacy just averages the word embeddings. I do not have access to Spacy right now, else would have give a demonstration but you can try:

spacy_nlp('hello I').vector == (spacy_nlp('hello').vector + spacy_nlp('I').vector) / 2


If this also gives False, it will be because the float values might not be exactly equal after the computation. So, just print them out separately and you will see that they are really close."
How to collect tweets by geo-location?,"To get location based tweets, you have to specify a location circle with center (lat and long) and radius using reverse_geocode.

There is no way to find tweets by setting a polygon or drawing a border."
Extract details from bibliometrics data,To learn from traditional software in that domain you could start at https://en.wikipedia.org/wiki/Comparison_of_reference_management_software
How to identify certain term in a long document with NLP?,"To me this looks like Named Entity Recognition (NER), more generally a sequence labeling problem. The typical approach is to train a custom NER model using a large sample of annotated data.

Pro: this is a very well known problem, there are multiple libraries which implement this.
Cons: based on the description of your problem it's not sure that annotating a large sample of documents is doable, especially if there are very few target terms."
When calculating lexical richness (e.g. TTR) do you lemmatize first?,"To my knowledge there's no standard way to use lexical density measures such as Type-Token Ratio (TTR).

It's common to apply TTR to the raw tokenized text, in particular if there is no suitable lemmatizer available or if the lemmatization risks introducing errors. Otherwise there's no strong reason one way or the other, as far as I'm aware. However it's important to proceed consistently of course: if one wants to compare TTR between different texts, the TTR should have been calculated the same way."
NER - What advantage does IO Format have over BIO Format,"To my knowledge, there is no clear best among the different labelling schemes variants for NER: IO, BIO, BILO (L=last), BILOU (U=unique, for a unique word)... I might forget some.

In theory at least, the advantage of a simple scheme like IO is that the simplified set of labels may prevent the model from making mistakes cases where a word or type of word can appear in any position of an entity, for instance. A more complex scheme can be more accurate but is more likely to lead to overfitting, because the sample of cases for one class is smaller.

If possible, it's preferable to evaluate the different options and pick the best for specific target data."
How to automatically classify a sentence or text based on its context?,"To my knowledge, there is no such library or pre-trained model.

Imho there is an important issue in the task as defined in the question, more exactly in the example: these tags seem natural for a human in the sense that they represent the general topics of the sentence. But technically one could find many other tags which are semantically relevant, for example ellipse, surface, calculation, formula, sciences, knowledge, classes, exercise... The correct granularity (the level of specificity/genericity) of the tags is intuitive for a human, not for a machine.

So the task is possible: one can calculate all the semantically more general concepts, for instance with WordNet, but this would often return too many concepts like in my example. A standard method in this case would be too take the top N according to some measure of semantic similarity.

Notes: ""classify"" is not a good term for this, because classification is a supervised task where classes are known. And it's not really based on the ""context"" of the sentence (""context of X"" usually means ""information around X"" in NLP), it's based on its content or meaning."
Is it possible to add new vocabulary to BERT's tokenizer when fine-tuning?,"To my understanding words unknown to the tokenizer will be masked with [UNKNOWN].

Your understanding is not correct.

BERT's vocabulary is defined not at word level, but at subword level. This means that words may be represented as multiple subwords. The way subword vocabularies work mostly avoids having out-of-vocabulary words, because words can be divided up to the character level, and characters from the training data are assured to be present in the subword vocabulary. Therefore, as long as the alphabet used in your fine-tuning data is the same as the training data, there should be no out-of-vocabulary words: your out-of-domain terms will simply be divided into smaller subwords. So you should not need to include new entries for them in the embedding table.

There are some answers in this site that may give you more examples, like this and this."
Why does an attention layer in a transformer learn context?,"To provide a simplistic and less mathematical reasons. You can assume like this:

In a simple feed-forward neural network (a black-box of course), you shall learn the set of weights, learning a function to map inputs to outputs.

But, in the transformers based architecture, you have Attentions. Here, the weights are structured into Query, Key and Value (Q,K,V). These 3 set of weights drivers of attention and are responsible to learn the context. How precisely it works still remains a black-box like feed forward networks. But yeah, it works something like this, every token's embedding is transformed its query, key and value vectors using their respective weight matrices. For a given token, its query vector is multiplied with all the other tokens' key vector to obtain a value vector. These values determines the importance of every token with respect to the query token. Thus, with back-propagation, you try to optimize these Q, K, V weights, and thus learn it to better map the relationship between tokens."
Bertopic with embedding: unable to use find_topic,"To resolve this, I did the following.
Firstly, I updated BERTopic to 0.12.0. This did not resolve on its own.
What works is excluding seed_topics: i.e. not using semi-supervised mode.

%%time
## instantiate BERTopic with bigram
model_ngram_noembed2 = BERTopic(top_n_words=10, min_topic_size=5, n_gram_range=(1,2), nr_topics=41, verbose=True)  
                     #, embedding_model=embeddings, seed_topic_list=seed_topic_list)

## Fit the models on doc with embedding, generate topics, and return the docs with topics
topics_bert_noembed2, probs_noembed2 = model_ngram_noembed2.fit_transform(docs_bert_dtm)


For completeness, the rest are

## Explicate 'define' within the topic model
keyterm_explore = 'define'

#pd_keyterm_explore = pd.DataFrame(topics_bert.find_topics(keyterm_explore,))
'''
Exception: This method can only be used if you did not use custom embeddings.'''

pd_keyterm_explore = pd.DataFrame(model_ngram_noembed2.find_topics(keyterm_explore), 
             columns=model_ngram_noembed2.find_topics(keyterm_explore)[0],
            dtype='float').round(decimals=2)

print(f'Exploring {keyterm_explore}: \n{pd_keyterm_explore}')

    [out]
    Exploring define: 
        21     12     19    7     3 
    0  21.00  12.00  19.00  7.00  3.00
    1   0.37   0.36   0.34  0.32  0.32"
German Chatbot or conversational AI,"To start with chatbot first of all you to decide which type of chatbot are you trying to build.

Order placing bot
Chitchat bot

Both required different approach to solve the problem.

Order Placing Bot required multiple model for different task like intent identification, named entity recognition, state machine.

Chitchat bot required only 2 person conversation dataset which is available easily on kaggle.com

But if you are looking for specific language dataset then it difficult to find it in both type of bots. For that either you use any translation api which you to pay for it or use web scrapping techniques to do same task at free of cost."
"BERT vs GPT architectural, conceptual and implemetational differences","To start with your last question: you correctly say that BERT is an encoder-only model trained with the masked language-modeling objective and operates non-autoregressively. GPT-2 is a decode-only model trained using the left-to-right language objective and operates autoregressively. Other than that, there are only technical differences in hyper-parameters, but no other conceptual differences.

BERT (other masked LMs) could also be used for zero- or few-shot learning, but in a slightly different way. There is a method called PET (Pattern- Exploiting Training). It uses the language modeling abilities of BERT via templates. E.g., for sentiment analysis, you can do something like:

<...text of the review..><.TEMPLATE......> <  ?  >.
The pizza was fantastic. The restaurant is [MASK].


Then you check what score was would good and bad get at the position of the [MASK] token.

Working with the GPT-2 model is not that straightforward as with BERT. Calling the forward method returns the hidden states of GPT-2 given the input you provided that can be further used in a model. You can use hidden states of GPT-2 as contextual embeddings, the same way that you the output of BERT, however, this is not how GPT-2 is usually used.

The usual way of using GPT-2 sampling from the model. This means that you provide a prompt (as plain text) and hope that the model will continue in a reasonable way. There are many tutorials on how to generate from the GPT-2 models, e.g., this blog post by Huggingface."
What is the difference between TextVectorization and Tokenizer?,"Tokenization is the process of splitting a stream of language into individual tokens.

Vectorization is the process of converting string data into a numerical representation."
Topic Modeling: LDA vs LSA vs ToPMine,"ToPMine Python Implementation:
https://github.com/anirudyd/topmine
pip install topmine

from topmine.phrase_lda import PhraseLDA
from topmine.phrase_mining import PhraseMining
a = PhraseMining([""you are a goode boy boy boy boy boy yes joke joke joke.""]*10+[""you are a big joke""]*20)
p = a.mine()
PhraseLDA(*p).run()
lda = PhraseLDA(*p).run()


This is an implementation of the algorithm detailed in El-Kishky, Ahmed, et al. ""Scalable topical phrase mining from text corpora."" Proceedings of the VLDB Endowment 8.3 (2014): 305-316.APA

In order to run the code, simply follow these steps:

Put the file on which you want to run topmine in the folder named ‚Äúinput‚Äù
python -m  topmine_src.run_phrase_mining input/{filename}
python -m  topmine.run_phrase_lda {num_of_topic}
example
python -m  topmine_src.run_phrase_mining input/dblp_5k.txt
python -m  topmine.run_phrase_lda 4

LDA vs LSA

Latent Semantic Analysis (LSA) is a mathematical method that tries to bring out latent relationships within a collection of documents onto a lower-dimensional space. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Rather than looking at each document isolated from the others, it looks at all the documents as a whole and the terms within them to identify relationships.

Latent Dirichlet Allocation(LDA) algorithm is an unsupervised learning algorithm that works on a probabilistic statistical model to discover topics that the document contains automatically.

LDA assumes that each document in a corpus contains a mix of topics that are found throughout the entire corpus. The topic structure is hidden - we can only observe the documents and words, not the topics themselves. Because the structure is hidden (also known as latent), this method seeks to infer the topic structure given the known words and documents."
Detecting Offensive Text Content in English and German,"Toxic comment classification challenge might be a good place to start. It contains a set of comments and 6 binary classifications indicting if it's a toxic comment and of which type.

I imagine this would be a sufficient start."
Help regarding NER in NLTK,"Training a model, related to information extraction, in general, and named entity recognition/resolution (NER), in particular, is described in detail in Chapter 7 of the NLTK Book, available online at this URL: http://www.nltk.org/book/ch07.html.

Additionally, I think that you might find useful my related answer on Cross Validated site. It has a lot of references to relevant sources on NER and related topics as well as to various related software tools."
Why do good word embeddings come out of maximizing cosine similarity?,"Training word embeddings does not rely on optimizing the cosine similarity of words. It usually relies on prediction problems. Take for instance the skipgram model: you are predicting the context of a word, given the word. Such models, project words in geometric spaces (for example the commonly used ~300 dimensions). In other words, a word is associated with 300-dimensional dense vectors. Due to the way that these vectors are learned, they capture the semantics of the words and hence similar words are close to the induced space.

Cosine similarity (or dot product) captures exactly this semantic closeness. Intuitively, we want similar words to be close, because they are similar and we hope that the space models some of the word properties and meaning."
Updating Google News Word2vec Word Embedding?,"Transfer-learning is one possible approach :

Design and implement a neural net to match Google Word2Vec's design (In terms of number of layers, activation functions and etc.,).
Pre-initialize weights with these vectors
Retrain with domain-specific corpus

This is an implementation that can be used as base and modified for step #1"
HMM and its competitive alternatives,"Transformer based architectures are some of the most popular in NLP right now. You can check this blog post for more information:

https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

Other than performance, one major advantage of transformers is that operations can be parallelized, making it much faster than RNNs/LSTMs."
ML model to transform words,"Try a totally different approach, using Generative Adversarial Networks.

For this purpose you need:

A Generator
A Discriminator

See the scheme (credit O'Reilly):

The ""Real Images"" block in the scheme should be your training dataset (or ground truth). The Generator should generate the distorted words and the Discriminator should verify if the word is ""adequately"" distorted, based on a criterion of your choice, which can be any similarity measure between known words (database) and the generated one. Both the Generator and the Discriminator get trained on-the-go while in the training phase and in the end you will have two trained networks, of which the Generator would be very useful for your purpose.

Helpful source: https://deeplearning4j.org/generative-adversarial-network"
"Is my model over-fitting (LSTM, GRU)","Try increasing your batch size.

I think it could be because you specify a batch size of 35 and the validation will be tested on a batch_size of 32 by default. Testing a batch of 32 on the weights that were just reached the epoch might indeed lead to a slightly better average performance as all samples in the batch get the newest and best current weights.

If the samples in your train and validation set are extremely similar, I would expect the validation curve to always slightly beat the train score, given you use similar batch sizes (35 vs. 32).

You can see that the train and validation curves do indeed level out over time."
How to change plot size in nltk.plot(),"Try something like this:

import matplotlib.pyplot as plt
plt.figure(figsize=(30, 20))  # the size you want

# your code goes here"
Handle with very short and very long sequences with Neural Network,"Try the following:

Check if text preprocessing is done correctly.
With different sequence lengths (Ex: 1000, 750, 500 etc.)
After the LSTM layer, try adding the ANN layers and check the results.
Check the Transformers architecture for text classification. Reference: https://ai-brewery.medium.com/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa"
Need some info regarding string matching algorithms?,"Try the simplest approach first - deterministic check looking for intersection overlap between the set of fruit names and the set of items bought.

Set comparisons are scalable because the look-up time for each item is constant.

If scaling is an issue with regular set membership check, bloom filter is an option."
Increasing SpaCy max NLP limit,"Try to raise the nlp.max_length parameter (as your error message suggests):

nlp = spacy.load('en_core_web_sm') 
nlp.max_length = 1500000 #or any large value, as long as you don't run out of RAM


also, when calling your spaCy pipeline, you can disable RAM-hungry intensive parts of the pipeline that are not needed for lemmatization:

doc = nlp(""The sentences we'd like to do lemmatization on"", disable = ['ner', 'parser'])


Finally, you should get the results you expect with the following:

print([x.lemma_ for x in doc])"
Sentiment analysis using python,"Try to search from the databases of official ""bad words"" that google publishes in this link Google's official list of bad words. Also, here is the link for the good words Not the official list of good words

For the code, I would do it like this:

textArray = file('dir_to_your_text','r').read().split()

#Bad words should be listed like this for the split function to work
# ""*** ****** **** ****"" the stars are for the cenzuration :P
badArray = file('dir_to_your_bad_word_file).read().split()
goodArray = file('dir_to_your_good_word_file).read().split()

# Then you use matching algorithm from difflib on good and bad word for every word in an array of words
import difflib

goodMachingCouter = 0;
badMacihngCouter = 0;


for iGood in range(0, len(goodArray)):
    for iWord in range(0, len(textArray)):
        goodMachingCounter += difflib.SequenceMatcher(None, goodArray[iGood], textArray[iWord]).ratio()
     
for iBad in range(0, len(badArray)):
    for iWord in range(0, len(textArray)):
        badMachingCounter += difflib.SequenceMatcher(None, badArray[ibad], textArray[iWgoodord]).ratio()

goodMachingCouter *= 100/(len(goodArray)*len(textArray))
badMacihngCouter *= 100/(len(badArray)*len(textArray))

print('Show the good measurment of the text in %: '+goodMachingCouter)
print('Show the bad measurment of the text in %: '+badMacihngCouter)
print('Show the hootnes of the text: ' + len(textArray)*goodMachingCounter)


The code will be slow but accurate :) I didn't run and test it please do it for me and post the correct code :) because I wanna test it too :)"
Extracting tokens from a document: applying Deep Learning or Classification?,"Trying to find certain things in text can be done quite easily if you have the real text, and not a scanned document that is a PDF or even an image. This is actually quite a big topic and can become quite difficult.

Pure text

If you have pure text, you can parse out parts you need using custom regular expressions, e.g. to find a date, you might use this:

^(19|20)\d\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])$


matches a date in yyyy-mm-dd format from 1900-01-01 through 2099-12-31, with a choice of four separators (source).

I believe there are even a few libraries that specifically find dates for you within text.

PDF

There are actually many types of PDF, i.e. there are many ways a pdf can be encoded behind the scenes. Some types are easier to parse that others, but luckily there are libraries that can help with that. For example, check out PDFMiner.

After using such a library, you will hopefully be left with the pure text, and can maybe go back to using methods from that section.

Images

If you are unlucky enough to have an image as a starting point, then you are now in the realms of OCR - Optical Character Recognition. I would recommend reading this blog post for a more complete description of possible methods,but in a nutshell, you can try using either:

a traditional algorithm from compute vision (applying filters and looking for edges etc.)
a trained model specialised for text (e.g. EAST: an Efficient and Accurate Scene Text Detector)
a general model

A nice model to help out with OCR is the Tesseract library.

You said you are learning NLP, so actually extracting tokens from a PDF might not be the best example with which to start. I would recommend first deciding exactly what you really want to learn and follow a course or a tutorial on that topic.area."
"For text classification, would a BoW or Word Embeddings based model ever be better than a Language Model?","Unfortunately, there is little theoretical knowledge about what complex neural networks do. Transformers are known to be universal approximations, so in theory they can learn to do any function with the input sentence, unlike the other alternatives that you mention. Most of the time, the accuracy of the BERT-like model would be strictly better.

In practice, however, everything depends on the data you have. Neural language models have very many parameters, which makes them often prone to overfitting and hard to train. Some classification problems might also be so easy that a stronger model would not help. There is also the question of computational efficiency, the accuracy gain might not be worth the slow-down from using a more complex model. BoW models might also offer better interpretability.

To conclude, there might be many situations and many reasons why smaller and simpler models might be a better choice."
What's the right input for gpt-2 in NLP,"Updated answer

After reading @Jessica's answer, I carefully read the original GPT-2 paper and I confirm that the authors do not add special tokens, but simply the text TL;DR: (be careful to include the :, which is not present in the referenced answer) after the text to summarize. Therefore, the format would be:

<|endoftext|> + article + "" TL;DR: "" + summary + <|endoftext|>

Note that in the original GPT-2 vocabulary there is no <|startoftext|> token, and neither there is in the Huggingface implementation.

Old answer

GPT-2 is a causal language model. This means that, by default, it receives either no input at all or the initial tokens of a sentence/paragraph. It then completes whatever it was passed as input. Therefore, it is not meant to be used the way you are trying to do it.

Normally, in order to do conditional text generation, people use an encoder-decoder architecture, that is, a full encoder-decoder Transformer instead of GPT-2, which only has the decoder part.

Nevertheless, while it was not meant to work the way you are using it, it is possible that this works. This kind of thing has been done before, for instance, in this NeurIPS 2018 article that uses only a Transformer decoder for machine translation, concatenating source and target sides, like you do:

You would need, nevertheless, to perform some adaptations. Specifically, the original GPT-2 vocabulary does not have the special tokens you use. Instead, it only has <|endoftext|> to mark the end. This means that if you want to use your special tokens, you would need to add them to the vocabulary and get them trained during fine-tuning. Another option is to simply use <|endoftext|> in the places of your <BOS>, <SEP> and <EOS>.

For GPT-2, there is only a single sequence, not 2. Therefore, the maximum token length would apply for the concatenation of text and reference summary.

P.S.: I think that your use of <SEP> comes from the fact that other non-generative models like BERT, use similar special tokens ([SEP], [CLS]) and are specifically designed to receive two concatenated segments as input. However, BERT is not a generative language model, in the sense that it was not trained in an autoregressive manner, but with a masked LM loss:"
Weights shared by different parts of a transformer model,"Updated answer

The Transformer model has 2 parts: encoder and decoder.

Both encoder and decoder are comprised of a sequence of attention layers. Each layer is comprised of a combination of multi-head attention blocks, positional feedforward layers, normalization, and residual connections.

The attention layers from the encoder and decoder are slightly different: the encoder only has self-attention blocks while the decoder alternates self-attention with encoder attention blocks. Also, the self-attention blocks are masked to ensure causal predictions (i.e. the prediction of token N only depends on the previous N - 1 tokens, and not on the future ones).

In the blocks in the attention layers no parameters are shared.

Apart from that, there are other trainable elements that we have not mentioned: the source and target embeddings and the linear projection in the decoder before the final softmax.

The source and target embeddings can be shared or not. This is a design decision. They are normally shared if the token vocabulary is shared, and this normally happens when you have languages with the same script (i.e. the Latin alphabet). If your source and target languages are e.g. English and Chinese, which have different writing systems, your token vocabularies would probably not be shared, and then the embeddings wouldn't be shared either.

Then, the linear projection before the softmax can be shared with the target embedding matrix. This is also a design decision. It is frequent to share them.

Finally, the positional embeddings (which can be either trainable or pre-computed) are shared for the source and target languages.

Old Answer (mistakenly addressing multi-encoder/decoder transformers)

In general, parameters are not shared in multi-encoder/multi-decoder transformer architectures, in the sense that each encoder/decoder has its own parameters. This is because sharing parameters defeats the very purpose of having multiple encoders/decoders: if you have two encoders that share parameters, they are effectively the same encoder.

This, of course, is not a rule, and there may be cases where it makes sense to share some or all encoder/decoder parameters. An example could be a scenario where two sentences in a certain language (or two very similar languages, like dialects) are received as input. It may be beneficial to share parameters between both encoders if the amount of data in one of them is not enough.

Background: multi-encoder and/or multi-decoder Transformers are generally not used for ""simple"" sequence to sequence tasks like machine translation. There are certain special cases where this kind of architectures have been used with different purposes, e.g.:

Automatic Post-Editing (APE) consists of having a primary machine translation system which output is refined with a secondary translation system that corrects the errors from the primary system. The secondary system normally receives as inputs both the original source sentence and the translation from the primary system and generates the fixed translation as output. An option for this kind of scenario is having a dual-encoder single-decoder transformer, where the output of the encoders are both fed to the decoder, either concatenating them (example) or injecting them to different attention blocks (example). In this scenario, normally there is no parameter sharing at all.

Multimodal translation: in this case, we receive different data modalities (e.g. speech and text), so we need modality-specific architectures, so normally there is no parameter sharing.

Note that here I understand that the multiplicity in encoders and/or decoders happens both at training and inference time. There are other cases where such a multiplicity happens only at training time (as in multi-task learning), but at inference time only one encoder and decoder are selected for each case, e.g.:

Multilingual translation: while many multilingual MT systems have a single model for all supported languages (normally supplying special tokens that identify the source/target language), it is possible to have language-specific encoders or decoders. You can see examples of this kind of architecture in works like this or this. In this scenario, each encoder/decoder has its own parameters and there is no sharing."
What can be done so that 'teacher' and 'teaches' are treated similar?,"Use an aggressive stemmer. The Lancaster Stemmer is one the most aggressive and popular stemmers around.

Here is the Python code:

from nltk.stem.lancaster import LancasterStemmer

lancaster_stemmer = LancasterStemmer()
assert 'teach' == lancaster_stemmer.stem('teacher') == lancaster_stemmer.stem('teaches')"
word/sentence alignment for English document,"Use kaldi's align-text which align two sentences using Levenshtein distance.

Code: https://github.com/kaldi-asr/kaldi/blob/master/src/bin/align-text.cc"
Supervised learning model for extracting terror attack motives from attack summary (NLP),"Use word embedding and encode the entire sentence into one fixed feature vector by using vanilla RNN or more sophisticated model like attentional LSTM.
see Sentiment analysis using RNNs(LSTM)
Concat the other features with the fixed length representation of the sentence and append on top of them either dense layer.
the architecture feedforward(concat(other_features,RNN(sentence)))"
Organization finder in spaCy,"Using dependency parsing alone will not give you what you need. You may be able to get your answer by interpreting the dependency tree. For instance, in this case ABC-EFG GROUP Inc. is a pobj of between, which could infer that it is party 1. In this particular case, the dependency parsing isn't fully correct and party 2 (Rob Cummins) is difficult to find.

I would recommend you try the following things:

It seems that the parties are either organization or people, so you could narrow down your candidates to proper nouns and you could use NER to see whether any ORG or PER are detected. That will not tell you exactly whether they are the parties, but they could give you useful hints.

In this case, it feels like a plain regular expression could work. Whatever proper noun occurs after between seems to be party 1, and whatever follows and seems to be party 2. Of course, a regular expression would only work for cases written with the same pattern, but it is always wise to consider. Perhaps, you are already a fair amount of your scenarios with a solution that will always work. You could also use the presence of words like ""between"" and ""and"" as a feature.

If you don't have a lot of training examples, you could take a look at libraries like mimesis and faker. They allow you to generate company names, addresses, people names, etc."
NER vs Text classification for very short sentences,"Using NER (more generally sequence labeling) means classifying every token in the sentence, so if the goal is only to label every sentence there's no strong need for it in your case.

However NER might be more appropriate in case the order of the words is important, because sequence labeling models take it into account whereas traditional text classification methods often use a ""bag of words"" representation (order doesn't matter).

To some extent it also depends whether the labels are always related to a particular term in the sentence: if yes, then NER might be better at locating these terms (this is related to the point about order). If no, then classifying at the level of sentences is likely to perform better."
Can I use Sentence-Bert to embed event triples?,"Using triples could lead to wrong results because some headlines could contain double negations or other complex structures that are difficult to classify with triples.

However, you can apply directly on the headlines Bert sentiment analysis instead, which can process complex semantics correctly.

Here is an example using Bert's twitter roberta sentiment analysis:

Note: in this specific case neutral and positive have almost the same value, and you will want to set some threshold to consider a headline as positive, like positive > 0.4. It could also require some fine tuning because tweets are a bit different from headlines.

You can even apply sentiment analysis levels (very negative, negative, neutral, positive, very positive) to get even better predictions."
Effect of Stop-Word Removal on Transformers for Text Classification,"Very interesting question.

Easy, but probably lazy answer

When using pre-trained models, it is always advised to feed it data similar to what it was trained with. Basically, if it matters, don't remove them, and if it doesn't matter, it doesn't hurt to keep them in. Obviously, if you can, try with or without stopwords, and see what works best for your problem.

Longer answer

You actually have two ways to ""remove"" your stopwords. Either you remove them from the input sequence altogether. Or, you could replace them with a mask token (i.e. <#UNKNOWN> or <#MASK>).

In the latter case, the transformer will implicitly guess what these masks are and you will have achieved the original goal of stopwords removal: make sure they don't affect the predictive outcome. Indeed, take the sentences:

""I like basketball with an audience"" & ""I like basketball without an audience""

These two sentences are both about basketball and you wouldn't want with/without to make your model think these sentences are about different topics. By masking with and without, you both ""remove"" the stopwords, and you don't potentially mess with the fact that the pre-trained model didn't use data without stopwords.

Now, what would happen if you fed an ""incomplete"" sentence to a transformer. The positional encodings would retain the notion that certain words are before or after each other, which is what you want. But if you remove stopwords, some words may appear ""too close"" to each other. But does that matter? I do not think so.

If the vector your transformer outputs for ""word1 <#Mask> word2"" is widely different than the one for ""word1 word2"", it must mean that the masked token is crucial to the overall meaning of the sentence, which would also suggest that it shouldn't be a stopword to begin with.

Final answer

I would suggest masking the stopwords instead of removing them. But, if performance is so critical that you need to feed smaller sequences, I think that you will be alright."
How does word2vec handle the input word being in the context?,"We can look at the source for guidance.

How does word2vec handle the input word being in the context?

It is skipped; for both the skip-gram and CBOW models.

If word2vec encounters the same word multiple times in the same window, what occurs?

The relationship is strengthened."
How to obtain vector representation of phrases using the embedding layer and do PCA with it,"We can use the below code to fetch the embeddings for each word

 words_embeddings = {w:embeddings[idx] for w, idx in tokenizer.word_index.items()}

res_vectors = np.empty((0, 2), float)
words = []
for k,v in words_embeddings.items():
  print(k,""-->"", v)
  words.append(k)
  res_vectors = np.append(res_vectors, [v], axis=0)


Since each word is represented as a 2D vector, I have not reduced the dimensionality of the vector. With the below code we can get the word representations.

import matplotlib.pyplot as plt
plt.figure(figsize=(13,7))
plt.scatter(res_vectors[:,0],res_vectors[:,1],linewidths=10,color='blue')
plt.xlabel(""PC1"",size=15)
plt.ylabel(""PC2"",size=15)
plt.title(""Word Embedding Space"",size=20)
for i, word in enumerate(words):
  plt.annotate(word,xy=(res_vectors[i,0],res_vectors[i,1]))


To get better results, try to increase the vector dimensions of each word.

If we use a 100-dimensional vector for a word. We can make use of PCA as below.

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
res_vectors = pca.fit_transform(res_vectors)
print(res_vectors)


Word representations in this case. 

We can get the phrase embeddings as below:

doc_vecs = np.empty((0,2), float)
for i, doc in enumerate(padded_docs):
  vec = np.empty((0,2), float)
  for token in doc:  
    vec = np.append(vec, [embeddings[token]], axis=0)
  vec = vec.mean(axis=0)  
  print(docs[i], ""-->"", vec)
  doc_vecs = np.append(doc_vecs, [vec], axis=0)


Phrase Representation:

import matplotlib.pyplot as plt
plt.figure(figsize=(13,7))
plt.scatter(doc_vecs [:,0],doc_vecs [:,1],linewidths=10,color='blue')
plt.xlabel(""PC1"",size=15)
plt.ylabel(""PC2"",size=15)
plt.title(""Phrase Embedding Space"",size=20)
for i, doc in enumerate(docs):
  plt.annotate(doc,xy=(doc_vecs [i,0],doc_vecs [i,1]))"
Why do we calculate the vector of a document by averaging the vectors of all the words?,"we should obtain the vector of a document by averaging the vectors of all the words

This is not necessarily the case. But surely it is a convenient approach. The main advantage in particular, is to avoid issues due to different lengths for different documents. By obtaining a single final vector, we make sure we can compare any document of any length. Concatenating or performing other operations with the word vectors would probably force you define a max length and pad shorter documents / trim longer documents. A final note is that usually it's always a good practice to remove stop words from the documents, i.e. most frequent words which don't provide much semantic meaning.

May I know how does the vector of all the words in the document retain the information of the words?

This really depends on how you obtain the words vectors. If you just perform one hot encoding then performing an average is actually meaningless, since you would generate real numbers out of binary representations. So I assume you're planning to use embeddings generate through word2vec, skipgram, glove or other deep learning models. In that case to understand why averaging provide useful information you need to understand first how these models turn words into vectors. The extensive explanation is beyond the question so to keep it short: dense representation allows to do simple math with words. When translating words into dense representation similar words will be turned into similar (close in space) vectors. Of course there would be differences depending on the chosen model. For example Skipgram is better in capturing semantic more than word2vec, which in contrast still encode quite a lot of grammatical similarities, so if comparing two document talking about celebrities planets, both will probably contain the word ""star"", but a skipgram model would probably be able to better distinguish the documents since start would have more skewed values in the dimensions encoding both domains, and the other words in the document would provide the information to boost the right dimension, whereas a more grammatical model would have a harder time distinguishing the documents since grammatically, start and similar words are used in a similar fashion.

Would it be better if i retrieved similar words of the query and checked if these words were in each document?

You surely can try that, but it would hardly perform better than using any dense representation. Reason being that words themselves provide no information at all about the contextual relationship between them. For example ""apple"" could be present in a shop list, a review of an Apple product, or it could even be used as a slang for drugs."
What to do if training loss decreases but validation loss does not decrease?,"Welcome to DataScience. This looks like a typical of scenario of overfitting: in this case your RNN is memorizing the correct answers, instead of understanding the semantics and the logic to choose the correct answers.

A typical trick to verify that is to manually mutate some labels. For instance, you can generate a fake dataset by using the same documents (or explanations you your word) and questions, but for half of the questions, label a wrong answer as correct. If you re-train your RNN on this fake dataset and achieve similar performance as on the real dataset, then we can say that your RNN is memorizing.

Note that it is not uncommon that when training a RNN, reducing model complexity (by hidden_size, number of layers or word embedding dimension) does not improve overfitting.

If it is indeed memorizing, the best practice is to collect a larger dataset. I understand that it might not be feasible, but very often data size is the key to success. If you haven't done so, you may consider to work with some benchmark dataset like SQuAD or bAbI."
Where can I learn the complete mathematics involved in LDA?,"Welcome to DS.SE!

I believe the Latent Dirichlet Allocation paper by Blei, Ng, and Jordan (all big names in AI/ML/NLP) is a point to start. They provided the math foundation as well as multiple examples.

See: Blei, David M., Andrew Y. Ng, and Michael I. Jordan. ""Latent dirichlet allocation."" Journal of machine Learning research 3.Jan (2003): 993-1022."
Is there an NLP corpus that contains common medical terms?,"Welcome to the biomedical domain, one of the few domains in NLP where there are too many resources to choose from :)

Data resources:
Medline is a database corpus of 30 millions abstracts.
Each Medline abstract is annotated with Mesh descriptors, Mesh being a structured hierarchy of medical concepts.
PubMed Central (PMC) is a database of around 6 millions full biomedical articles (not only abstracts).
UMLS is a database of millions of medical terms grouped by concept, themselves grouped by semantic type (e.g. disease, gene, etc.)
PubTator is a resource which provides all the Medline and PMC documents fully annotated with a combination of Mesh and other ontologies.
Software tools:
MetaMap is the venerable annotator system which annotates any medical text with UMLS labels.
cTakes is another annotator system which is more specialized with clinical texts.
SciSpacy is a Spacy variant specialized for biomedical text. It can also annotate medical terms with UMLS labels.

I think that's all the main ones that I know of... so far.

From your description it looks to me like you probably just need cTakes or SciSpacy. In case you're going to start working with Medline or PMC, be aware that these datasets are massive (a few hundreds GBs)."
How to preprocess data for Word2Vec?,"Welcome to the community,

I do not know about other libraries, but gensim has a very good API to create word2vec models. In order to preprocess data, you have to decide first what things you are gonna keep in your vocab and whatnot. for ex:- Punctuations, numbers, alphanumeric words(ex - 42nd) etc.

In my knowledge, the most generic preprocessing pipeline is the following:-

1) Convert to lower 2) Remove punctuations/symbols/numbers (but it is your choice) 3) Normalize the words (lemmatize and stem the words)

Once this is done, now you can tokenize the sentence into uni/bi/tri-grams.

Have a look at this

The generic format to put data in gensim.models.word2vec()'s sentence parameter is : [[tokeneized sentence 1], [tokenized sentence 2].....and so on]

Hope it helps, thanks!!"
Preprocessing text before use RNN,"Welcome to the Data Science forum.

Yes, data preprocessing is an important aspect of sentiment analysis for better results. What sort of preprocessing to be done largely depends on the quality of your data. You'll have to explore your corpus to understand the types of variables, their functions, permissible values, and so on. Some formats including html and xml contain tags and other data structures that provide more metadata.

At a high level the sentiment analysis (using bag of words) will involve 4 steps:

Step 1: Data Assembly
Step 2: Data Processing
Step 3: Data Exploration or Visualization
Step 4: Model Building & Validation (train & test)

Lets understand different possible data preprocessing activities:

Convert text to lowercase ‚Äì This is to avoid distinguish between words simply on case.

Remove Number ‚Äì Numbers may or may not be relevant to our analyses. Usually it does not carry any importance in sentiment analysis

Remove Punctuation ‚Äì Punctuation can provide grammatical context which supports understanding. For bag of words based sentiment analysis punctuation does not add value.

Remove English stop words ‚Äì Stop words are common words found in a language. Words like for, of, are etc are common stop words.

Remove Own stop words(if required) ‚Äì Along with English stop words, we could instead or in addition remove our own stop words. The choice of own stop word might depend on the domain of discourse, and might not become apparent until we‚Äôve done some analysis.

Strip white space ‚Äì Eliminate extra white spaces.

Stemming ‚Äì Transforms to root word. Stemming uses an algorithm that removes common word endings for English words, such as ‚Äúes‚Äù, ‚Äúed‚Äù and ‚Äú‚Äôs‚Äù. For example i.e., 1) ‚Äúcomputer‚Äù & ‚Äúcomputers‚Äù become ‚Äúcomput‚Äù

Lemmatisation ‚Äì transform to dictionary base form i.e., ‚Äúproduce‚Äù & ‚Äúproduced‚Äù become ‚Äúproduce‚Äù

Sparse terms ‚Äì We are often not interested in infrequent terms in our documents. Such ‚Äúsparse‚Äù terms should be removed from the document term matrix.

To give you more insight onto the steps involved, here are some example sentiment analysis using logistic regressions codes https://github.com/srom/sentiment

https://github.com/jadianes/data-science-your-way/blob/master/04-sentiment-analysis/README.md

Hope this helps."
Why is data science not yet widely applied to Law? [closed],"Welcome to the site and thanks for the great question! I recently led an NLP project that dealt with a lot of laws. While I have to obfuscate my actual work, here's a general view:

The laws themselves may not be the best source data. It would take a massively transformed recordset in order to make most laws actionable for modeling. I'm talking about big rooms, full of lawyers providing an annotated version of laws in order to create a recordset that can actually be useful
The above assumes that the laws have been digitized in some easy to digest format. That may not always be the case. In a lot of instances, you are referring back to classic OCR approaches as part of your data prep and I don't know anyone that likes working with OCR :-)
The human-in-the-loop requirements are very high. So you have an algorithm, now what? That's not something you can just put out on Mechanical Turk for the layman to verify. You need more lawyers to help with the verification of your approach and correct mistakes that are happening
Finally, you must get very sophisticated with your embedding layers in how you create and apply them. That's not an easy thing to do and very processor intensive - a GPU is highly recommended and not a lot of grassroot efforts are going to have this processing power

Good luck!"
Detect sensitive data from unstructured text documents [closed],"Welcome to the site! Assuming that I understand your problem correctly, I think you can achieve a working model.

If I was in your position I would:

Obtain the cleanest data possible from the documents. For example, you don't state if the docs are already in simple text or if you need to do something like OCR or whatnot. Having the cleanest set possible will be key for this.
Make sure you have a consistent marker for the sensitive data. You mention four dots - is that the case for ALL instances? If not, clean that data now
You're going to need to do standard NLP cleansing stuff like removing punctuation but you may or may not want to keep stop words (this will be part of your model testing). Also, this is key, be 100% certain that the four dots are viewed as a single work in your tokenization process - you should be able to verify this prior to committing to your tokenization file.
I would take all my documents and create 3 word ngrams. I would then separate out ngrams that contain sensitive data and not sensitive data. That, essentially, becomes your labeled dataset and you should label them accordingly.
My base model would use all entries that contain sensitive data in the second position of the ngram (the middle of the three words). I would train a neural network on that and see what kind of results I achieve with that. NOTE that your four dots will not be an input, only the word previous and after will be your inputs. You could almost treat this as a binary classification model - the middle word is either sensitive or it's not.
Future iterations of my model would maybe use a multi-classification approach with something like (1) No sensitive data (2) sensitive data in first position (3) sensitive data in second position and (3) sensitive data in third position and so on and so on.
From there, you can play with variations on the size of the ngram since the immediate words may or may not actually have an effect on the predictions. There's no limit to how crazy you can get with this - you won't know until you start modeling.

Finally, your entire project becomes even more interesting when you go to the prediction phase with new data. You will do the same and break down your document into ngrams and create a prediction for each one and output the result. In other words, you will need to break down your document only to turn around and build it up again - that should be a fun script to write! Good luck with this, let us know how it turns out."
Which neural network to choose for classification from text/speech?,"Welcome to the site. I'm a little disturbed by the other two answers you received here. It sounds like you are skipping a whole lot of steps and wanting to jump right into modeling - that's a massive mistake!

You are a scientist! Your role is to create the most fair, unbiased environment possible to let the data speak to you (not the other way around!). What worked before (LSTM) may or may not be the best approach to this completely new data set. Therefore, after doing your EDA phase, you should keep an ""open field"" view to the multiple models that you will examine and test prior to making any decisions about which model to proceed with. The answer may not even be a neural network, it may be a whole different approach.

Please, be responsible your data science practice. You cannot jump into modeling right away. Let the data speak to you."
NLP grouping word categories,"Well ... the simplest approach is using Fuzzy String Matching and it will work. Just go through the examples in python implementation of it (fuzzywuzzy) and you will understand how it works. You need to find a threshold by practice to determine if two strings are similar enough to be considered as same concept.

If it didn't work please drop a line in the comments so I can propose more sophisticated algorithms.

Good luck!"
Automating 3D Modeling using NLP,"Well sure it's doable: the NLP part is just speech recognition + extracting the formal command from the text, it's very similar to ""virtual assistants"" like Apple Siri, Amazon Alexa, Ok Google.

However the hard part is to formalize all the possible commands that can be given, and then train a model to correctly map voice commands to software instructions.

I'm not convinced that it's very useful, because giving detailed instructions like ""move object to position x=3.45678, y=-9.8765, z=1.2345"" is not as intuitive as using a mouse to move the object. There's a fallacy in imagining that language commands are easier than learning to use a software or programming language: it works for very simple tasks, but as soon as one needs a bit of precision general language is too ambiguous."
Word2Vec vs. Sentence2Vec vs. Doc2Vec,"Well the names are pretty straight-forward and should give you a clear idea of vector representations.

The Word2Vec Algorithm builds distributed semantic representation of words. There are two main approaches to training, Continuous Bag of Words and The skip gram model. One involves predicting the context words using a centre word, while the other involves predicting the word using the context words. You can read about it in much detail in Mikolov's paper.

The same idea can be extended to sentences and complete documents where instead of learning feature representations for words, you learn it for sentences or documents. However, to get a general idea of a SentenceToVec, think of it as a mathematical average of the word vector representations of all the words in the sentence. You can get a very good approximation just by averaging and without training any SentenceToVec but of course, it has its limitations.

Doc2Vec extends the idea of SentenceToVec or rather Word2Vec because sentences can also be considered as documents. The idea of training remains similar. You can read Mikolov's Doc2Vec paper for more details.

Coming to the applications, it would depend on the task. A Word2Vec effectively captures semantic relations between words hence can be used to calculate word similarities or fed as features to various NLP tasks such as sentiment analysis etc. However words can only capture so much, there are times when you need relationships between sentences and documents and not just words. For example, if you are trying to figure out, whether two stack overflow questions are duplicates of each other.

A simple google search will lead you to a number of applications of these algorithms."
How can I group texts with similar content together?,"Well, after further googling I found the solution: MinHash or SimHash will do the job and I also found a tool implementing MinHash written in Scala on GitHub right at this link"
What are tokens and tokenizations?,"Well, I think it is better to watch a course about it. But, for instance, when people comment on different products on Amazon, there is a way to understand are their comments positive or negative, therefore their comments are used as data, analyzed and then labeled as positive or negative. Further, using this approach, one can find whether an email is spam or not, etc.

In order to analyze a text, we need to split it into its elements, meaning, its words (usually) each of these elements is called a token.

after splitting the text to words, useless words that do not represent anything, some signs, etc (the, ""."", ""my"",...) are omitted, then we create bags of words, that are the most important words in the whole dataset (all comments), then each word is a column in the table and each row of the table is a comment and has 1 in a column if it contains that specific word. there is also a label column that shows if the comment is positive or not. then having this dataset, we can fit a classification model to the dataset."
Is binary classification the right choice in this case?,"Well, I think that you identified the options and problems quite well.

The main problem in this kind of text classification task is that it's impossible to obtain a representative sample of the negative class.
Binary classification is certainly a reasonable option, but since a classifier learns to separate the two classes there's always a risk that some future negative example won't look like any of the training examples and end up misclassified.
One-class classification is also a reasonable option. By definition it's supposed to handle the open classification problem better, but it's not always the case in practice.
Calculating a similarity measure against the reference documents is possible, but it's not efficient so rarely convenient. The performance also depends a lot on the data and measure chosen, and of course there is the problem of determining an optimal threshold.
The class imbalance should probably not be treated by resampling, it's not going to solve anything. Imbalance is a problem only because the model doesn't find good indications in the features, so imho the only good option is to investigate and understand why the features don't help the model in cases of errors. Very often it's because the minority class is simply not representative enough."
How do we evaluate the outputs of text generation models?,"Well, you missed the old good human evaluation, which is the only actual measure that can actually be trusted in terms of semantic evaluation. Also, in the reference n-gram matching area you missed BLEU, which is the standard evaluation measure in machine translation, and METEOR, which applies stemming and handles synonyms to avoid the surface word problem you mentioned.

Apart from that, in the space of embedded vector approaches, it is worth mentioning BERTscore, which seems to work very very well. This is the abstract of the article, published at ICLR'20:

We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.

These are some references to better understand BERTscore's measures and their traits:

A Fine-Grained Analysis of BERTScore (WMT'21)
A Study of Automatic Metrics for the Evaluation of Natural Language Explanations (EACL'21)
BERTScore is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation (EMNLP'22)

Also worth mentioning the WMT Metrics Shared Task, which aims at devising automatic evaluation metrics for machine translation. There, each year you can find practical comparisons of the submitted evaluation metrics."
Stackoverflow API Structure data storage,"What do you want to define as your document? You could define a document as a single question, each question-answer pair, or a question with all its answers. How you define a document depends on why you want to cluster documents and how you plan to use this information. From reading your question, my guess is that you want to define a question with all its answer as a document.

Next, it sounds like you want to apply bag of words techniques to extract features (e.g. tf-idf). If so, you would first want to concatenate the question title, question body, and each answer body into one string object. Then I would suggest using a Python package like scikit learn or nltk to do the other preprocessing steps. For example in scikit learn you can then apply TfidfVectorizer to convert a string to a vector.

This is unrelated to your question, but it may be easier to use the data explorer instead of the StackOverflow API. The data explorer allows you to download questions and answers in a large batch and does not have a quota. Unless you need to collect data in real time, the data explorer may be more efficient. It is updated every week and you can use SQL queries to output data tables as csv."
What can I use to post process an NLP tree generated from the python library `spaCy`?,"What do you want to do with the chat bot? How you parse it will depend on the final use case and, believe it or not, many people get the job done by simply collecting the POS they want and using some filtering.

If you want to try to maintain more of the data and perhaps abstract it makes sense to try clustering of some kind, perhaps using hierarchical methods, such as the (relateively new) hdbscan. The features on which you cluster will again depend on what you want to achieve.

If you haven't already, check out the spaCy examples for some inspiration!

Once you have a corpus with word all tagged, you can try training models that might be able to answer questions, based on user input. This will involve steps such as creating encoding of the words (or entire user questions), using embeddings such as Word2Vec, GLoVe, or simple sparse one-hot encodings. You basically need to transform words into numerical input somehow.

I hope this gives you some keywords to help you on your search :-)"
Classification of substrings?,"What is the appropriate method to find n-grams/sub-phrases/parts-of-sequences that are referring to a specific topic or belong to a certain category?

An important question to solve this problem would be: what is the range of input topics? Is the topic selected among a predefined closed list? Can it be any search query?
A similar question might be asked about the target documents and/or terms: can they be processed so that any candidate term is extracted in advance, and the task only consists in identifying the right terms for the particular topic?

Assuming the most open variant of the question (i.e. nothing is available beforehand), I think that one would need:

a terminology extraction system which extracts any candidate term from the text (preferably specific to the data to be processed).
a third-party resource in order to calculate a semantic representation (typically a vector) of any possible term including the input topic query, so that the topic can be matched/compared against any term."
Continuous Bag Of Words (CBOW) network architecture?,"What is the proper architecture to train CBOW encodings?

The original paper by Mikolov et al uses 1 hidden layer. However, for NLP tasks (and deep learning in general), there is no ""correct"" number of layers to use.

For some tasks, you might find using more hidden layers to be better, for other tasks maybe one hidden layer is sufficient. The number of layers is a hyperparameter, along with other 'design choices' like the dimension of the embedding vectors, the size of the vocabulary, and many, many more.

If this multiple hidden layer approach is correct, how do you not lose semantic information when you only use one of the layers as the encodings?

I think of it this way. When training CBOW, the hidden layers learn some 'relationship function' between the input context words and the output target word. In order for the 'relationship function' to perform well, the embeddings must be arranged accordingly (in N-dimensional space), and it just happens that the optimal arrangement encodes semantic information.

So the hidden layers shouldn't really have anything to do with semantic information. However, because of the immense modeling power of hidden layers (which make up neural networks), the truth is the embeddings will inevitably lose some semantic information to the hidden layers.

If the single hidden layer approach is correct, does anyone have examples of this being implemented using this approach in PyTorch (fine if no)?

Here's literally the 1st github example off Google."
Overfitting while fine-tuning pre-trained transformer,"What makes you think your model is overfitting? Are you concerned about the difference between the training loss and validation loss?

If so, this is not overfitting. Overfitting is when the weights learned from training fail to generalize to data unseen during model training.

In the case of the plot shown here, your validation loss continues to go down, so your model continues to improve its ability to generalize to unseen data.

Once your validation loss starts creeping upward, then you have begun to overfit.

See chapter 5 of Jeremy Howard's Deep Learning for Coders with fastai and PyTorch for more details. https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"
How could I improve my classifier of text data?,"What type of information would you see as contact information? If it's just phone number and email address I would probably use a simple rule based classifier using (regex) pattern matching, which will probably already get you quite far. In addition you could use specific keywords that are often related to contacting someone such as call, send, mail, etc., for this you can use your existing training dataset to see what these types of words are for your use case."
Problem of continuous training - Supervised learning,"What you are describing is called auto-adaptive learning. This is what most recommendation systems use to adapt to ever changing data and feedback. It is also known as autoML. This https://towardsdatascience.com/how-to-apply-continual-learning-to-your-machine-learning-models-4754adcd7f7f Article does a good job of explaining it. Based on what your data looks like, you might have to choose the appropriate retraining strategy and do a staggered deployment."
Extract date/duration from text,"What you are trying to do here is named-entity recognition. Namely, the task consists of classifying substrings into a set of named entities (i.e. person, location, etc.). From a more formal perspective, this is a sequence labeling task that classifies parts of a sequence.

This task can be approached in different ways:

gazetteers/string matching
regular expressions
machine learning

I would highly recommend using SpaCy. They allow you to customize and retrain a model with your own data and labels, and typically, for such a use case, the model will perform well without requiring massive amounts of data."
to include first single word in bigram or not?,"What you describe is called padding and is indeed used frequently in language modeling. For instance if one represents the sequence ""A B C"" with trigrams:

# # A
# A B
A B C
B C #
C # #


The advantages of padding:

it makes every word/symbol appear the same number of times whether it appears in the middle of the sequence or not.
it marks the beginning and end of a sentence/text, so that the model can represent the probability to start/end with a particular word."
Is there any NLP framework/ algorithm which is able to derive relationships between different entities?,"What you describe touches on several tasks in NLP:

semantic role labeling
relationship extraction
paraphrase detection and textual entaiment

I might not be aware of the recent developments on these tasks, but as far as I know this is a very complex problem for which so far there is no satisfactory solution."
NLP: Information extraction,"What you need is perhaps Named Entity Recognition with custom entity dictionary. See this example:

Many packages like NLTK or Spacy have a large dictionary of such entities that enable models to identify them without using regular expressions. However, those pre-defined entities often is not applicable to one's application, thus won't recognize what you are looking for. That is why you need to spend time to update/add a list of such entities (products in your case). Also you mentioned that there are misspelling, variations etc. that make it harder. Here is where you need to expand your dictionary for such products with a fuzzy-matching algorithms to account for such edge cases programmatically.

You may start reading this article to the idea. More about Named Entity Recognition: a Blogpost, or the official Named Entity Recognition 101 by Spacy. Spacy supports adding products name in the Entity list, see page. Check out this question in stackoverflow too. But it is not easily available as you may have thought!!"
Classify text as logical/ not logical,"What you need is simply a language model. This is a very common task so you should be able to find code and data easily. This question gives some pointers for Python (be careful, the accepted answer is incorrect according to the two other answers).

Applying the language model to a sentence gives you a probability (or a perplexity score, which works the opposite way), so you have to define a threshold in order to classify as real language or not."
How to retrain Glove Vectors on top of my own data?,"What you should do is:

Create a new instance of a GloVe model with the old_words and new_words as vocabulary.
Replace the initial vectors/biases of the old_words with the ones you have already.
Train this model on mydata.txt.

The new old_words representations won't be the same but will be highly influenced by the old ones."
What ML techniques can be used to determine context across sentences?,"What you want to do is to extract the relation within the sentence and in this case beyond - so basically a cross-sentence relation extraction.

Ambiguity and the context of words is a big deal when doing this kind of task. There might be multiple ways to get there, but it also depends on your goal (production environment, research or just doing it for fun to get into NLP).

I would recommend you looking into Graph LSTMs (e.g. this Paper)"
Build a corpus for machine translation,"What you would typically do in your case is to apply a sentence alignment tool. Some popular options for that are:

hunalign: a classical tool that relies on a bilingual dictionary.
bleualign: it aligns based on the BLEU score similarity
vecalign: it is based on sentence embeddings, like LASER's.

I suggest you take a look at the preprocessing applied for the ParaCrawl corpus. In the article you can find an overview of the most popular methods for each processing step.

A different option altogether, as you suggest, is to translate at the document level. However, most NMT models are constrained in the length of the input text they accept, so if you go for document-level translation, you must ensure that your NMT system can handle such very long inputs. An example of NMT system that can be used for document-level NMT out of the box is Marian NMT with its gradient-checkpointing feature."
Extract most informative parts of text from documents,"What you're describing is often achieved using a simple combination of TF-IDF and extractive summarization.

In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its ""importance."" Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary.

Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: A Smattering of NLP in Python."
Summing three lexicon based approach methods for sentiment analysis?,"What you're proposing is some very simple form of ensemble learning. You need to have at least a sample of labelled data in order to evaluate any method. Using this labelled data you can:

evaluate each of the three methods on their own
evaluate your idea of averaging the 3 methods predictions
if you have enough labelled data, you could even train a model which combines their predictions optimally (this would be full-fledged stacked generalization)"
Integration of NLP and Angular application,"When deploying a model into production, similar data transformation steps have to be taken during training and prediction.

Scikit-learn has a Pipeline class that makes it more straightforward to do that.

Something like:

import pickle

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes             import GaussianNB
from sklearn.pipeline                import Pipeline

pipeline = Pipeline(
    [
        (""vect"", CountVectorizer()),
        (""clf"", GaussianNB()),
    ]
)

# Training
pipe.fit(X_train, y_train)
saved_model = pickle.dumps(clf)

# Predicion in app
pipe = pickle.loads(saved_model)
pipe.predict(X)"
Sentiment Analysis: Creating dictionary from dataset,"When looking on the probability of word occurrence, you will get stop words and other popular words. You are interested in words that appear more in the comments (assumed hate related) than in normal use.

Get a neutral resource (e.eg., a German newspaper, the German wikipedia, maybe Google ngrams for German). Compute the probability on the neutral source 
P
neutral
(word)
ùëÉ
ùëõ
ùëí
ùë¢
ùë°
ùëü
ùëé
ùëô
(
ùë§
ùëú
ùëü
ùëë
)
, the probability of the comments 
P
comments
(word)
ùëÉ
ùëê
ùëú
ùëö
ùëö
ùëí
ùëõ
ùë°
ùë†
(
ùë§
ùëú
ùëü
ùëë
)
 and look for the words of lift 
P
comments
(word)
P
neutral
(word)
>1
ùëÉ
ùëê
ùëú
ùëö
ùëö
ùëí
ùëõ
ùë°
ùë†
(
ùë§
ùëú
ùëü
ùëë
)
ùëÉ
ùëõ
ùëí
ùë¢
ùë°
ùëü
ùëé
ùëô
(
ùë§
ùëú
ùëü
ùëë
)
>
1
. These are the words that are more popular at the comments.

As @chi wrote, many repositories can both give you a head start and help you tune the needed lift threshold (you might want words that appear much more often in the comments).

After this phase you might need do do a finer analysis. For example, I guess that there will be politicians names that will appear more often in the comments. See here for a possible approach."
Stemming/lemmatization for German words,"When spacy can't find the model just download it with ""python -m spacy download de_core_news_sm"".

For your second problem, you should use stemming or lemmatization depending on your use case. If you have a lot of words and not enough processing power stemming will be the better solution. That should also solve your Katze/Katzen problem.

With lemmatization you can also aggregate words together that have the same meaning but not the same word stem. For example, ""besser"" would be transformed to ""gut"". So this really depends on how much detail you want."
Combining textual and numeric features into pre-trained Transformer BERT,"When they are talking about aggregating their outputs, they mean the final embeddings (just before the classification layer) not the output of the network itself.

You take the embeddings from both of the networks and concatenate them vertically. This concatenated embedding you use to predict the final output you wanted.

Torch has a function torch.cat for this purpose."
When to use cosine simlarity over Euclidean similarity,"When to use cosine similarity over Euclidean similarity

Cosine similarity looks at the angle between two vectors, euclidian similarity at the distance between two points.

Let's say you are in an e-commerce setting and you want to compare users for product recommendations:

User 1 bought 1x eggs, 1x flour and 1x sugar.
User 2 bought 100x eggs, 100x flour and 100x sugar
User 3 bought 1x eggs, 1x Vodka and 1x Red Bull

By cosine similarity, user 1 and user 2 are more similar. By euclidean similarity, user 3 is more similar to user 1.

Questions in the text

I don't understand the first part.

Cosine similarity is specialized in handling scale/length effects. For case 1, context length is fixed -- 4 words, there's no scale effects. In terms of case 2, the term frequency matters, a word appears once is different from a word appears twice, we cannot apply cosine.

This goes in the right direction, but is not completely true. For example:

cos((
1
0
),(
2
1
))=cos((
1
0
),(
4
2
))‚â†cos
((
1
0
),(
5
2
))
cos
‚Å°
(
(
1


0
)
,
(
2


1
)
)
=
cos
‚Å°
(
(
1


0
)
,
(
4


2
)
)
‚â†
cos
‚Å°
(
(
1


0
)
,
(
5


2
)
)

With cosine similarity, the following is true:

cos((
a
b
),‚ãÖ(
c
d
))=cos((
a
b
),n‚ãÖ(
c
d
))¬†with¬†n
‚ààN
cos
‚Å°
(
(
ùëé


ùëè
)
,
‚ãÖ
(
ùëê


ùëë
)
)
=
cos
‚Å°
(
(
ùëé


ùëè
)
,
ùëõ
‚ãÖ
(
ùëê


ùëë
)
)
¬†with¬†
ùëõ
‚àà
ùëÅ

So frequencies are only ignored, if all features are multiplied with the same constant.

Curse of Dimensionality

When you look at the table of my blog post, you can see:

The more dimensions I have, the closer the average distance and the maximum distance between randomly placed points become.
Similarly, the average angle between uniformly randomly placed points becomes 90¬∞.

So both measures suffer from high dimensionality. More about this: Curse of dimensionality - does cosine similarity work better and if so, why?. A key point:

Cosine is essentially the same as Euclidean on normalized data.
Alternatives

You might be interested in metric learning. The principle is described/used in FaceNet: A Unified Embedding for Face Recognition and Clustering (my summary). Instead of taking one of the well-defined and simple metrics. You can learn a metric for the problem domain."
How to consider the effect of exclamation marks in sentiment analysis,"When tokenizing, separate and keep the exclamation points. Then use a count-based vectorization to create exclamation points feature with the number of occurrences.

The model will then have the opportunity to learn to weigh how the frequency of exclamation points contribute to positive/negative sentiment."
How to do feature selection after using pre-trained word embeddings?,"When using a RNN, you don't feed all the data at once, you usually have a seq2seq model. The models are created with an encoder-decoder architecture. The LSTM is used in the encoding phase.

So, let's say you have a text of 78 words. You will feed the embedding vector (size 300) of those 78 words, 1-by-1 into your LSTM and in the end you will get a hidden vector which represents your sentence.

Then, you can take this hidden vector and use it for classification (with a feedforward neural network, for example).

So, it doesn't matter that you have 4293 unique words in your data. You need to feed your LSTM a sequence of size [<number of words in sentence> √ó 300]."
"In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?","When you concatenate, you have to define a priori the size of each vector to be concatenated. This means that, if we were to concatenate the token embedding and the positional embedding, we would have to define two dimensionalities, 
d
t
ùëë
ùë°
 for the token and 
d
p
ùëë
ùëù
 for the position, with the total dimensionality 
d=
d
t
+
d
p
ùëë
=
ùëë
ùë°
+
ùëë
ùëù
, so 
d>
d
t
ùëë
>
ùëë
ùë°
 and 
d>
d
p
ùëë
>
ùëë
ùëù
. We would be decreasing the total size we devote to tokens in favor of positional information.

However, adding them together is potentially a super case of the concatenation: imagine that there is an ideal split of 
d
ùëë
 into 
d
t
ùëë
ùë°
 and 
d
p
ùëë
ùëù
 in terms of minimizing the loss; then, the training could converge to position vectors that only take 
d
t
ùëë
ùë°
 elements, making the rest zero, and the positions were learned and happened the same, taking the complementary 
d
p
ùëë
ùëù
 elements and leaving the rest to zero.

Therefore, by adding them, we leave the optimization of the use of the 
d
ùëë
 dimensions to the optimization process, instead of assuming there is an optimal partition of the vector components and setting a new hyperparameter to tune. Also, the use of the vector space is not restricted by a hard split in the vector components, but takes the whole representation space."
How to handle negative words in word2vec?,"When you look at the vectors that word2vec generates - negative words may have unique features but can be treated just like positive words. That is to say, as far as the NN is concerned - these are just similar words. You may have to construct ""concept vectors"" on top of the word vectors to do what you would like to do.

Your parts of speech tagging should automatically mark negating words as ADV. You can then train on these adverbs in conjunction to your verbs as a positive or negative output. Here's an example using spacy:-

import spacy

nlp = spacy.load('en')        # this can take a while
sample_text = u'Do not go.'
parsed_text = nlp(sample_text)
token_text = [token.orth_ for token in parsed_text]
token_pos = [token.pos_ for token in parsed_text]


At this point token_text will be a list of your words and token_pos will be the POS tagging:-

Do - VERB
not - ADV
go - VERB
. - PUNCT


As you can see, ""not"" is tagged as ADV here. You can now feed this tagged output (or a better parse tree) into a second network to train for a negative or positive output.

Hope this helps."
How to get sentence embedding using BERT?,"Which vector represents the sentence embedding here? Is it hidden_reps or cls_head?

If we look in the forward() method of the BERT model, we see the following lines explaining the return types:

outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here
return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)


So the first element of the tuple is the ""sentence output"" - each token in the input is embedded in this tensor. In your example, you have 1 input sequence, which was 15 tokens long, and each token was embedding into a 768-dimensional space.

The second element of the tuple is the ""pooled output"". You'll notice that the ""sequence"" dimension has been squashed, so this represents a pooled embedding of the input sequence.

So they both represent the sentence embedding. You can think of hidden_reps as a ""verbose"" representation, where each token has been embedded. You can think of cls_head as a condensed representation, where the entire sequence has been pooled.

Is there any other way to get sentence embedding from BERT in order to perform similarity check with other sentences?

Using the transformers library is the easiest way I know of to get sentence embeddings from BERT.

There are, however, many ways to measure similarity between embedded sentences. The simplest approach would be to measure the Euclidean distance between the pooled embeddings (cls_head) for each sentence."
Program to fine-tune pre-trained word embeddings on my data set,"While I am not aware of software specifically for tuning trained word embeddings, perhaps the following open source software might be helpful, if you can figure out what parts can be modified for the fine-tuning part (just an idea off the top of my head - I'm not too familiar with the details):

GloVe: Global Vectors for Word Representation (part of Stanford NLP Group software);

SENNA (its use for word embeddings is mentioned in this blog post);

Code on GitHub for Deep Learning-based word embeddings training;

Neural Probabilistic Language Model Toolkit, mentioned above (also Deep Learning-based)."
Approaches for implementing Domain specific Question answering System,"While the actual ""best system"" depends heavily on a number of factors including your goals and your resources, it is possible to discuss some general pros and cons to each system.

1. IR Based approaches: Information retrieval approaches allow the algorithm to make effective use of what we might call ""explicit knowledge"" or hardcoded facts. They can work very well when the answer to the query exists in some form already and where the algorithm simply needs to find this answer amongst a set of all the other possible answers. Such systems typically rely on either an explicit dataset of possible answers or facts, or by treating the web as such a dataset and querying it in some way. IR systems are often the only reasonable choice when the answer cannot be generated from abstract principles. For example, if we built a system to answer trivia questions about movie stars, we would need to have explicit knowledge of which features are associated with which movie stars (i.e. Brad Pitt has brown hair and starred in Fight Club.) No machine learning algorithm can be expected to generate this without the knowledge already present (although machine learning systems with access to such knowledge may serve as effective information retrieval systems.) Due to the external nature of the data with respect to the algorithm, IR systems tend to be very effective for large and rapidly changing data sets, which would otherwise require retraining of the machine learning model or manual reconfiguration of the ontology to handle. It is also worth noting that more complex information retrieval systems may include other systems such as NLP engines to translate the data (ex. natural language text) into a form usable by the IR system.

2. Ontology based approaches: Ontology methods take the most time to implement and are usually the most finicky. They require the iterative hypothesizing of a structured representation of the domain, testing of the representation, and modification of the representation based on discovered flaws. For complex real-world domains with many possible contingencies, this process can take a very long time and be quite frustrating. Because everything is hard-coded, even the best ontology-based systems display a relative degree of inflexibility compared to their machine learning and IR counterparts. Ontologies can be effective, however, when you do not have access to a data set for information retrieval or machine learning training, as the domain knowledge are hard-coded into ontology systems by hand. They can also be useful when the domain is relatively well-structured and where it is important to understand and plan every decision made by the system such as in automated attendant phone systems.

3. Machine learning based approaches: As machine learning approaches extract patterns from data, they are relatively robust to subtleties present in complex domains that were not explicitly built into an algorithm, model, or ontology. They work best when a prediction needs to be made about which ""class"" or category a given input (or query in your case) belongs to, or when a quantitative prediction needs to be produced from data. For example, if I have a large collection of animal pictured labeled with the type of animal present in these images, I can train a machine learning model to predict which animal type is present in future images, as long as the model has seen enough pictures of this animal that it can extract the patterns in the image that correspond to the image likely representing a lion or a tiger or a bear, etc. This works because there are properties in the image itself that predict which animal it represents and these patterns can be extracted from labeled data (there are also machine learning algorithms that pick out patterns in unlabeled data called unsupervised algorithms, but these are not relevant for your domain of interest.) Traditionally, machine learning systems are relatively poor at modeling domains where the answer depends not on recognizing a pattern, but having access to knowledge like in the movie stars example above. Machine learning systems are also great because they require relatively little manpower to implement. As you mentioned, they also require access to a dataset, which may need to be quite large depending on the complexity of the domain.

We can roughly summarize the advantages and disadvantages by ranking the methods according to a few criteria: 

All of the best systems (ex: IBM Watson) use a hybrid approach, taking advantage of each method for its relative strengths and substituting other methods to address their weaknesses. Depending on the performance you want, a QA system can be built by a single person with some knowledge of any of the above, or may require a team of upwards of 100 engineers."
BERT embedding layer,"Why are positional embeddings learned?

This was asked in the repo of the original implementation without an answer. It didn't get an answer either in the HuggingFace Transformers repo and in cross-validated, also without answer, or without much evidence.

Given that in the original Transformer paper the sinusoidal embedding were the default ones, I understand that during preliminary hyperparameter tuning, the authors of BERT decided to go with learned embeddings, deciding not to duplicate all experiments with both types of embeddings.

We can, nevertheless, see some comparisons between learned and sinusoidal positional embedding in the ICLR'21 article On Position Embeddings in BERT, where the authors observe that:

The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.

How does the model handle multiple sentence segments?

This is best understood with the figure of the original BERT paper:

The two sentences are encoded into three sequences of the same length:

Sequence of subword tokens: the sentence tokens are concatenated into a single sequence, separating them with a [SEP] token. This sequence is embedded with the subword token embedding table; you can see the tokens here.
Sequence of positional embedding: sequentially increasing positions form the initial position of the [CLS] token to the position of the second [SEP] token. This sequence is embedded with the positional embedding table, which has 512 elements.
Sequence of segment embeddings: as many EA tokens as the token length of the first sentence (with [CLS] and [SEP]) followed by as many EB tokens as the token length of the second sentence (with the [SEP]). This sequence is embedded with the segment embedding table, with has 2 elements.

After embedding the three sequences with their respective embedding tables, we have 3 vector sequences, which are added together and used as input to the self-attention layers.

Are the weights in these embedding layers adjusted when fine-tuning the model to a downstream task?

Yes, they are. Normally, all parameters are fine-tuned when fine-tunine a BERT-based model.

Nevertheless, it is also possible to simply use BERT's representations as input to a classification model, without fine-tuning BERT at all.

In this article you can see how these two approaches compare. In general, for BERT, you obtain better results by fine-tuning the whole model."
Difficulty interpreting word embedding vector similarity (spaCy),"Why are the vector similarities so high for unrelated words for the embedding?

For the specific example you give, I would argue that it makes sense that car and plant have high similarity. This is likely due to phrases such as car manufacturing plant

Also I am able to get vectors for non-words like ""asdfasfdasfd"" or ""zzz123Y!/¬ßzzzZz"", and they differ from each other. How is this possible?

For your specific case, since you use the en_trf_xlnetbasecased_lg, the answer is straightforward. Embeddings provided by XLNet are contextual, meaning that even if the word itself isn't a word, you'll get an embedding given the words in its context. Also, it is likely that HuggingFace's implementation uses Byte-Pair Encoding as tokens, making it much more robust to out-of-vocabulary situations."
How to properly use approximate_predict() with HDBSCAN clusterer for text clustering (NLP)?,"Why do you recompute the distance matrix?

Just compute the 1x5000 vector directly for all new points. You can even do this in batches, then feed one row at a time to the predictor."
Does high number of output labels affect the performance of BERT and how to handle the class imbalance issue while doing multi text classification?,"Why does your distribution contains 14 classes? What about the 102 others?

Quick but generic answers:

The number of classes always affect performance, because it's always easier to predict the correct class among a small number of possibilities than a big one. Even a random classifier has 50% chance to be correct with two classes, but only 1% chance with 100 classes. however performance also depend on the data, especially how easy it is to distinguish the classes.
The imbalance is part of the problem. You shouldn't try to resample because it doesn't work with text data. you could consider data augmentation techniques, but I'm skeptical. Typically your very small classes like M,N don't have a representative sample, imho they should be discarded. It could be useful to start by training a classifier which deals only with a few large classes to see how well it works, and then to progressively add the other classes. Don't forget that performance might be better by ignoring some of the small classes."
Text classification with multiple documents per labeled datapoint,"Why don't you make a person id and add this to your model?

If I understand you correctly, you do:

y=Œ≤X
ùë¶
=
ùõΩ
ùëã
,

where each row in 
X
ùëã
 are combined docs per person and 
y
ùë¶
 is a vector of true/false, right?

You could try:

y=Œ≤X+Œ≥z
ùë¶
=
ùõΩ
ùëã
+
ùõæ
ùëß
,

where each row in 
X
ùëã
 is only one doc now and 
z
ùëß
 is a vector of ids per person (so a factor).

Might be worth a try."
How to compute document similarities in case of source codes?,"Why not use all non-code indicators as 'handprints?' For example, many IDE's will add specific comments to documents when they are started. Also, and this is one that would be easy to detect my work, I have a tendency in python to copy entire import blocks from one script to another, whether or not I actually need all the imports or not. If someone uses a copied import or include statement, then their imports will all be in the same order. If you track the order of imports, you may be able to find patterns.

There are also issues of whether you use spaces or tabs for indentation, preferred number of spaces or tabs, etc. For languages with braces, you can see if braces are used following an if statement, or at the start of the next line. I believe this will give you enough 'handprints' to identify people distinctly.

Once you do this, you can use multilevel clustering to attempt to assign documents first to a group of possible originators (i.e. all originators who use a certain IDE or text editor). Then you can go through and look for certain patterns within each group, clustering again within each cluster."
Why are NLP and Machine Learning communities interested in deep learning?,"Why to use deep networks?

Let's first try to solve very simple classification task. Say, you moderate a web forum which is sometimes flooded with spam messages. These messages are easily identifiable - most often they contain specific words like ""buy"", ""porn"", etc. and a URL to outer resources. You want to create filter that will alert you about such suspecious messages. It turns to be pretty easy - you get list of features (e.g. list of suspicious words and presence of a URL) and train simple logistic regression (a.k.a. perceptron), i.e. model like:

g(w0 + w1*x1 + w2*x2 + ... + wnxn)


where x1..xn are your features (either presence of specific word or a URL), w0..wn - learned coefficients and g() is a logistic function to make result be between 0 and 1. It's very simple classifier, but for this simple task it may give very good results, creating linear decision boundary. Assuming you used only 2 features, this boundary may look something like this:

Here 2 axes represent features (e.g. number of occurrences of specific word in a message, normalized around zero), red points stay for spam and blue points - for normal messages, while black line shows separation line.

But soon you notice that some good messages contain a lot of occurrences of word ""buy"", but no URLs, or extended discussion of porn detection, not actually refferring to porn movies. Linear decision boundary simply cannot handle such situations. Instead you need something like this:

This new non-linear decision boundary is much more flexible, i.e. it can fit the data much closer. There are many ways to achieve this non-linearity - you can use polynomial features (e.g. x1^2) or their combination (e.g. x1*x2) or project them out to a higher dimension like in kernel methods. But in neural networks it's common to solve it by combining perceptrons or, in other words, by building multilayer perceptron. Non-linearity here comes from logistic function between layers. The more layers, the more sophisticated patterns may be covered by MLP. Single layer (perceptron) can handle simple spam detection, network with 2-3 layers can catch tricky combinations of features, and networks of 5-9 layers, used by large research labs and companies like Google, may model the whole language or detect cats on images.

This is essential reason to have deep architectures - they can model more sophisticated patterns.

Why deep networks are hard to train?

With only one feature and linear decision boundary it's in fact enough to have only 2 training examples - one positive and one negative. With several features and/or non-linear decision boundary you need several orders more examples to cover all possible cases (e.g. you need not only find examples with word1, word2 and word3, but also with all possible their combinations). And in real life you need to deal with hundreds and thousands of features (e.g. words in a language or pixels in an image) and at least several layers to have enough non-linearity. Size of a data set, needed to fully train such networks, easily exceeds 10^30 examples, making it totally impossible to get enough data. In other words, with many features and many layers our decision function becomes too flexible to be able to learn it precisely.

There are, however, ways to learn it approximately. For example, if we were working in probabilistic settings, then instead of learning frequencies of all combinations of all features we could assume that they are independent and learn only individual frequencies, reducing full and unconstrained Bayes classifier to a Naive Bayes and thus requiring much, much less data to learn.

In neural networks there were several attempts to (meaningfully) reduce complexity (flexibility) of decision function. For example, convolutional networks, extensively used in image classification, assume only local connections between nearby pixels and thus try only learn combinations of pixels inside small ""windows"" (say, 16x16 pixels = 256 input neurons) as opposed to full images (say, 100x100 pixels = 10000 input neurons). Other approaches include feature engineering, i.e. searching for specific, human-discovered descriptors of input data.

Manually discovered features are very promising actually. In natural language processing, for example, it's sometimes helpful to use special dictionaries (like those containing spam-specific words) or catch negation (e.g. ""not good""). And in computer vision things like SURF descriptors or Haar-like features are almost irreplaceable.

But the problem with manual feature engineering is that it takes literally years to come up with good descriptors. Moreover, these features are often specific

Unsupervised pretraining

But it turns out that we can obtain good features automatically right from the data using such algorithms as autoencoders and restricted Boltzmann machines. I described them in detail in my other answer, but in short they allow to find repeated patterns in the input data and transform it into higher-level features. For example, given only row pixel values as an input, these algorithms may identify and pass higher whole edges, then from these edges construct figures and so on, until you get really high-level descriptors like variations in faces.

After such (unsupervised) pretraining network is usually converted into MLP and used for normal supervised training. Note, that pretraining is done layer-wise. This significantly reduces solution space for learning algorithm (and thus number of training examples needed) as it only needs to learn parameters inside each layer without taking into account other layers.

And beyond...

Unsupervised pretraining have been here for some time now, but recently other algorithms were found to improve learning both - together with pretraining and without it. One notable example of such algorithms is dropout - simple technique, that randomly ""drops out"" some neurons during training, creatig some distortion and preventing networks of following data too closely. This is still a hot research topic, so I leave this to a reader."
Accuracy of word and sent tokenize versus custom tokenizers in nltk,"Why would we want a custom tokenizer?

Segementation is a very large topic, and as thus there is no perfect Natural Language Tokenizer. Any toolkit needs to be flexible, and the ability to change the tokenizer, both so that someone can experiment, and so that it can be replaced if requirements are different, or better ways are found for specific problems, is useful and important.

How could I measure if it was better that NLTK's tokenizer?

Anytime you are trying to quantify performance (ie: better) you will need to first define what is meant by better. Once this is done, then typically you would perform the using the various methods under measurement, and then compare the results against your definition of better. A couple of links which discuss these topics:

Performance of Different NLP Toolkits in Formal and Social Text
Optimizing to Arbitrary NLP Metrics using Ensemble Selection"
How to perform text classification on a dataset with many imbalanced classes,"With 200k instances, a class which has less than 10 instances represents less than 0.005% of the data. It's very unlikely that a model can learn to distinguish all these classes, especially the smallest ones.
Data augmentation is really not recommended with text. Text is very diverse so there's no way to generate a good representative sample from a few instances. This would only lead to either repeating almost the same instances (pointless) or generating made up data which doesn't really represent the class (biased dataset).

Realistically, it's unlikely that anything can be done with the very small classes. What I'd suggest is to start by training a model with only the top 5 or 10 classes, and then improve from there if it works reasonably well. Note that short texts are often difficult to classify, since they might contain too little information. As a simple rule of thumb, if a human expert looking at an instance cannot find which class it belongs to, then chances are that a ML model cannot either."
Online vs Batch Learning in Latent Dirichlet Allocation using Scikit Learn,"With batch you feed the entire data through each EM iteration. In the online implementation you feed only some of the data through each EM iteration (a ""mini-batch""). From the sklearn user guide:

While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points.

From the paper about online LDA:

We then update 
Œª
ùúÜ
 using a weighted average of its previous value and 
Œª
~
ùúÜ
~
. The weight given to 
Œª
~
ùúÜ
~
 is given by 
œÅ
t
=(
œÑ
0
+œÑ
)
‚àíŒ∫
ùúå
ùë°
=
(
ùúè
0
+
ùúè
)
‚àí
ùúÖ
, where 
Œ∫
ùúÖ
 ‚àà (0.5, 1] controls the rate at which old values of 
Œª
~
ùúÜ
~
 are forgotten and 
œÑ
0
‚â•0
œÑ
0
‚â•
0
 slows down the early iterations of the algorithm.

In the sklearn implementation:

weight = np.power(self.learning_offset + self.n_batch_iter_, -self.learning_decay)


So learning_offset is 
œÑ
0
ùúè
0
 which slows down early iterations, and learning_decay is 
Œ∫
ùúÖ
 which controls rate at which old weights are forgotten."
How can I tokenize a text file with BERT or something similar?,"With Huginface's Transformers, it should be doing with not much boilerplate.

A minimum example using PyTorch:

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1
outputs = model(input_ids)
last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple


Depending on how data you want to process, you might want to speed it up a little bit. In that case, you should do it batches which would require doing padding and passing the padding mask to the model."
Not clear about relative position bias,"With relative position bias, you are extending the concept of self-attention to also encode the distance between any two tokens. Basically you let the model itself learn the relative distance between any 2 tokens instead of feeding that information yourself. Most of the time (as shown in the paper), the model does a good job at figuring out the relationships between various tokens. If that is the case, it is always better to make the model generic by giving it the flexibility rather than providing this data & forcing the model to use it. Usually generalized models perform better. This is the simplest reason to not feed explicit positional embeddings. For more details on the bias parameterization, you can refer to the relevant section in - https://towardsdatascience.com/swin-vision-transformers-hacking-the-human-eye-4223ba9764c3"
NER with LSTM - How to recognize person names that are not part of the vocabulary?,"With word-level vocabularies, unknown words are normally encoded with a special token <unk>. If in the training data there were enough examples of person names, then the bidirectional LSTM may have learned to identify the person name from the context, not just the word itself. In that case, and provided that in the specific input sentence there is enough context, the model may be able to identify it. If the model were a normal LSTM instead of a bidirectional one, it would be more difficult for the model to identify it, as it would only be able to use the context to the left of the unknown word.

If the training data did not contain such examples, the it won't be able to identify the person from the context.

One option to improve the handing of this problem would be to force this kind of examples in the training data, by replacing person names with unknown words with certain probability.

Another option would be to use a subword-level vocabulary, like a byte-pair encoding (BPE) one, eliminating altogether the out-of-vocabulary (OOV) word problem."
NLP - How to perform semantic analysis?,"With your three labels: positive, neutral or negative - it seems you are talking more about sentiment analysis. This answer the question: what are the emotions of the person who wrote this piece of text?

Semantic analysis is a larger term, meaning to analyse the meaning contained within text, not just the sentiment. It looks for relationships among the words, how they are combined and how often certain words appear together.

To gain a deeper insight into your text, you could read about topics such as:

Semantic Analysis in general might refer to your starting point, where you parse a sentence to understand and label the various parts of speech (POS). A tool for this in Python is spaCy, which words very nicely and also provides visualisations to show to your boss.
Named Entity Recognition (NER) - finding parts of speech (POS) that refer to an entity and linking them to pronouns appearing later in the text. An example is to distinguish between Apple the company, and apple the fruit.
Embeddings - finding latent representation of individual words e.g. using Word2Vec. Text is processed to produce a single embedding for individual words in the form of an n-dimensional vector. You can then compute similarity measures (e.g. cosine similarity) between the vectors for certain words to analyse how they are related.
Lemmatisation - this method reduces many forms of words to their base forms, which means they appear more regularly and we don't consider e.g. verb conjugations as separate words. As an example, tracking, tracked, tracker, might all be reduced to the base form: track.

Your next step could be to search for blogs and introductions to any of those terms I mentioned.

Here is an example parse-tree from spaCy:

Reducing dimensions

This is something that would then refer to the vectors, which describe each of your words. Generally, the Word2Vec vectors are something like 300-dimensional. You might want to visualise the words, plotting them in 2d space. You can try a method like t-SNE, which will map the 300d vectors to 2d space, allowing nice plots showing relationships, while retaining as much of the original relationships described in the 300d space. There will, of couse, be some information loss, but you could not have visualised the 300d vectors in the first place!

Using the vectors for your words, you can compute things like the similarity (on a scale between 0 and 1) between president and CEO is something like 0.92 - meaning they are almost synonyms!"
"TF-IDF for 400,000+ unique words in corpus?","Without knowing your domain one cannot comment whether this is an appropriate size of feature names or not. However, consider this. Wordnet has database contains 155 327 words organized in 175 979 synsets for a total of 207 016 word-sense pairs[1].

Does your domain rely on more than 200% of the words in Wordnet.

I'm familiar with sklearn's TF-IDF implementation[1]. Your mileage may vary on the google cloud platform. My sense is that you have a combination of the following

Stop words
Words in multiple tense - run, runs, ran, running etc.
Misspelled words - running, runing etc.
Low frequency words
High frequency words
N-grams
Sparse/dense vector output

Stop Words: Use your own list or use lists for a known source to remove them. They don't contribute much to the resulting vector. In another sense, you will not be able to distinguish one document from another with these present.

Multiple tense: Needs preprocessing, but you could use nltk to get lemmatize your words. Again, you'll know best if converting run/ran/running etc. to run will hinder or improve your output.

Misspelled words There are dictionaries that find the nearest word to correct to. Domain dependent and could alter acronyms in ways that could hinder performance.

Low frequency/High frequency words: Again these are words that either occur too many times or too few times to distinguish documents. sklearn's tfidf implementation has two parameters, max_df and min_df. You could use something like this to throttle your feature set.

N-Grams: Are you using the standard tokenizer that returns a unigram or are you asking for unigrams, bigrams, trigrams, and/or others. This is useful as different information is encoded at different levels, but also increases feature space.

Sparse/dense vector output: Finally and this applies only to sklearn and python implementations - the output from the tfidf vectorizer is a sparse matrix. Converting to a dense matrix is useful when debugging, but makes a very big difference when you have a large document set. It will consume a lot of memory and slowdown subsequent processing. Make sure you are retaining a sparse matrix

My recommendation is that you try each of these and examine the output for validity for your downstream processes before combining.

References

[1] https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

[2] https://en.wikipedia.org/wiki/WordNet"
Need help in improving accuracy of text classification using Naive Bayes in nltk for movie reviews,"Without reading your code (sorry!) I can suggest that you drop the verbs-only approach and use a word embedding to account for word similarity. Your feature dictionary is bound to find very few ""hits"" in the documents, unless you have a humongous dataset, and even then YMMV because you are just using verbs. Also: what's the justifications for using verbs? ""I liked"" and ""I didn't like"" both contain the verb ""to like"" and they mean opposite things. Same goes for hate (because you only have binary classification: no nuances) and so on.

A popular python implementation of word2vec is gensim, but you could use that of tensorflow or some other embedding like the (allegedly superior) conceptnet numberbatch. If you want to summarize whole documents into numbers you can try doc2vec (aka paragraph2vec, paper here), also available in gensim, tensorflow, etc.

You will need a big dataset. Once you have this up and running you can try other classifiers."
Policy gradient/REINFORCE algorithm with RNN: why does this converge with SGM but not Adam?,"Without the code there's not much we can do but, I'd guess you need to significantly lower the learning rate. From my experience Adam requires a significantly lower learning rate compared to SGD."
Word embedding/Word2vec for POS tagging,"Word Embedding for Pos tags can easily be trained using pos tags sequence. There are lot of ways that you can get the trained model. I did it by gensim's word2vec api. Here is the link to it: https://radimrehurek.com/gensim/models/word2vec.html

Also if you want memory efficient solution, radim(creater of gensim) provides a great tutorial: https://rare-technologies.com/word2vec-tutorial/

You just need to pass pos sequence of your training data, resulting vector size, min frequency count etc. You can view api's documentation for more detail."
Are word embeddings further updated during training for document classification?,"Word embeddings are generally used as input features, which like you noticed for image based models, do not get modified during training.

It is in fact quite difficult to update embeddings during training (or at all after they have been computed!), because they are often in some latent space that holds information regarding their relationships to one another. It is for this reason that adding new vocabulary to existing embeddings is extremely challenging, and updating computed embeddings is also difficult without using all the data that was originally used to create them.

I am afraid I don't have an example I can point to where embedding are used during training and simultaneously updated/improved upon. I can imagine that there are ways to store them, however, such that it is technically feasible (although perhaps memory intensive!)"
Word embedding vectors for keyphrase extraction,"Word embeddings are just a way to represent tokens (often words, but could be characters) in a way that it inherently carries semantic meaning (i.e. as opposed to simply one-hot encoding), but it is not a keyphrase extraction technique.

Word embeddings can help you extract key phrases better because they will make your input more meaningful, but unless you have a technique to extract such phrases, they won't help you."
K-means clustering of word embedding gives strange results,"Word embeddings are trained by substitutability, not similarity.

If you consider a sentence like ""This food is unflavored."" Then a good substitute word would be ""flavored"", and the sentence will still be ""correct"".

In many cases, substitutability arises from similarity (crunchy, crispy) but it does also arise from opposites. You may consider ""king"" and ""queen"" to be opposites, too.

You probably should use a supervised approach then."
How to select target words for Lexical Simplification dataset,"Word frequency is the best way to identify complex words.

You have also other factors like word length, number of syllables, number of synonyms, or the complexity of pronunciation.

Then, those words are processed in Decision Trees to find the best solution for simplification.

See also: https://aclanthology.org/S16-1085.pdf

About the data source, they are in the test/train sample datasets.

https://github.com/MMU-TDMLab/CompLex/blob/master/test/lcp_multi_test.tsv

For instance, europarl refers to the european parliament datasets:

https://www.statmt.org/europarl/"
Detect related sentences,"Word Mover‚Äôs Distance (WMD) is an algorithm for finding the distance between pairs of strings. It is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors.

The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document.

For example:

 Source: ""From Word Embeddings To Document Distances"" Paper

The gensim package has a WMD implementation."
Word2Vec and Tf-idf how to combine them,"Word2Vec algorithms (Skip Gram and CBOW) treat each word equally, because their goal to compute word embeddings. The distinction becomes important when one needs to work with sentences or document embeddings; not all words equally represent the meaning of a particular sentence. And here different weighting strategies are applied, TF-IDF is one of those successful strategies.
At times, it does improve quality of inference, so combination is worth a shot.
Glove is a Stanford baby, which has often proved to perform better. Can read more about Glove against Word2Vec here, among many other resources available online."
Word2Vec embeddings with TF-IDF,"Word2Vec algorithms (Skip Gram and CBOW) treat each word equally, because their goal to compute word embeddings. The distinction becomes important when one needs to work with sentences or document embeddings: not all words equally represent the meaning of a particular sentence. And here different weighting strategies are applied, TF-IDF is one of them, and, according to some papers, is pretty successful. From this question from StackOverflow:

In this work, tweets were modeled using three types of text representation. The first one is a bag-of-words model weighted by tf-idf (term frequency - inverse document frequency) (Section 2.1.1). The second represents a sentence by averaging the word embeddings of all words (in the sentence) and the third represents a sentence by averaging the weighted word embeddings of all words, the weight of a word is given by tf-idf (Section 2.1.2)."
Is there any way to calculate a relevance score between a title and the content of a text?,"Word2Vec and cosine similarity are certainly a plausible solution. Note that the longer the text is, the more function words are in the text, therefore the average Word2Vec vectors are more similar to each other with the growing text length. This can be partially solved by removing the stop-words.

Another method to try might be cosine similarity with representation from contextual embeddings like ELMo or BERT. Also if you have some training examples (small hundreds might be enough) training a classifier based on BERT should work.

Finally, the fanciest solution is using the summarization model. Generating a title from an article content is one of the tasks that is done in summarization literature. If your data is similar to datasets that are used for training summarization models (or you have enough domain-specific data to train your own summarization model), you can use the model to estimate the probability of the title given the content. Even models that perform poorly on generation, perform well when used discriminatively."
How can I get a measure of the semantic similarity of words?,"Word2vec does not capture similarity based on antonyms and synonyms. Word2vec would give a higher similarity if the two words have the similar context. Eg The weather in California was _____ . The blank could be filled by both hot and cold hence the similarity would be higher. This concept is called Paradigmatic relations.

If you are interested to capture relations such as hypernyms, hyponyms, synonyms, antonym you would have to use any wordnet based similarity measure. There are many similarity measures based on wordnet. You may check this link"
Why we need to 'train word2vec' when word2vec itself is said to be 'pretrained'?,"word2vec is an algorithm to train word embeddings: given a raw text, it calculates a word vector for every word in the vocabulary. These vectors can be used in other applications, thus they form a pretrained model.

It's important to understand that the model (embeddings) depends a lot on the data they are trained on. Some simple applications can simply use a general pretrained model, but some specific applications (for example specific to a technical domain) require the embeddings to be trained on some custom data."
Which algorithm Doc2Vec uses?,"Word2Vec is not a combination of two models, rather both are variants of word2vec. Similarly doc2vec has Distributed Memory(DM) model and Distributed Bag of words (DBOW) model. Based on the context words and the target word, these variants arised.

Note: the name of the model maybe confusing

Distriubted Bag of words is similar to Skip-gram model
Distributed Memory is similar to Continuous bag of words model"
What algorithm can help me discover synonyms?,"word2vec is probably the way to go. It maps words to a point in n-dimensional space. You can use Euclidean (or whatever distance) to find the nearest points to a given word. If training went well, the closest points should be a synonym."
applying word2vec on small text files,"Word2Vec isn't a good choice for a dataset of such size. From researches I have seen, it will unleash its power if you feed at least couple of million of words, 3k tweets wouldn't be enough for a concise word similarity."
Sequence models word2vec,"Word2Vec operates on words and you want to compare 'texts' (series of words of varied length). For that, doc2vec might more appropriate.

You have very short 'texts' (names of campaigns) so generating embedding from them (only from them) want give greate effects. You could start with some pretrained vectors but anyway probably you want achieve much more. But the questions is:

It's not sure from your examples and explanation what will be nature of texts you want to find the most similar campaign name with. On one side you are writing about finding the one most frequent with a given word - then you could just create statistics of words and campaigns. On the other side, in your example, you pass whole text - then above mentioned apply, finding most similar text using text vectors."
Predicting a word using Word2vec model,"Word2vec works in two models CBOW and skip-gram. Let's take CBOW model, as your question goes in the same way that predict the target word, given the surrounding words.

Fundamentally, the model develops input and output weight matrices, which depends upon the input context words and output target word with the help of a hidden layer. Thus back-propagation is used to update the weights when the error difference between predicted output vector and the current output matrix.

Basically speaking, predicting the target word from given context words is used as an equation to obtain the optimal weight matrix for the given data.

To answer the second part, it seems a bit complex than just a linear sum.

Obtain all the word vectors of context words
Average them to find out the hidden layer vector h of size Nx1
Obtain the output matrix syn1(word2vec.c or gensim) which is of size VxN
Multiply syn1 by h, the resulting vector will be z with size Vx1
Compute the probability vector y = softmax(z) with size Vx1, where the highest probability denotes the one-hot representation of the target word in vocabulary. V denotes size of vocabulary and N denotes size of embedding vector.

Source : http://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf

Update: Long short term memory models are currently doing a great work in predicting the next words. seq2seq models are explained in tensorflow tutorial. There is also a blog post about text generation."
Annotating the vocabulary using Word2vec model,WordNet is certainly an interesting resource to explore for this task. It might not cover all your vocabulary but I can't think of any other way to capture fine-grained semantic relationship between words.
Generate text using user-supplied keywords,"Yes fine-tuning GPT2 could help you through this objective. But the only concern is regarding the size of input data you have. To get a better performing model, you must a have larger input set."
Can AI (NLP) convert user questions (text) into database SQL queries?,"Yes it is possible. You can train end to end an RNN over the training data. It means input the user query as an input and set the output for the sql query. Definitely the most challenging part is preparing the training set as RNN needs a lot of data to be learned.

Also, for the training set, it might you can use a GAN‚Äå to generate more queries for the users' query dataset that you have. Notice that, as you want the query generator for the specific schema of the database, you need your own training data and it could not be generalized over all kinda schemas."
NLP Interview Coding Task,"Yes the task is very clear. Maybe one suggestion is to change language from build a classifier to provide more details. Create a Rule based clasifier which calculates cosine similarity between sentence and all given classes and assign it to class where it has maximum cosine similarity

This would be a very good excercise and test coding as well as general understanding of the candidate. If a person is able to achieve even 90% of this, they should be a good candidate

It should be good enough for middle level and senior DS engineer.

If you want to complicate it maybe instead of count vectoriser you can ask them to code TF-IDF"
Feature extraction from resume using Python without rule based logic,"Yes there is. Look at this paper of general information extraction

Where you can construct features and generalise around extracting, without any hard coded rules."
Is there a sensible notion of 'character embeddings'?,"Yes, absolutely.

First it's important to understand that word embeddings accurately represent the semantics of the word because they are trained on the context of the word, i.e the words close to the target word. This is just another application of the old principle of distributional semantics.

Characters embeddings are usually trained the same way, which means that the embedding vectors also represent the ""usual neighbours"" of a character. This can have various applications in string similarity, word tokenization, stylometry (representing an author's writing style), and probably more. For example, in languages with accentuated characters the embedding for √© would be closely similar to the one for e; m and n would be closer than x and f ."
Visualizing F-score differences in information extraction,"Yes, I think that's a sound approach and a good way to compare different systems.

A ROC curve comparison is usually more informative than the raw performance scores, but it's still quite general. In case you want to observe even more detail, you could also try to look at specific groups of instances. One way to do that is to count for every instance how many systems correctly classify it: an instance almost always correctly classified is ""easy"", and conversely an instance which is almost always misclassified is ""hard"". It's often interesting to look at what happens specifically for the ""hard"" instances with the different systems. You could take a subset of ""hard"" instances and calculate the performance or ROC curve only on those, in order to distinguish more precisely the best systems.

For the record, if it makes sense for your task you might also want to consider more flexible scoring methods for text spans: currently it seems that your evaluation considers an answer correct only of the exact span is predicted. You could consider counting the fact that a span is partially correct, for instance by counting the number of tokens correctly annotated."
Unsupervised feature learning for NER,"Yes, it is entirely possible to combine unsupervised learning with the CRF model. In particular, I would recommend that you explore the possibility of using word2vec features as inputs to your CRF.

Word2vec trains a to distinguish between words that are appropriate for a given context and words that are randomly selected. Select weights of the model can then be interpreted as a dense vector representation of a given word.

These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations. Basic vector arithmetic even reveals some interesting learned relationships between words.
For example, vector(""Paris"") - vector(""France"") + vector(""Italy"") yields a vector that is quite similar to vector(""Rome"").

At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information.

For that matter, LDA and LSA are also valid options for unsupervised feature learning -- both attempt to represent words as combinations of ""topics"" and output dense word representations.

For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you'll have to train your own model."
How to detect the match precision of OneVsRestClassifier,"Yes, of course.

Assuming that you have used sklearn's OneVsRestClassifier and so you have a decision function for example a Support Vector Classifier with say linear kernel. Use set_params to change probability key to True, default is False. Use this in the OneVsRestClassifier classifier and then go with the inbuilt function predict_proba like

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
mod = OneVsRestClassifier(SVC(kernel='linear').set_params(probability=True)).fit(samples,classes)
print mod.predict_proba(np.array([your_sample_vector]).reshape(1,-1))


Edit:

You can use your old LinearSVC with decision_function to find the distance from the hyperplane and convert them to probabilities like

mod = OneVsRestClassifier(LinearSVC()).fit(sample,clas)
proba = (1./(1.+np.exp(-mod.decision_function(np.array(your_test_array).reshape(1,-1)))))
proba /= proba.sum(axis=1).reshape((proba.shape[0], -1))\
print proba


Now you don't need tuning the parameters, I guess. :)"
"How does an LLM ""parameter"" relate to a ""weight"" in a neural network?","Yes, the parameters in a large language model (LLM) are similar to the weights in a standard neural network. In both LLMs and neural networks, these parameters are numerical values that start as random coefficients and are adjusted during training to minimize loss. These parameters include not only the weights that determine the strength of connections between neurons but also the biases, which affect the output of neurons.In a large language model (LLM) like GPT-4 or other transformer-based models, the term ""parameters"" refers to the numerical values that determine the behavior of the model. These parameters include weights and biases, which together define the connections and activations of neurons within the model. Here's a more detailed explanation:

-Weights: Weights are numerical values that define the strength of connections between neurons across different layers in the model. In the context of LLMs, weights are primarily used in the attention mechanism and the feedforward neural networks that make up the model's architecture. They are adjusted during the training process to optimize the model's ability to generate relevant and coherent text.

-Biases: Biases are additional numerical values that are added to the weighted sum of inputs before being passed through an activation function. They help to control the output of neurons and provide flexibility in the model's learning process. Biases can be thought of as a way to shift the activation function to the left or right, allowing the model to learn more complex patterns and relationships in the input data.

The training process involves adjusting these parameters (weights and biases) iteratively to minimize the loss function. This is typically done using gradient descent or a variant thereof, such as stochastic gradient descent or Adam optimizer. The loss function measures the difference between the model's predictions and the true values (e.g., the correct next word in a sentence). By minimizing the loss, the model learns to generate text that closely resembles the patterns in its training data.

Researchers often use the term ""parameters"" instead of ""weights"" to emphasize that both weights and biases play a crucial role in the model's learning process. Additionally, using ""parameters"" as a more general term helps communicate that the model is learning a complex set of relationships across various elements within the architecture, such as layers, neurons, connections, and biases."
Automatic topic labelling for topic modelling,"Yes, there are ways to automatically label topics in topic modeling. One common approach is to use the top words in each topic as labels. For example, if your topic model identified the following topics:

Topic 1: [""dog"", ""cat"", ""pet"", ""fur"", ""animal""] Topic 2: [""car"", ""road"", ""speed"", ""engine"", ""wheel""] Topic 3: [""apple"", ""fruit"", ""juice"", ""pie"", ""tree""] Then you could automatically label these topics as ""Pets"", ""Cars"", and ""Apples"" based on the top words in each topic.

In Python, the scikit-learn library provides a convenient way to automatically label topics using this approach. Here's an example of how you could use it:

# import the necessary modules
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# create a CountVectorizer to generate word counts from the text data
vectorizer = CountVectorizer()

# fit the vectorizer to the text data and transform it into a word count matrix
word_counts = vectorizer.fit_transform(text_data)

# create a LatentDirichletAllocation model and fit it to the word count matrix
lda = LatentDirichletAllocation()
lda.fit(word_counts)

# get the top words for each topic
top_words = [vectorizer.get_feature_names()[i] for i in lda.components_.argsort()[:, ::-1][:, :5]]

# automatically label each topic using the top words
labels = ["" "".join(words) for words in top_words]

# print the labels for each topic
print(labels)


This code will automatically label each topic using the top 5 words in each topic, as described above. You can adjust the number of top words to use as labels by changing the value of n_top_words in the code."
Is there any named entity reconginition algorithm trained for the french language?,"Yes, there is a french model free and ready to use via the spaCy package!

Here are the small amd medium sized models, that should be ready to go.

Here is the basic summary of the dataset, shown at the spaCy website:"
"Word2vec outperforming BERT, possible?","Yes, this could be possible if your dev/test data comes from the same domain as the training data, in which case word2vec will encounter fewer OOV tokens that mess up the loss.

This could also mean that the benefits of BERT - subword tokenization to handle OOV characters in generalized domains - are lost. If your vocabulary size is small, your word2vec model needs to capture relationships between fewer tokens and can model those relationships better than a subword model which loses the relationships between fixed tokens in your data and instead tries to generalize relationships across >30K subword tokens (in the bert-based-uncased model), which could lead to noise."
NLP and one-class classifier building,"Yes, this is feasible.

One-class classification is a thing, but it is usually used in a context where it is hard or impossible to get negative samples. In your case, I would argue, you can quite easily get tweets that are not about activism, therefore you can render it as a binary classification, because you have data points of two classes or labels: 1 for tweets that are part of your class and another 1 for tweets that are not.

There are many ways to build a classifier, SVM is only one of them. You could also use a Naive Bayes algorithm, or as @Kasra mentioned a neural network model. No matter what you use, you will have to organise your data such that you have samples of both classes: activism and non-activism within your set. This means that you should randomly pick tweets from your big dataset and manually check if they relate to activism, even if they don't have the hashtags in them that you used for identifying the activism tweets in the beginning. Further, you have to think about the features that your classifier will use. The simplest might be the bag of words within the tweets, but you might also pre-process the tweet to exclude stop-words. Depending on which algorithm you use, you might find that your classifier relies a lot on the presence of your particular hashtags as features for predicting the class. In this case it might struggle to identify other tweets without this hashtags as activism, even if they are activism. I would experiment with pre-processing the tweets in your entire dataset to remove those hashtags from the the tweets."
Text post-processing,"Yes, you can do that by add them to the existing NLTK Stop-word dictionary for all such words/Creating a Custom Stop-word dictionary.

for Custom Stop-word dictionary you need to include all the key words in the dictionary before processing, make sure to check if the words are present in the stop-word dictionary. If yes, remove them from the text else nothing needs to be done.

set of code is for updating new words to the NLTK Stop-words dictionary:

#Custom function to remove the stop-words
    def removeStopWords(str):
    #select english stopwords from NLTK
    cachedStopWords = set(stopwords.words(""english""))
    #add custom words, mentioned above
    cachedStopWords.update((""viewed"", ""flutter"", ""function"", ""k"", ""neighbour""))
    #remove stop words
    new_str = ' '.join([word for word in str.split() if word not in cachedStopWords]) 
    return new_str

set of code is for making a Custom stop-word dictionary:

stopwordList = [""viewed"", ""flutter"", ""function"", ""k"", ""neighbour""]
StopWordsRemover(inputCol=""words"", outputCol=""filtered"" ,stopWords=stopwordList)"
Weighting of words in lexicon based sentiment analysis,"Yes, you can. Not quite sure what else to add. Your formula can then look like:

Score=
‚àë
i
f(salienc
e
i
,frequenc
y
i
,sentimen
t
i
)
ùëÜ
ùëê
ùëú
ùëü
ùëí
=
‚àë
ùëñ
ùëì
(
ùë†
ùëé
ùëô
ùëñ
ùëí
ùëõ
ùëê
ùëí
ùëñ
,
ùëì
ùëü
ùëí
ùëû
ùë¢
ùëí
ùëõ
ùëê
ùë¶
ùëñ
,
ùë†
ùëí
ùëõ
ùë°
ùëñ
ùëö
ùëí
ùëõ
ùë°
ùëñ
)

Where 
f
ùëì
 is a function that weighs your sentiment score with the salience and frequency. Up to you to define how.

What if you don't know which 
f
ùëì
 to use?

Now, bear with me, this isn't something I've tried per se, but this could be an interesting approach. You could use a recurrent neural network and your input could be the salience, frequency, and sentiment score for each word. Not only will your RNN ""create"" (ideally) the best 
f
ùëì
 for your particular problem, but it will also use the sequential information of the words, which may even improve your results."
Weighted sum of word vectors for document similarity,"Yes, your method is valid and it has been studied before it is known as Mean of Word Embeddings (MOWE) or Sum of Word Embeddings (SOWE), although your method is more a weighted mean of vectors. I think that a good starting point for knowing more about the academics of the method is this paper: How Well Sentence Embeddings Capture Meaning. It discusses your method and give some pointers to other papers that also discuss the validity of the method."
Is it valid to include your validation data in your vocabulary for NLP?,"Yes. That should be fine. What you suggest makes sense and should not bias the results. The reasons you give are good ones. For any reasonable classifier, if the value of an attribute is always zero in the training set, that should cause the attribute to be essentially ignored.

There is a simple test to let you confirm this. You can try, for each document in the validation set, zeroing out the entries in the feature vector that correspond to words that were not present in any document in the training set, and see if that changes the classification. If it doesn't, then you know that your method has had no effect and hasn't introduced any bias.

Here is another alternative, if you prefer:

Conceptually, in an ideal world the vocabulary for each fold would only include words in the training set.

As a matter of implementation, it's certainly possible to implement that in a more efficient way than re-generating the vocabulary and re-generating the feature vectors for each fold. As long as your implementation generates the same vectors, it doesn't matter how it obtains them.

Thus, you could have an implementation that works like the following:

Generate a ""superset vocabulary"" that includes all of the words found anywhere in the dataset (not just the training set).

Generate a sparse feature vector for each document, based on the ""superset vocabulary"". I suggest that you represent these in an efficient way, e.g., as a Python dictionary that maps from word to count.

For each fold:

Split into training and validation sets.

Generate a subset vocabulary, containing only the words in the training set. Probably there is a small set 
S
ùëÜ
 of words that are in the superset vocabulary but not the subset vocabulary, so I suggest storing that set 
S
ùëÜ
.

For each document in the training set and validation set, generate a derived feature vector for the document, using only the subset vocabulary. I suggest generating this by starting from the feature vector for the superset vocabulary, then removing the words in 
S
ùëÜ
. This should be more efficient than regenerating the feature vector from scratch.

Now train and validate these derived feature vectors.

This is equivalent to the ""conceptually ideal"" approach, but will run faster.

That said, I think your proposal is the pragmatic one, and in practice should yield equivalent results, and be even faster still."
How to determine the complexity of an English sentence?,"Yes. There are various metrics, such as the fogg index. Textacy in python has a nice list and implementations.

>>> ts.flesch_kincaid_grade_level
10.853709110179697
>>> ts.readability_stats
{'automated_readability_index': 12.801546064781363,
 'coleman_liau_index': 9.905629258346586,
 'flesch_kincaid_grade_level': 10.853709110179697,
 'flesch_readability_ease': 62.51222198133965,
 'gulpease_index': 55.10492845786963,
 'gunning_fog_index': 13.69506833036245,
 'lix': 45.76390294037353,
 'smog_index': 11.683781121521076,
 'wiener_sachtextformel': 5.401029023140788}"
Classify phrases as biomedical or non-biomedical,"Yes. There has been work going on in the healthcare domain which involves NLP.

Resources like Pubmed have been put together for the exact same purpose.

You can make use of the freely available Pubmed vectors for training your models like maybe a sentence tagging style model for identification. We(my team at work) use them for identification of medical terms like diseases, medicines, etc."
Number of features of the model must match the input. Model n_features is 740 and input n_features is 400,"You are currently using the fit_transform method on both your training dataset as well as your test set. This is incorrect since you should not fit the model on your test set as (depending on the model used) this would be overfitting, and it can give issues with dataset shapes when creating new columns based on the values in the data (count vectorizer, creating dummy columns etc.). The correct way would be to fit and transform the train data and then only transform the test dataset:

#Initializing BoW
cv = CountVectorizer()

#Test-Train Split
X_train,X_test,y_train,y_test = train_test_split(experiment_df['Sentence'],experiment_df['Label'])

#Transform
train = cv.fit_transform(X_train)
test = cv.transform(X_test)


#Train Classifier
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(train,y_train)

#Pred
y_pred = clf.predict(test)"
How to identify word in a sentence representing the song genre?,"You are describing named-entity recognition (NER) which seeks to locate and classify named entities mentioned in text into pre-defined categories. In this case, the pre-defined categories are music genres.

There are many algorithms for NER. In addition to Recurrent Neural Networks (RNN), Hidden Markov Models (HMMs) and CRF (Conditional Random Field) are often used.

In most cases, the algorithm is not the most difficult part of NER. The difficult part of NER is creating a large, accurate annotated training corpus."
Is there a clustering algorithm which accepts some clusters as input and outputs some more clusters?,"You are describing semi-supervised learning where the training dataset is only partially labeled.

One common set of techniques to solve that problem is active learning. In active learning, there is a learning loop where the algorithm makes predictions and a human corrects those predictions.

Pre-clustering is a specific active learning technique to address the problem you describe. The goal is to repetitively select the most representative training examples to add new labels and as well as avoiding repeatedly labeling samples in same cluster. ""Active Learning Using Pre-clustering"" by Nguyen and Smeulders goes into greater detail."
Is there any advanced collocation tool based on NLP+DL?,"You are describing the ""Language Modeling"" problem in NLP. Language modeling finds the probability distribution over sequences of words - given the words I have seen so far, what are the most likely words that will come next or are missing.

word2vec can find reasonable collocations/cooccurrence for individual words. However, word2vec will do a poor job with sentence completion because it does not model grammatical dependencies.

You can find a word2vec nearest individual word demo here."
Dynamic clustering in NLP dataset,"You are describing the Chinese restaurant process, analogous to seating customers at tables in a restaurant. Customer one (item one) sits at table one (cluster one). The next customer (item two) can either sit at table one (cluster one) or sit at table two (cluster two). This process is repeated for each item.

There needs to be a decision rule to either join an existing cluster or create a new cluster. One example of a decision rule is to measure similarity and use a threshold to decide."
Real time topic identification of news article,"You are looking for either ""online"" or ""streaming"" topic modeling. A hierarchical Dirichlet processes can automatically choose the number of topics. Here is a paper on streaming/online Latent Dirichlet Allocation model (LDA). A Python implementation of streaming/online can be found here. If your scale warrants it, Apache Spark has an implementation of online LDA also."
Splitting a sentence to meaningful parts [closed],"You are looking to create a parse tree to find multi-token clauses.

Here is code to generate a parse tree:

import spacy
from nltk import Tree

nlp = spacy.load('en')

def to_nltk_tree(node):
    if node.n_lefts + node.n_rights > 0:
        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])
    else:
        return node.orth_

query = u'tell me about people in konoha who have wind style chakra and are above jonin level'
doc = nlp(query)
[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]


Output:

    tell                          
  ____|______                       
 |         about                   
 |           |                      
 |         people                  
 |     ______|_____                 
 |    |           have             
 |    |       _____|____________    
 |    |      |     |     |     are 
 |    |      |     |     |      |   
 |    |      |     |   chakra above
 |    |      |     |     |      |   
 |    in     |     |   style  level
 |    |      |     |     |      |   
 me konoha  who   and   wind  jonin


From that parse tree, you can then select the phrases."
How do I use NLTK Text object with Re library?,"You are passing the variable text (where you stored the nltk.Text object) to re.sub (which expects a string str object, not a nltk.Text object).

Contrary to what that class's name may suggest, Text is not a string-like object itself, it simply stores strings. That may be what misled you.

Those strings are probably what you need when calling the re-related functionality below. You can access them normally and in a pythonic via standard enumeration methods, e.g. for-looping over that instance, for word in theText.... See the code below for a full example.

import re
import random
from nltk.text import Text

# Initialize a dummy text with integers as its words.
# With random proability, we add a non-alphanumeric 
# character to the word so that we can test the regular
# expression in the original example.
text = Text([
    str(random.randrange(1, 100)) + (
        '' if random.random() >= 0.5 else 
        random.choice(['!', ',', '?', ':', ';'])
    )
    for _ in range(20)
])

# The tokens in the `Text` object can be accessed
# via enumeration (the class implements a Python iterator
#¬†underlyingly)
print(text)
print(text.tokens[:10])
print(list(text)[:10])
print([x for x in text][:10])

# To avoid the exception, we comment the original line
# We can't pass the variable `text` to `re.sub` because
# it expects a string and `text` stores an instance of 
# NLTK's `Text` class.
#lowerx = re.sub('[^a-zA-Z0-9]','',text)

# Test for the expected behavior:
for token in text:
    lowertoken = re.sub('[^a-zA-Z0-9]', '', token)
    print('<input=""%s""  output=""%s"">' % (token, lowertoken))"
What is the state of the art in the field of NLP? [closed],"You are probably aware that deep learning is all the rage these days, and it has touched NLP too. There is a tutorial on it from a recent conference: Deep Learning for Natural Language Processing (without Magic) by Richard Socher and Christopher Manning, who are from Stanford."
Do transformers (e.g. BERT) have an unlimited input size?,"You are right that a transformer can take in an arbitrary amount of tokens even with fixed parameters, excluding the positional embedding matrix, whose size directly grows with the maximum allowed input length.

Apart from memory requirements (O(n¬≤)), the problem transformers have regarding input length is that they don't have any notion of token ordering. This is why positional encodings are used. They introduce ordering information into the model. This, however, implies that the model needs to learn to interpret such information (precomputed positional encodings) and also learn such information (trainable positional encodings). The consequence of this is that, during training, the model should see sequences that are as long as those at inference time because for precomputed positional encodings it may not correctly handle the unseen positional information and for learned positional encodings the model simply hasn't learned to represent them.

In summary, the restriction in the input length is driven by:

Restrictions in memory: the longer the allowed input, the more memory is needed (quadratically), which doesn't play well with limited-memory devices.
Need to train with sequences of the same length as the inference input due to the positional embeddings.

If we eliminate those two factors (i.e. infinite memory and infinite-length training data), you could set the size of the positional embeddings to an arbitrarily large number, hence allowing arbitrarily long input sequences.

Note, however, that due to the presence of the positional embeddings, there will always be a limit in the sequence length (however large it may be) that needs to be defined in advance to determine the size of the embedding matrix."
So what's the catch with LSTM?,"You are right that LSTMs work very well for some problems, but some of the drawbacks are:

LSTMs take longer to train
LSTMs require more memory to train
LSTMs are easy to overfit
Dropout is much harder to implement in LSTMs
LSTMs are sensitive to different random weight initializations

These are in comparison to a simpler model like a 1D conv net, for example.

The first three items are because LSTMs have more parameters."
What information does output of [SEP] token captures in BERT?,"You are right, CLS token tries to capture the sentence representation, because during pretraining this token is used to decide if 2 sentences are contiguous or not.

But the author mentioned that CLS token was not supposed to be a sentence representation, and should be used carefully.

The SEP token is a token used simply to separate sentence, in order to make is easier for BERT to know that the input is made of several sentences. Since the SEP token is not used at pretraining time, the SEP token does not represent anything.

About your last question : We don't know.

In pretraining, the model was not train with several sentence, so the model will not know how to handle several sentence.

But if you finetune it, the model may be able to learn new representation and can learn so that the CLS token contain information about all the sentences.

For example, this code finetuned BERT with a new pattern :

[CLS] Sen 1 [SEP] [CLS] Sen 2 [SEP] [CLS] Sen 2 [SEP] ...


And the CLS token is used to represent each sentence. Because the model is finetuned, BERT is learning to represent each sentence in the corresponding CLS token."
Intuition of LDA,"You are right, LDA is not very intuitive. It involves a lot of mathematics and concepts. However this video should help you

https://youtu.be/3mHy4OSyRf0

Also this article

‚ÄúIntuitive Guide to Latent Dirichlet Allocation‚Äù by Thushan Ganegedara https://link.medium.com/texozcnAc6"
How to load the pre-trained BERT model from local/colab directory?,"You are using the Transformers library from HuggingFace.

Since this library was initially written in Pytorch, the checkpoints are different than the official TF checkpoints. But yet you are using an official TF checkpoint.

You need to download a converted checkpoint, from there.

Note : HuggingFace also released TF models. But I'm not sure if it works without conversion from official TF checkpoints. If you want to use the TF API of HuggingFace, you need to do :

from transformers import TFBertForMaskedLM"
How can I find contextually related words and classify into custom tags/labels?,"You are working with word classification so you really don't have any contextual information that you can leverage. So the best thing would be to use freely available contextual information called as word2vec. I would suggest using the pretrained GLOVE Word2Vec as they are much cleaner than the Google ones. I am going to suggest two approaches for doing this:

Naive Approach

Load the pretrained word vectors.
For every new word calculate cosine similarity with every item in your training list.
Predict the label for the word which is closest to your new word.
Alternatively you can calculate cosine similarity with the labels and just predict the label which comes the closest.

Better Approach

Load the pretrained word vectors.
Extract the vectors for every item in the training set.
One hot encode the labels.
Train a neural net or any multiclass classification algorithm using the vectors as features and save the model.
For every new word extract the vector and run the model on this vector.
The predicted label is your output.

You can use the first approach to run quick tests and check the efficacy of the approach and then move on to the second one if you found it fit to your use."
How to add more features in addition to a 100D word vector,"You can add a second Input layer to your architecture.

Look at this link Keras Guide Multiple Inputs for a more detailed explanation.

The output of the embedding layer can be combined with the secondary input and passed to the next layer.

auxiliary_input = Input(shape=(5,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])"
Normalize summary of customer feedback text / word-cloud /word-count,"You can apply text classification with Bert.

It would give a classification, whatever the message length is.

Therefore, you can use multi-class text classification, for instance:

https://huggingface.co/palakagl/Roberta_Multiclass_TextClassification?text=I+love+AutoTrain+%F0%9F%A4%97

To implement it, here are several tutorials:

https://www.kaggle.com/code/thebrownviking20/bert-multiclass-classification

https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613

https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f#:~:text=There%20are%20two%20different%20BERT,hidden%20size%2C%20and%20340%20parameters."
Creating labels for Text classification using keras,"You can build the text classification application with CNN algorithm by Keras library. Please take a look at this git repository. Here

As you can see, you need to create training and testing data by loading polarity data from files, splitting the data into words, generating labels and returning split sentences and labels. And you can create the convolutional neural network with Dense, Embedding, Conv2D, MaxPool2D of keras.

Here is the final model training snippet.

from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D
from keras.layers import Reshape, Flatten, Dropout, Concatenate
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
from keras.models import Model
from sklearn.model_selection import train_test_split
from data_helpers import load_data

print('Loading data')
x, y, vocabulary, vocabulary_inv = load_data()

# x.shape -> (10662, 56)
# y.shape -> (10662, 2)
# len(vocabulary) -> 18765
# len(vocabulary_inv) -> 18765

X_train, X_test, y_train, y_test = train_test_split( x, y,     test_size=0.2, random_state=42)

# X_train.shape -> (8529, 56)
# y_train.shape -> (8529, 2)
# X_test.shape -> (2133, 56)
# y_test.shape -> (2133, 2)


sequence_length = x.shape[1] # 56
vocabulary_size = len(vocabulary_inv) # 18765
embedding_dim = 256
filter_sizes = [3,4,5]
num_filters = 512
drop = 0.5

epochs = 100
batch_size = 30

# this returns a tensor
print(""Creating Model..."")
inputs = Input(shape=(sequence_length,), dtype='int32')
embedding = Embedding(input_dim=vocabulary_size,     output_dim=embedding_dim, input_length=sequence_length)(inputs)
reshape = Reshape((sequence_length,embedding_dim,1))(embedding)

conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0],     embedding_dim), padding='valid', kernel_initializer='normal',     activation='relu')(reshape)
conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1],     embedding_dim), padding='valid', kernel_initializer='normal',     activation='relu')(reshape)
conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2],     embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)

maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)
maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)
maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)

concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
flatten = Flatten()(concatenated_tensor)
dropout = Dropout(drop)(flatten)
output = Dense(units=2, activation='softmax')(dropout)

# this creates a model that includes
model = Model(inputs=inputs, outputs=output)

checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5',     monitor='val_acc', verbose=1, save_best_only=True, mode='auto')
adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
print(""Traning Model..."")
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  #     starts training


By running this code, you will get the trained model with the format of hd5. Finally, you can use your model for prediction."
Get long answers from BERT,"You can check quora for your QnA dataset.

As far as long answers are concerned it may not be because of BERT as SQuADhas very crisp and short answers for most of its questions.

BERT already does understand long term dependencies in your data(one of reasons for using deeply bidirectional tranformer)."
KeyError: Selecting text from a dataframe based on values of another dataframe,"You can consider this simple example, and take this forward to solve your problem. I have data set of quotes about animals and fruits. and I need to find out the top occurring word in each category. Count Vectorizer will be useful here

Consider data:

Code Snippet:

from sklearn.feature_extraction.text import CountVectorizer

def return_word_count_segment_wise(data, type):
    
    tfidf_vec = CountVectorizer(max_features=5)
    model = tfidf_vec.fit(data[data['Type'] == type].description)
    model_transform = tfidf_vec.transform(data[data['Type'] == type].description)
    
    feature_list = model.get_feature_names();    
    count_list = model_transform.toarray().sum(axis=0)    
    return dict(zip(feature_list,count_list))


return_word_count_segment_wise(data, 'Animal') 


Outputs: {'cats': 3, 'is': 2, 'love': 4, 'my': 4, 'than': 3}

return_word_count_segment_wise(data, 'Fruits')


Outputs: {'fruit': 8, 'of': 5, 'that': 2, 'the': 3, 'we': 3}

Answering question asked in comment:

Try to merge both the dataframes, and then call the function while filtering out the customer segment using class (1/2/3)

merged_df = pd.merge(badges, comments, on = 'UserId')

return_word_count_segment_wise(merge_df, 1) # Get top 10 words for Gold class 
return_word_count_segment_wise(merge_df, 2) # Get top 10 words for Silver class
return_word_count_segment_wise(merge_df, 3) # Get top 10 words for Bronze class


And just in case you can't merge, you can filter out the another dataframe using this dataframe

to_check = comments[comments['userId'].isin(Badges[Badges['class'] == 1].userId)]
return_word_count_segment_wise(to_check, 3)"
"How to extract specific information from raw , unstructured text using NLP and Deep Learning?","You can create your own named-entity recognition through a pre-trained model like Spacy. https://spacy.io/usage/linguistic-features#section-named-entities

these keywords in your example should be considered as an age entity

 above 21 years
 65 years or less


try to play its linguistic features to get what you need. Hope this helps."
how to deal with large numbers of unlabelled target dataset?,"You can do it the usual way: train on 200 labelled instances, test on the remaining 4800 instances. But actually you should probably keep a labelled test set for evaluating performance first, or use cross-validation on the 200 instances.

However you might have an even more serious problem: it's not clear to me that the English required level can be predicted this way. Either it's explicitly mentioned in the description and then it's just a matter of extracting it, or it's not and there's no way to know it."
How can I encode a 'Name' so that similar names are represented by vectors close in n-dimensional plane?,"You can do that. I propose the simplest one conditioned on the fact that number of data is not very large. In case you need more ideas please drop a comment.

In this case, you can use the idea of similarity encoding based on Fuzzy String Matching and get the spectral embedding. The amount of data is crucial here as you need to do order of 
n
2
ùëõ
2
 comparisons to get affinity matrix for spectral embedding. Follow the code bellow (and get a wonderful idea from this paper)

data = ['sarah connor', 'sara jones', 'jack blabla', 'jackie jones', ' jakob blabla', 'sara conor']
n = len(data)

aff_mat = np.zeros((n,n)) # This is the S matrix in the paper
D = np.zeros((n,n))

for ii in range(n):
    for jj in range(n):
        name1 = data[ii]
        name2 = data[jj]
    
        surname1 = name1.split()[0]
        lastname1 = name1.split()[1]
    
        surname2 = name2.split()[0]
        lastname2 = name2.split()[1]
    
        aff_name1_name2 = fuzz.ratio(surname1,surname2) + fuzz.ratio(lastname1,lastname2)
        # Fuzz ratios are betweein 0 and 100 and we add 2 of them 
        # so we normalize the whole score to 0 and 1 by dividing by 200
        aff_mat[ii,jj] = aff_name1_name2/200
    
for ii in range(n):
    D[ii,ii] = np.sum(aff_mat[:,ii])

L = D - aff_mat # This is Laplacian matrix


Having the Laplacian matrix, you simply calculate the eigenvectors, directly from the code inside the paper. Here I choose second and third eigenvector as the forst eigenvector is trivial. please not that there are tones of ways to calculate Laplacian matrix and what we did here is different that the one in the paper. So despite the paper which chooses first 
k
ùëò
 eigenvectors, we drop the first one. For more details on this you may refer to the literature.

# compute eigenvectors / eigenvalues of L
evals, evcts = eig(L)
# extract ""smallest"" 2 eigenvectors (ignoring first one)
sortedevals = argsort(evals)
U = evcts[:,sortedevals[1:3]]


Now U is your embedding in 2 dimensions. Just plot and see the result:

for (x,y), label in zip(U, data):
    plt.text(x, y, label, ha='center', size=10)
plt.xlim((-1,1))
plt.ylim((-1,1))
plt.show()


and this is the result:

Now it's up to you how you would like to query similar names. The main job is done.

PS: As obvious above, I assumed you are interested in similarity of both first names and last names. In case you want the same code only for first name just simply take the 
lastname
ùëô
ùëé
ùë†
ùë°
ùëõ
ùëé
ùëö
ùëí
 variable out.

Hope it helped. Good Luck!"
Clustering Strings Based on Similar Word Sequences,"You can either use a sentence embedding model to associate a vector to each of your inputs, and use a clustering algorithm like KMeans, or build a similarity matrix between your strings using a string distance metric, and use a similarity-based algorithm like Spectral Clustering or Agglomerative Clustering.

The first one using KMeans might not work the best because the sentence embedding model will have been trained on data that doesn't specifically look like what you have, but it will be able to process new data.

For the second one, because you can use any string distance you want, you can design one that works really well with your data. But because it uses similarity based clustering, you wont be able to process new data as easily."
What is a better solution for text classification than use of perplexity,"You can evaluate pre-built language models. Advantage is :

These models are usually trained on very large text corpora (Such a a snapshot of text from billions of Web pages).
It requires very little development effort for testing models for a given classification problem.

Examples :

https://spacy.io/usage/examples

https://uber.github.io/ludwig/examples/"
Extracting location from text - NOT sensetive to letters (Upper or Lower Case) or already known vocabulary words,"You can first detect the ""out of vocabulary"" words, and check if they are part of a location dataset.

There are locations datasets that you can use and adapt them to be not case sensitive.

Here are the ones for the cities: https://simplemaps.com/data/world-cities

About streets, you can use the world roads dataset and apply the same logic: https://sedac.ciesin.columbia.edu/data/set/groads-global-roads-open-access-v1"
Which kind of model is better for keyword-set classification?,"You can leverage word-vector similarity in embedding models.

TL;DR similiar vectors of words (for example fruits) will be clustered together in this high (vector) dimensional space. For every possible class-set you will have a class-set representative (centroid) that is actually a key (so in your case fruit, vegetable etc) all you need to do is train/find a representative word embedding model of your corpus."
Estimating data set size for grammar extraction,"You can never get 100% coverage for real-world grammar extraction. Grammar is complex and undefined for real-world data. In addition, it is an ""open world"" data problem because novel grammar phrases can always be created.

You might get 100% coverage for a small data set through overfitting.

Statistical learning theory provides a framework for predicting the limits of machine learning."
How to replace words in a sentence with their POS tag generated with SpaCy efficiently?,"You can not replace tags in a sentence because Python strings are immutable.

You can make another string with just the tags:

>>> import spacy
>>> nlp = spacy.load('en_core_web_sm')
>>> doc = nlp(""Apple is looking at buying U.K. startup for $1 billion"")
>>> "" "".join(token.tag_ for token in doc)
'NNP VBZ VBG IN VBG NNP NN IN $ CD CD'


That example is based on the spaCy documentation.

If the tokens are split by several different non-space delimiters (called ""multiple infix tokenization"" in SpaCy), you would have to track that and the code would be more complex."
Extract information from sentence,"You can possibly use a combination of Named Entity Recognition and Syntactical Analysis - while the word Edwin is certainly propping up, imagine a situation where the name is Edward Philip Martel. NER detects each word as a separate entities (hence 3 different entities) - thus, you will anyways have to string them together based on some logic. Further, in the case of multiple names being present, it can get harder to disambiguate (e.g. John & Ramsey dined at Winterfell).

This is where the analysis of the sentence syntax would also help (assuming that the end user enters a relatively coherent and proper sentence - if slang and short forms of text are used, even the Stanford NLP can help upto a certain extent only).

One way of leveraging on syntax analysis / parsing and NER is in the following examples -

 1. User: Edwin is my name.
 2. User: I am Edwin.
 3. User: My name is Edwin.


In each of the cases (as is generically the case as well), the Entity Name (Proper Noun / Noun) is associated in close proximity to a Verb. Hence, if you first parse the sentence to determine verbs and then apply NER to surrounding (+/- 1 or 2) words, you may have a relatively decent way to resolve the problem. This solution would depend primarily on the syntax rules you create to identify NERs as well as the window around the verbs."
BERT Optimization for Production,"you can start by using torchscript, it may require changing ur whole code, and switching to transformers( by loading the backbone of the model and the last layers) so basically u get out from GIL interpreter, coz it does not support multithreading. by with torchscript u can run ur model in c++ env, there's also onnx which I believe it enhances performance.

if ur use case is not a real-time and you are using an API, you can use a queue mechanism like rabbitmq"
Automatic code checking,"You can start with MOSS (Measure Of Software Similarity). It can find similar software documents in a set of software documents. It has several nice properties - whitespace insensitivity, noise suppression, and position independence. It is based on the more general idea of document fingerprinting where a hash is constructed of the document. The document hash values are compared. There is a more technical explanation in the Winnowing paper.

Here is a review paper covering additional techniques, such as Levenshtein String Edit Distance, RTED‚ÄîRobust Algorithm for Tree Edit Distance, and Graph Edit Distance."
How can I discover topics in a social media data-set?,"You can take a look at Latent Dirichlet Allocation. In my experience this does very well without too much effort. You need to remove words that don't help like stopwords (and in your case Twitter handles and probably URLs) before feeding it to the algorithm. The only really important parameter that you need to give it is the number of topics. This will depend on your population (are these random tweets, or only tweets from a specific subgroup/hashtag?) and you need to compare some settings. What you can do is print the most important words per topic and see if they indeed do belong together.

If there are different languages in your tweets you need to deal with that beforehand, maybe classify them on language and only keep the English ones for example."
Why are bigger embedding vectors not necessarily better?,"You can think about phenomenons close to the curse of dimensionality.

Embedding words in a high dimension space requires more data to enforce density and significance of the representation. A good embedding space (when aiming unsupervised semantic learning) is characterized by orthogonal projections of unrelated words and near directions of related ones. For neural models like word2vec, the optimization problem (maximizing the log-likelihood of conditional probabilities of words) might become hard to compute and converge in high dimensional spaces.

You‚Äôll often have to find the right balance between data amount/variety and representation space size."
"In natural language processing, why each feature requires an extra dimension?","You can think of each feature running along its own axis on a graph. Just because we store all feature e.g. in a single DataFrame ‚Äì one feature per column ‚Äì it doesn't mean the data's structure is just 2d (rows and columns). This is not the case only in NLP, but in most contexts involving statistics and modelling.

We can see this with your example data. There are text blocks, which you should match to a category (as far as I can tell).

The initial dataset contains other meta-data, such as a desctiption of the dataset, the names of the target categories and also the location of each sample's file. We don't really care about these for the pure modelling part. So there are only text blocks, called data, and the target categories, called target. Your input is then 1d - the text blocks.

I will show how to put that into a dataframe, being very verbose about dimensions and features:

from sklearn.datasets import fetch_20newsgroups
groups = fetch_20newsgroups()

import pandas as pd                    # needed to use a dataframe

# Get the desired parts from ""groups""
desired = ['data', 'target']           # we don't care about the 'filenames' ona so on

# make a new dictionary with only desired key-value pairs
only_data = {k: v for k, v in groups.items() if k in desired}


Now we put this into a dataframe

df = pd.DataFrame.from_dict(only_data)

# Check the shape of the dataframe
df.shape
(11314, 2)


So there are 11314 samples of 1 feature, to 1 target variable. This is therefore 1-dimensional input data (we don't count the target variable).

When we have e.g. 50 features, explaining some target variable, it may be referred to as a 50-dimensional input space. People then may use dimensionality reduction techniques, such as Principal Components Analysis, which will attempt to squeeze the 50 dimensions into a lesser number (you can choose how many to use).

In your data, you will likely pre-process the text samples to create more features. These will just be new columns in the dataframe, whose shape could become e.g. (11314, 40) if you add another 38 features, by doing things like counting words or constructing some word-embeddings."
How to get the number of syllables in a word?,"You can try another Python library called Pyphen. It's easy to use and supports a lot of languages.

import pyphen
dic = pyphen.Pyphen(lang='en')
print dic.inserted('Rohit')
>>'Ro-hit'"
Semantic network using word2vec,"You can try converting your word representation into a document representation by simply taking the average over the word vectors for a document. For example, if a document has 9 words with (9, 200) dimensions, by taking an average over the words you can have a document representation with a dimension (1,200).

After you have your document-representation, you can use T-SNE to find similar documents. The documents with a similar topic or theme will cluster near each other. You can always improve the document representation by improving your word vectors.

Check this"
Choice of the number of topics (clusters) in textual data,"You can try Perplexity or Coherence Score for the number of topics.

Perplexity is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of k, you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents. - Lower the better.
Coherence Score is defined as the average/median of the pairwise word-similarity scores of the words in the topic - The higher the better.
Perplexity R implementation"
Generate tags for live chat transcripts,"You can try RAKE(Rapid Automatic Keyword Extraction) and there is a python implementation here. RAKE is an document-oriented keyword extraction algorithm and also language-independent(theoretically, since RAKE use a generated stop word list to partition candidate keywords, and considering different languages, we need to find a better way to generated stop word list.). However, about English documents, RAKE can extract keywords(or tags) in a acceptable precision and recall. RAKE is also efficient, because to use it we don't have to training a whole corpus, RAKE can generate a keyword list by calculating the word's degree and frequency then comes up a score for every candidate keyword then pick the top N words.

Hope this answer helps you or lead you a direction for your next step investigation."
Convert cosine similarity to probability,"You can try to normalize the similarity: norm_sim = (sim + 1) / 2, where sim is the cosine.

In this case, 0 means opposite similarity, 0.5 means no relationship between words and 1 means complete similarity.

In my opinion, taking the absolute value of the cosine wouldn't have much sense because a word that has complete similarity with another one gets the same value as its antonym.

source

If you want to assign an higher score to antonyms than to unrelated words, you could try assigning weights to the cosine values < 0 that reduce more the value as it approaches 0.

For example:

def weights(x):
  if x < 0:
   return ((0.75*x)+1)/2
  else:
   return (x+1)/2


I know this may be an awful choice of weights, but i think the idea is valid"
How to capture the detail of an attribute in a sentence?,"You can try using dependency parsing for that, e.g., as implemented in Spacy or any other NLP toolkit.

The details should be adjective modifiers (dependency label amod) of what you call attribute that you detected using a NER-like approach.

Alternatively, they can form compound nouns as ""airy cotton"" in your example. In this case, it is a word that depends on the entity with the dependency label compound. But I guess this will be rarely the case because such compounds will be more often recognized as one entity.

Spacy has an online demo for dependency parsing."
How to prepare texts to BERT/RoBERTa models?,"You can use existing libraries to tokenize.

From the docs on Github:

For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple. Just follow the example code in run_classifier.py and extract_features.py. The basic procedure for sentence-level tasks is:

Instantiate an instance of tokenizer = tokenization.FullTokenizer

Tokenize the raw text with tokens = tokenizer.tokenize(raw_text).

Truncate to the maximum sequence length. (You can use up to 512, but you probably want to use shorter if possible for memory and speed reasons.)

Add the [CLS] and [SEP] tokens in the right place.

In the original paper (Section 3) it is said that:

To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence. Throughout this work, a ‚Äúsentence‚Äù can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A ‚Äúsequence‚Äù refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.

Masked LM (Task 1) and Next Sentence Prediction (NSP, Task 2) are both part of pretraining in the original paper (Section 3.1). For ""classification only"" you may be ""okay"" with Task 1 (MLM) depending on the problem. However, both MLM and NSP seem to be important to achieve ""good"" results. The motivation for NSP is described in the paper in the following words:

Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus

For more technical aspects (i.e. using the transformers library), you may see this discussion on SO."
How do I get ngrams for all combinations of words in a sentence?,"You can use itertools.combinations().

For example:

s = ""I need multiple ngrams""
s_list = s.split("" "") # Assumes you tokenize with white space.

import itertools

combinations = list(itertools.combinations(s_list, 2)) # the second argument (""2"" in this case) is the size of the n-gram.


You will get the following output:

[('I', 'need'), ('I', 'multiple'), ('I', 'ngrams'), ('need', 'multiple'), ('need', 'ngrams'), ('multiple', 'ngrams')]"
How to fetch text from pdf to further proceed with question answer based model from the same document?,"You can use pypdf2 to extract text from pdf.

import PyPDF2

with open('sample.pdf','rb') as pdf_file, open('sample_output.txt', 'w') as text_file:
    read_pdf = PyPDF2.PdfFileReader(pdf_file)
    number_of_pages = read_pdf.getNumPages()
    for page_number in range(number_of_pages):   # use xrange in Py2
        page = read_pdf.getPage(page_number)
        print('Page No - ' + str(1 + read_pdf.getPageNumber(page)))
        page_content = page.extractText()
        text_file.write(page_content)"
Using doccano for Aspect Based Sentiment Analysis annotation,You can use sequence labeling feature to annotate the text:
Same TF-IDF Vectorizer for 2 data inputs,"You can use something like this

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
tfidf_vectorizer = TfidfVectorizer()
raw_data = pd.DataFrame(*raw_data, columns = ['id', 'is_identical', 'q1', 'q2'])
data['tf_idf_q1'] = tfidf_vectorizer.fit_transform(data['q1'])
data['tf_idf_q2'] = tfidf_vectorizer.fit_transform(data['q2'])
data_for_model = data[['tf_idf_q1', 'tf_idf_q2', 'is_identical']]
X = data_for_model[['tf_idf_q1', 'tf_idf_q2']].as_matrix()
Y = data_for_model['is_identical'].as_matrix()
model = Sklearn.LogisticRegression()
model.fit(X, Y)


Combined Model - here you actually learn the transformation for all questions. Then transform each one(question) separately to create features for your model training.

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
tfidf_vectorizer = TfidfVectorizer()
raw_data = pd.DataFrame(*raw_data, columns = ['id', 'is_identical', 'q1', 'q2'])
tf_train_data = pd.concat([data['q1'], data['q2']])
trained_tf_idf_transformer = tfidf_vectorizer.fit_transform(tf_train_data)
data['tf_idf_q1'] = trained_tf_idf_transformer.transform(data['q1'])
data['tf_idf_q2'] = trained_tf_idf_transformer.transform(data['q2'])
data_for_model = data[['tf_idf_q1', 'tf_idf_q2', 'is_identical']]
X = data_for_model[['tf_idf_q1', 'tf_idf_q2']].as_matrix()
Y = data_for_model['is_identica'l].as_matrix()
model = Sklearn.LogisticRegression()
model.fit(X, Y)"
Are there any good NLP APIs for comparing strings in terms of semantic similarity?,You can use the Universal Sentence Encoder from Google and calculate the similarity between texts using the cosine similarity or angular distance between their vector representations.
How should I treat these non-English documents in the NLP task?,"You can use these tips :

Should I exclude them for the corpus and from training the model?

You can do this if you don't have a lack of data. But I think excluding 500 docs from 30K docs won't make a big difference in training. The model's generalisation power won't be compromised.

should I manually translate them (Requesting natives from each language to translate it for me) and include them in the final corpus?

You should do this only when you need the 500 docs as they hold significant information. I would not recommend this method personally.

should I use Google translate/DeepL to translate these non-English documents into English and then include them in the final corpus?

That is something you can do. Using Google Translate could be a help if the structure of the sentences in the docs is simple and sober. You can get good translations without the need of any natives of other countries.

I think you should opt for this method.

Conclusion:

Translate the docs which hold importance using Google Translate.
Else, omit them if you have an extra of 500 docs. They willl not affect the model's performance significantly.

Tip:

I don't think that you can paste the 500 docs in the Google Translate console. This will be time consuming if the docs belong to different languages. Hence, try to omit these documents. Otherwise, you will require a mini application which could use some translation API to transform the docs."
Word analysis in Python,"You can use word embedding in order to compare whole phrases. I am aware about two models: Google's word2vec and Stanford's GloVe. Now, word embedding works best with, well - words. However, you could concatenate every word in your phrase and re-train the models. Afterwards, you could calculate their similarity (say, with cosine similarity) and see how similar your whole phrases are semantically.

Hope this helps."
How should I format input and output for text generation with LSTMs,"You can use word embedding, to encode words as vectors of real numbers. Then all calculations, such as comparison of words (to find similarity), are performed in that high-dimensional space.

""What would be the best way to format my input (and my output) for this problem?""

I cannot tell which is the best approach (depends on your problem) but this one is very common. It's also very easy to implement in Keras, using the Embedding layer."
How to get sentence from embedding vector with Universal Sentence Encoder?,"You cannot run the universal sentence encoder in reverse. There is no practical way to take an arbitrary embedding vector and get a sentence.

My suggestion would instead be to find the sentence in your data with the embedding closest to your center. Euclidean distance works well, specially if you used K-means or another euclidean method to create your clusters."
Vectorize One line text data,You could alternatively use a pretrained embedder like word2vec or glove to vectorize your data into fixed length vectors.
"what machine/deep learning/ nlp techniques are used to classify a given words as name, mobile number, address, email, state, county, city etc","You could apply character grams - Intuitively, there might be a huge difference in character set between a phone number and an email address. and then pass the character gram vector to SVM to make a prediction. You could implement this using in sklearn using the below feature extractors.

TfIdfVectorizer(analyzer='character')

CountVectorizer(analyzer='character')

Cross validate on the ngram range and slack variables of SVM to fine tune your model."
"How to build confusion matrix , when predicted value and actual value is in sentence?","You could create 2 new columns (1 for the truth, 1 for the prediction) which have a more generalised value like PROCESS_RESUMED. You can then use those columns to create the confusion matrix."
Pass 2 different kinds of X training data to ML model simultaneously,"You could just apply two independent vectorization steps on your input X (one vectorizer for description and another for summary) and then concatenate the obtained feature matrices into a single feature matrix.

Doing it this way you will have features such as description_""such"", description_""long"", ..., summary_""bombay"", summary_""1971"", ..., so any model you apply will be able to:

Use all the features from the description and summary altogether
Give a different weight to desctiption tokens and summary tokens"
What is a suitable loss function and evaluation metric for a classification model with large number of unbalanced target classes?,"You could look at sensitivity and specificity. They can be combined effectively to either provide a basis for a correct classification, or the basis for an exclusionary classification within each class. That said, if you are looking for a measure for the model as a whole, you'll probably want to look at some sort of weighted average of the sensitivity scores.

sensitivity is the same calculation as precision, but specificity is not the same as recall.

That said, you may need to have several models that specialize in different areas or, perhaps as worst case scenario, you may need to produce a one-vs-the-rest ensemble of models to get the best results. This might help if, as anymous.asker suggested, perhaps you can introduce some cost around positive or negative outcomes.

HTH"
How can I create a negation of the sentence?,You could look in to Style Transfer. This is an active field of research.
What is the easiest way to identify a gender for a noun (in french)?,"You could try to apply a French POS tagger, e.g. as suggested here: https://stackoverflow.com/questions/44468300/how-to-pos-tag-a-french-sentence

Note that a POS tagger usually works better from full sentences than isolated words."
Best way to search for a similar document given the ngram,"You could use a hashing vectorizer on your documents. The result will be a list of vectors. Then vectorize your ngrams in the same way and calculate the projection of this new vector on the old ones. This is equivalent to the database join on an index, but may have less overhead."
How to find coherence between a large number of sentences,"You could use clustering with a more basic similarity measure, for example cosine or even simply the proportion of words in common (e.g. Jaccard, overlap coefficient). This should gives you groups of sentences which are ""quite similar"" against each other, whereas sentences in different clusters are supposed to be very different. This way you would only have to compute WMD distance between smaller groups of sentences. By increasing the number of clusters the clusters would be smaller so there would be less WMD computation needed, however there is more risk to miss a pair of sentences since they could end up in different clusters."
What's beyond topic modeling?,"You could use doc2vec to create vector representations of each document. Once you have all the vector representations you can use standard unsupervised clustering techniques like k-means, hierarchical clustering, or K-SOM.

The doc2vec model you create will be able to compute cosine similarity between two documents and also find the n most similar documents to a given document and provide a similarity score for each."
Finding the most phonetically similar word from WordNet,You could use e-speak to convert your words into a phonetic alphabet. What you can also do is use fuzzy-matching. Here is a nice blog post about this.
"When classifying documents with naive Bayes, which probabilities do you multiply in case of repeated terms?","You could use either of the two approaches:

Just count whether your documents in your corpus contain or not a word 
w
ùë§
 and then estimate the likelihoods of 
P(
w
1
|C)...P(
w
n
|C)
ùëÉ
(
ùë§
1
|
ùê∂
)
.
.
.
ùëÉ
(
ùë§
ùëõ
|
ùê∂
)
, as you describe

You could count how many instances of any word has each of the documents of your corpus, but then the estimation of the likelihoods becomes a bit more complicated than what you describe; in essence you need to use a multinomial distribution: 
P(
w
1
|C=k)=
‚àë
N
i=1
x
it
z
ik
/
‚àë
|V|
s=1
‚àë
N
i=1
x
is
z
ik
ùëÉ
(
ùë§
1
|
ùê∂
=
ùëò
)
=
‚àë
ùëñ
=
1
ùëÅ
ùë•
ùëñ
ùë°
ùëß
ùëñ
ùëò
/
‚àë
ùë†
=
1
|
ùëâ
|
‚àë
ùëñ
=
1
ùëÅ
ùë•
ùëñ
ùë†
ùëß
ùëñ
ùëò
, where 
N
ùëÅ
 is the number of documents, 
x
it
ùë•
ùëñ
ùë°
 in the numerator is the counts of word 
w
1
ùë§
1
 per document, 
s
ùë†
 in the denominator iterates the number of terms 
|V|
|
ùëâ
|
 in your corpus and 
x
is
ùë•
ùëñ
ùë†
 is the number times each term appears in each document, and finally 
z
ik
ùëß
ùëñ
ùëò
 is 1 if the document belongs to class k, 0 otherwise, in both numerator in the denominator

Hopefully the short explanation makes sense, anyway you have a very complete and useful reference here (the same notation in the formula above)."
How to add a CNN layer on top of BERT?,"You could use HuggingFace's BertModel (transformers) as the base layer for your model and just like how you would build a neural network in Pytorch, you can build on top of it. HuggingFace's other BertModels are built in the same way. For reference you can take a look at their TokenClassification code over here. This hasn't been mentioned in the documentation much and I think it should.

For Tensorflow however, you would have convert the Bert Model into a Keras layer. I haven't really gone through it much or tried it, but I think this blog post does a pretty good job of explaining it."
How to get sentiment score for a word in a given dataset,"You could use Integrated Gradients to see which words have led to a positive/negative sentiment and then aggregate their scores over your whole dataset. Integrated Gradients are an easy and good way to understand neural network inference.

I found an article on how to use integrated gradients for sentiment-analysis."
How to apply TFIDF in structured dataset in Python?,"You could use pandas pivot_table() to transform your data frame into a count matrix, and then apply sklearn TfidfTransformer() to the count matrix in order to obtain the tf-idfs.

import pandas as pd
from sklearn.feature_extraction.text import TfidfTransformer

# input data
df = pd.DataFrame({
    'RepID': [1, 1, 1, 2, 2, 5684, 5684, 5684],
    'Word': ['Doctor', 'diabetes', 'patient', 'patient', 'arm', 'cough', 'Xray', 'Covid'],
    'BOW': [3, 4, 1, 2, 7, 9, 3, 5]
})

# count matrix
df = pd.pivot_table(df, index='RepID', columns='Word', values='BOW', aggfunc='sum')
df = df.fillna(value=0)
print(df)
# Word   Covid  Doctor  Xray  arm  cough  diabetes  patient
# RepID
# 1        0.0     3.0   0.0  0.0    0.0       4.0      1.0
# 2        0.0     0.0   0.0  7.0    0.0       0.0      2.0
# 5684     5.0     0.0   3.0  0.0    9.0       0.0      0.0

# tf-idf transform
X = TfidfTransformer().fit(df.values)
print(X.idf_)
# [1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207]"
How many RNN units are needed for tasks involving sequences?,"You don't need to assign an RNN cell to each entry in a sequence, you just need one.

A single cell can process sequences because of its recursive nature(its output feeds back into itself):

h
t
=f(
h
t‚àí1
,W,
x
t
)
‚Ñé
ùë°
=
ùëì
(
‚Ñé
ùë°
‚àí
1
,
ùëä
,
ùë•
ùë°
)

where 
h
t
‚Ñé
ùë°
 is the hidden state at step 
t
ùë°
, 
W
ùëä
 is the weight used for the cell, and 
x
t
ùë•
ùë°
 is the current input at step 
t
ùë°
 .

Hopefully, you can see how it is only necessary for us one weight matrix, 
W
ùëä
, instead of multiple as your understanding would imply.

Where you got the notion of multiple RNN cells is probably when we feed the hidden state as the input, 
x
ùë•
, into another RNN cell. More layers of RNN cells, mean you can learn a more complex sequence.

Conclusively, your RNN cell cell can't work without new inputs, therefore it end when your sequence ends. Also, it is recommended to use LSTM or GRU over vanilla RNN units."
How to initialize word-embeddings for Out of Vocabulary Word?,"You have a few options here. Of these, I think 1 will be the easiest to implement, as it's a standard language model with an alignment term added to the loss. I'd recommend 2a if you think you have the time, as I imagine its performance might be much better.

1 Use existing corpus to learn embeddings for new words

You can do this as you describe, though I would initialize the unknown embeddings with random noise instead of zeros. This will probably do fine, but it suffers from the problem that the model has no way of knowing that the SENNA embeddings are to be trusted more than the randomly initialized ones. So, it will generally take more (time, examples, etc.) to train this one well.

Another option is to try to capture the difference between known embeddings and randomly initialized ones. My suggestion would be to create your own embeddings [20k x emb_dim] and initialize it however you want. Then add a penalty to your model's loss for deviation from the SENNA embeddings. This is what's done in bilingual embedding models like this one. This will push the known embeddings close to SENNA and allow the unknown ones to freely vary. You could also reduce the coefficient for this alignment penalty as training progresses.

You could either learn these embeddings on a dedicated embedding task (like CBOW) or as part of your primary task.

2 Hybrid model for unknown embeddings

A second approach is to forgo attempting to learn embeddings for your unknown words at all. Typically, mosty word RNN models do this for low-frequency words through something like the <unk> tag. For you, that would be half your vocabulary. Fortunately, there are a lot of tools for filling in these out-of-vocabulary (OOV) words.

2a Subword embeddings

As @Emre pointed out, subword embeddings have been used to great effect to solve out-of-vocabulary problems. Here's an example of a model that uses subword embeddings to achieve an ""open vocabulary"" model. This means you would combine your pre-trained embeddings (which you would not need to make learnable) with a character-level model to learn when and how to produce OOV words.

2b <unk> replacement

Another approach is to, as a post-processing step, replace the <unk> symbols with words from your corpus. You can build your own language model (even a simple markov chain would be an improvement). Also, some modern models have more sophisticated methods for replacement that work quite well."
Named Entity Recognition: NLTK using Regular Expression,"You have a great idea going, and it might work for your specific project. However there are a few considerations you should take into account:

In your first sentence, Obama in incorrectly classified as an organization, instead of a person. This is because the training model used my NLTK probably does not have enough data to recognize Obama as a PERSON. So, one way would be to update this model by training a new model with a lot of labeled training data. Generating labeled training data is one of the most expensive tasks in NLP - because of all the man hours it takes to tag sentences with the correct Part of Speech as well as semantic role.

In sentence 2, there are 2 concepts - ""Former Vice President"", and ""Dick Cheney"". You can use co-reference to identify the relation between the 2 NNPs. Both the NNP are refering to the same entity, and the same entity could be referenced as - ""former vice president"" as well as ""Dick Cheney"". Co-reference is often used to identify the Named entity that pronouns refer to. e.g. ""Dick Cheney is the former vice president of USA. He is a Republican"". Here the pronoun ""he"" refers to ""Dick Cheney"", and it should be identified by a co-reference identification tool."
What is NLP technique to generalize manually created rules in text?,"You have many options depending on the level of complexity and creativity you may have. Among all, I would suggest going through ""Natural Language Understanding"" techniques.

Introduction

Not so precise but practically so to speak, NLP is about processing text and taking each token/word/phrase as an object and learning different models based on appearance of these objects and answering different questions. Here you do not really work with what they mean. Even when you do sentiment analysis (which seems to be a semantic concept) what you really do is counting tokens and seeing their labels in training data and predict the sentiment of the new one.

In NLU, you mainly deal with semantics, meanings and relation between words. For example imagine a Google search query. When I search ""who was the president of US when Italy won the world cup last time?"". If Google wants to rank pages for me based on NLP then many things might come up including these words and their close words (which of course helps me find the answer in related webpages). But if google needs to deliver the precise answer (which is still not easy!), it needs to understand what I say as you see an example here. For this, they use a Knowledge Base (for Google, it's called Knowledge Graph) which is the answer I have to your question.

Suggested Idea

You can make a knowledge base for your corpus if there is enough text. For this, you can extract patterns of desired phrases manually and through this patterns search all occurrences of your desired sentences (e.g. ""|MEDICINE| is used for |DISEASE|"") and all occurrences of your desired objects (e.g. ""Tumor""). Hearst Patterns can be used for such purpose as you see in this paper. Then connecting things though a graph gives you an option for understanding that the similar pattern is happening. For synonyms and related words you can easily use existing Knowledge Bases which are a lot.

Hope it helps :)"
Deep learning techniques for concept similarity?,"You have plenty of existing NLP models already fulfilling this task.

For instance, the bert-base-nli-mean-tokens: 

This model is too general, you will want to use a more adapted one, or build your own model based on an existing one like Bert.

Here is a complete list of models for sentence similarity: https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads

Roberta, Mini-LM, MPnet are generally good options.

However, some of them could require more compute power than other solutions, but with a good GPU, it would be fast enough."
How to train a spacy model for text classification?,"You have several good tutorials on the web :

https://www.kaggle.com/poonaml/text-classification-using-spacy

Basically, you have to :

Import the data in python, here POSITIVE is the variable to predict and 0 and 1 are the 2 encoded classes.
TRAIN_DATA = [(Text1, {'cats': {'POSITIVE': 1}}),
(Text2, {'cats': {'POSITIVE': 0}})]

Initialize a textcat pipe in a spacy pipeline object (nlp), and add the label variable in it.
nlp = spacy.load('en_core_web_sm')
if 'textcat' not in nlp.pipe_names:
  textcat = nlp.create_pipe(""textcat"")
  nlp.add_pipe(textcat, last=True) 
else:
  textcat = nlp.get_pipe(""textcat"")

textcat.add_label('POSITIVE')

Iterate the training examples to optimize the model
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']

n_iter = 1

# Only train the textcat pipe
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    print(""Training model..."")
    for i in range(n_iter):
        losses = {}
        batches = minibatch(train_data, size=compounding(4,32,1.001))
        for batch in batches:
            texts, annotations = zip(*batch)
            nlp.update(texts, annotations, sgd=optimizer,
                      drop=0.2, losses=losses)"
Accuracy is getting worse after text pre processing,"You have to apply the same preprocessing to the test data.

Based on your code you apply the clean_text function only to train data but then predict on test/validation data that was not cleaned. That means that your model learns on clean data but you want it to predict on raw data which contains things the model never seen (because it was removed from the train dataset) which will result in worse performance.

Edit after discussion in comments:

You can either preprocess all data at the same time before splitting and have

all_data = dataset.sample(frac=1).reset_index(drop=True)
all_data['text'] = all_data['text'].apply(clean_text)
train_df, valid = train_test_split(all_data, test_size=0.2)


or just apply the sample preprocessing to the valid dataset after you split the data

all_data = dataset.sample(frac=1).reset_index(drop=True)
train_df, valid = train_test_split(all_data, test_size=0.2)
train_df['text'] = train_df['text'].apply(clean_text)
valid ['text'] = valid ['text'].apply(clean_text)


It is important to apply the same preprocessing for all data that goes into the model so all input is in the same format. This means that when you deploy your model into application you will also have to apply the same preprocessing to any text input coming into the model before making prediction."
Extracting words belonging to a key from the text,"You have two options:

Use regular expressions to extract features of interest based on your dictionary. You may have mixed results, since, for example, dual sim cards will be micro or nano, so you will get two types from single description. However, that just says, that the dictionary isn't well structured.
Train your own NER (named entity recognition) system. For that, first, you will need a data set which will consist of your descriptions and marked entities. This would require manual labour, either yours or paid one one some crowd sourcing platform. When you will have a marked-up dataset, you could use NLTK to train a NER. Markup may be done in any way familiar to you, but, generally speaking, xml-like markup is the best choice since there are many ways to train a NER, and most of frameworks know how to parse xml-like tags:

Example markup:

<screen_size>5.5""</screen_size> (13.97cm) full hd <resolution>1920 x 1080</resolution> pixels ips screen gorilla glass display etc."
BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence,"You just stumble over one big problem in the NLP field : finding the perfect metric..

Most traditional metrics (BLEU, ROUGE, ...) simply does not take into account the distance in terms of semantics between barking and crying.

So according to these metrics, The dog is crying is as similar as The dog is salmon to the reference, the dog is barking.
From a human viewpoint, this is not correct, the first sentence is closer to the reference, because for example the second sentence makes no sense.

People recently tried to provide better metrics in this sense. You might be interested in BERT score.

The idea is simply to use a BERT model (that have been pretrained, therefore have some linguistic knowledge) to compute how similar 2 sentences are."
Obtaining frequently occurring phrases using Word2Vec,"You may use gensim phrase vectorizer module available in Python.

You need to give threshold value which is some sort of pmi of words. The higher this value less are the number of phrases the default is 10. You can play around with this value to get results for your data.

phrase_threshold = 1

bigram = Phrases(sentences,threshold=phrase_threshold)

This is based on this based on the skipgram paper by Tomas Mikolov."
Different methods for clustering skills in text,"You may want to consider preprocessing - convert different wording for the same type of talent to same wording. For example, a talent in machine learning is called data scientist at Coursera job site, data engineer at Udacity job site, or data analyst. This preprocessing is similar to stemming in concept."
Creating statements from a raw text,"You may want to read about the task of Relation Extraction. The most common scenario is to extract triplets (entity1, relation, entity2). Usually entity1 and entity2 are given and the list of possible relations is known. Common methods use a classifier, others can be rule-based.

With your example from the sentence ""Bob is married to Lisa since 1983."" the system can extract (Bob, is_married, Lisa)

A variant is Open Information Extraction which tries to extract relations but with no information specified.

There exist multiple projects on the subject such as DeepDive, IEPY or openie-standalone.

This this presentation or this review give an overview of the task."
Reversing a dependency tree into the original sentence,"You may want to use a nltk.Tree.

from nltk import Tree

lisp = """"""(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (ADVP (RB also))
    (VP (VBZ likes)
      (S
        (VP (VBG eating)
          (NP (NN sausage)))))))""""""

t = Tree.fromstring(lisp)

print(' '.join(t.leaves()))


Beware that your Lisp string needs two additional ) at the end. Punctuation and spacing can be fixed with Sacremoses."
How to use two different datasets as train and test sets?,"You may want to use a pipeline to do this operation. Specifically, you do NOT want to train the TFIDFVectorizer the entire corpus- doing so gives your model hints about what features may be in the test set that don't exist in the training set- a concept frequently referred to as ""leakage"" or ""data snooping"".

The correct pattern is:

transf = transf.fit(X_train)
X_train = transf.transform(X_train)
X_test = transf.transform(X_test)


Using a pipeline, you would fuse the TFIDFVectorizer with your model into a single object that does the transformation and prediction in a single step. It's easier to maintain a solid methodology within that pattern.

In the example code, you're both fitting and transforming in the same step fit_transform, which is creating different features each time and is the source of your error."
NLP: How to group sub-field into fields?,"You mentioned:

My understanding is that standard NLP techniques may not fit here, because the relationship between sub-fields and fields are linked by its meanings but not word-frequency or word-embedding etc.

However, your understanding is not totally correct, because word embeddings do convey meaning in them and could be used in your case.

Here is an example, given a list of countries, you can figure out their capitals in the vector space. Even though they are linked by the geographical location.

You would for example be able to do the following: Rome - Italy + France and you would get Paris.

So, you could create your own word-embeddings where Physics - Quantum Mechanics + Abstract Algebra = Mathematics. The only things you would need is a seed relationship (e.g. Quantum Mechanics - Physics), then all the other relationships would be a simple displacement in the vector space, which you can figure out by subtracting and adding the words."
Bidirectional Encoder Representations from Transformers in R,"You might be interested in the open-source R package RBERT: https://github.com/jonathanbratt/RBERT

It's a work in progress, but the goal is to be able to use BERT directly in R."
Word2Vec how to choose the embedding size parameter,"You might find this paper might be the closest thing to what you are looking for if you don't want to treat it as a regular hyperparameter: Towards Lower Bounds on Number of Dimensions for Word Embeddings

The paper claims that there is a lower bound on the embedding based on the corpus. It also purposes a method for finding said lower bound which I will leave the paper to explain since I think I will not do it justice. Here is the most relevant section of the conclusion of the paper:

We discussed the importance of deciding the number of dimensions for word embedding training by looking at the corpus. We motivated the idea using abstract examples and gave an algorithm for finding the lower bound. Our experiments showed that performance of word embeddings is poor, until the lower bound is reached. Thereafter, it stabilizes. Therefore, such bounds should be used to decide the number of dimensions, instead of trial and error.

It has sourced and cited previous work regarding embedding dimension which also might be of interest to you. Unfortunately the conclusion seems to be the following:

As is evident from the above discussion, the analysis of the number of dimensions have not received enough attention. This paper is a contribution towards that direction."
sentence vector to sentence,"You might need an encoder-decoder framework that can be implemented using two LSTM/GRU models. The Encoder takes the first phrase in, outputs the last (summary) hidden layer, it is taken by the first Decoder state, alongside BOS token, outputs the second token, uses it to predict the third, and so on until you reach the end of the (response) phrase."
NLTK: Tuning LinearSVC classifier accuracy? - Looking for better approaches/advices,"You might try looking into sentiment analysis. There was a kaggle competition on it, and you might find insight there.

Treating this as either a regression or a classification problem is fair. Also, it's important to judge your performance against the proper baselines.

Your feature space might not be rich enough for the classes to be linearly separable. You might do better using an SVM with a non-linear kernel.

It also appears you haven't scaled the counts of the bigrams, which is generally helpful for SVMs.

Another thought for an approach would be to apply LDA to the set of documents (reviews) and use the topics as your feature space (you'll have a topic vector per document).

Some places to get python LDA implementations:

gensim

Blei"
Creating an easy but not trivial dataset,"You might use a Markov chain for generating a simple enough dataset. You can estimate the probabilities of punctuation by scanning a large corpus of text (e.g. wikipedia), then you can use the model to generare fictious sentences with punctuation marks. You can set the order of the Markov chain so as to make the generated dataset increasingly closer to real-world text. The Markov chain should be defined on the set of words, however, so you have to carefully choose the vocabulary."
User profiling based on multiple posts,"You might want to have a look in XLNet.

In XLNet you can input a sentence, then take the state and inject it into a new run if the XLNet with a second input sentence, inject it into a third one with a third sentence and so on.

This, in essence, allows you to process text of virtually unlimited length."
Which method of NLP is this?,"You might want to look at Multi-Label-Classification . If you have sufficient number of samples as your training data, you can build a model that can predict more than one label for a test sample. You can find more about the implementation at the sklearn page for the same here ."
Predicting topics for customer reviews based on topics mapped to n-grams?,"You might want to look at the Facebook starspace library - their examples are similar but with suggesting Facebook pages and hashtags.

https://github.com/facebookresearch/StarSpace"
Word2Vec - document similarity,"you might want to look into 'approximate nearest neighbors' analysis, and particularly the annoy package from Spotify:

https://github.com/spotify/annoy

It's not as precise as an exact comparison, but it may be close enough to get you what you need."
Improve the speed of t-sne implementation in python for huge data,"You must look at this Multicore implementation of t-SNE.

I actually tried it and can vouch for its superior performance."
Can I use a pre-trained model for sentiment analysis/text classification of unlabelled data?,"You need at least a few labelled vaccine tweets (positive, neutral, negative) to train a BERT model so that it starts to understand the domain.

For VADER you don't need any labelled data.

However, when we compare the accuracies, the BERT model always performs better."
Which deep learning network would be suitable for classifying this kind of text data?,"You need character embeddings. I assume you are already familiar with word2vec technology. Its goal it to make a model ""learn"" the relative meaning of words, placing them into a highly dimensional space.

The same can be done with single characters, instead of whole words. The preprocessing steps you need will be a little bit different, but the embedding technique is the same. In that way, you can generate representations of characters, feed their sequences into some RNN model, and perform the final classification task.

Therefore, RNNs are perfectly suitable for this task. If you are working with tensorflow.keras you can simply tokenize characters, and feed them through an Embedding() layer that will do the job for you. An alternative to RNNs is 1D conv layers, that can do the job as an alternative to recurrent cells. That's up to your preference."
How to use unigram and bigram as an feature on SVM or logistic regression [closed],"You need to create a vocabulary of the n-grams, i.e., a numbered inventory of bigrams that you are going to use as features. Typically, these are the most frequent ones. When you create the feature vector, you start with a zero vector and put one (or add one) if the n-gram with the corresponding index appears is in your sentence.

Machine learning libraries typically have functions that do that. For instance, in scikit-learn, you can use CountVectorizer to do the job. The fit method has an ngram_range argument that controls the length of n-grams that are considered in the feature vectors."
Words as features of a neural networks,"You need to make a dictionary of words. It means you have to make a dictionary which you assign each word a unique value. then you can use one-hot-encoding to represent each word uniquely. If this is what you need it will do what you want. But this has a big problem. When you think about cats and dogs, you may find similarities and differences between them. This is because you have more knowledge than the only representation of words in your brain. Consequently, you should use approaches to assign a unique number to each word, and put near concepts as neighbors. For the first part take a look at here and the second part, take a look at here."
Data Extraction from images using NLP and ML [closed],"You need to perform OCR (Optical Character Recognition). However, this can lead to bad results if your images are of very bad quality, or they are skewed and distorted). One tool you can use is tesseract tesseract from Google. You can train it for your purpose to recognize the text in your images."
Training an AI to play Starcraft 2 with superhuman level of performance?,"You pose an interesting question. The problem that I see is that even if you develop all the items you mentioned (and I'm not sure you need all of them) you wouldn't have the computing power needed for this.

You see, most of these items are based on reinforcement learning. Which means that the models are given a relatively small set of data (roughly speaking, the rules of the game) and then are set off to play millions & millions of games. Read the whitepaper that was written by Google on beating Go. They basically admit that there whole point was to write something very, very simple that could learn on it's own and then just set it free on Google's massive power. It's nothing for them to say, ""let's have this robot play 10 million games"". So the level of sophistication from your initial model is pretty low, you just need to make it an exceptional learner.

So you make an exceptional learner of Starcraft - then what? How are you going to have the power to make your algorithm to play millions of games?"
Should I use regex or machine learning?,"You receive an email from a friend that says, ""let's have lunch next Tuesday"" and your email program detects it and asks if you want to save a new calendar entry for ""lunch on Tuesday"".

What you describe is called Information extraction and is a big field of NLP (Natural Language Processing). You are looking for temporal expression identification. You can have a look at the Stanford Temporal Tagger: SUTime to get a ""live"" demo. From what I see here it is a regex-based rule system.

To give you an impression how powerful rule-based systems can be:

Weizenbaum‚Äôs own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing, ‚ÄúI had not realized‚Ä¶ that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.‚Äù

Source: Wikipedia: Eliza

See also
Apple Data Detectors
TimeML: Robust specification of event and temporal expressions in text"
How to classify named entities of the same type?,"You should apply the same plan that you used to extract times to categorize those times:

Start with a rule-based system
Then try a machine learning approach

In order to build a machine learning, you'll need a collection of text labeled as ""start-time"", ""end-time"", or ""neither"". You can first try traditional algorithms like logistic regression or naive bayes. Given this a relatively nuanced problem since you doing conditional classification, you might have be to build a more complex system that uses contextual information like a conditional random field (CRF)."
Should I create single feature for each specific word which i find in text or one for all them?,You should be creating binary features for each group of words. So if you have n groups you should create n-1 features. If you just create one feature it will have 1s for all rows where any word is found and 0 otherwise which will not make sense.
Transform single-label data set into multi-label data set,"You should consider using a neural network for this. By using binary crossentropy across multiple categories, you can get a probability ""rating"" for each category and how it applies to the text. From there, you can develop a script that establishes a threshold (say 0.8) and then creates a new entry of labeled data for that one particular piece of text across multiple categories. There are many examples out there of people taking IMDB data and the movie descriptions and assigning multiple genres to a single movie (like a ""horror"" movie can also be a ""suspense"" movie or a ""comedy"" movie can also be an ""animated"" movie). Those types of examples should fit what you need here."
How to feed data for ngram model?,"You should definitely use a sliding window.

An n-gram language model represents the probabilities for all the n-grams. If it doesn't see a particular n-gram in the training data, for example ""sliding cat"", it will assume that this n-gram has probability zero (actually zero probabilities are usually replaced with very low probability by smoothing, in order to account for out-of-vocabulary n-grams). This would result in a zero probability for a sentence which was actually in the training case (or a very low probability with smoothing).

Also it's common to use ""padding"" at the beginning and end of every sentence, like this:

#SENT_START# The
The sliding
sliding cat
cat is
is not
...
to dance
dance #SENT_END#


This gives the model indications about the words more likely to be at the beginning or end (it also balances the number of n-grams by word in a sentence: exactly 
n
ùëõ
 even for the first/last word)."
"In Text Classification if I get similar performance with 100 features and 200 features, which model should I go ahead with?","You should go with the simpler model, the one that needs fewer features. Fewer features means quicker training cycles, better interpretibility and a faster forward pass. All of these are important considerations if you would like to productionize your model."
Using LSTM to clear up corrupted text files,"You should use a Seq2Seq (which uses LSTM/GRU/RNNs) architecture for the task of cleaning text. The encoder network would take in the ""noisy"" sequence and the decoder would generate the ""cleaned"" sequence.

Seq2Seq models is often used for Neural Machine Translation (NMT).

The next thing is you will need a very large data set. Fortunately this shouldn't be too hard to generate noise injections (of real words, symbols, fake words, letter omissions etc) into sequences of text.

A good guide for a similar task is in the following post. The implementation uses Keras and has all of the code. https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8"
Testing Spacy NER model,"You should use the evaluate command to evaluate the test set. It would look like this:

spacy evaluate ./my-model ./test-data.spacy"
Which machine (or deep) learning methods could suit my text classification problem?,"You state that some of the words may occasionally be truncated or concatenated. Thus I would extract n-grams from your Strings and then use that into a bag-of-words vector.

How does this work?

n-grams is a feature extraction technique for language based data. It segments the Strings such that roots of words can be found, ignoring verb endings, pluralities etc...

The segmentation works as follows:

The String: Hello World

2-gram: ""He"", ""el"", ""ll"", ""lo"", ""o "", "" W"", ""Wo"", ""or"", ""rl"", ""ld"" 3-gram: ""Hel"", ""ell"", ""llo"", ""lo "", ""o W"", "" Wo"", ""Wor"", ""orl"", ""rld"" 4-gram: ""Hell"", ""ello"", ""llo "", ""lo W"", ""o Wo"", "" Wor"", ""Worl"", ""orld""

Thus in your example, if we use 4-grams, truncations of the word Hello would appear to be the same. And this similarity would be captured by your features.

Bag-of-Words builds a dictionary of the words it has seen during the training phase. Then using the word the frequency of each word in the example a vector is created. This can then be used with any standard machine learning technique.

Due to the high number of grams that will result, you will want to do some feature dimensionality reduction. You can use techniques such as PCA and LDA to determine which features (grams) are most pertinent to your decision boundary."
Text summarization with limited number of words,"You sure can,

for example in latent semantic analysis you can fixate number of topics (which is actually size of the decomposition matrix) beforehand."
Knowing Feature Importance from Sparse Matrix,"You would have a map of your features from the TFIDF map.

column_names_from_text_features = vectorizer.vocabulary_
rev_dictionary = {v:k for k,v in vectorizer.vocabulary_.items()}
column_names_from_text_features = [v for k,v in rev_dictionary.items()]


Since you know the column names of your other features, the entire list of features you pass to XGBoost (after the scipy.hstack) could be

all_columns = column_names_from_text_features + other columns


(or depending on the order in which you horizontally stacked)

Now, once you run the XGBoost Model, you can use the plot_importance function for feature importance. Your code would look something like this:

from xgboost import XGBClassifier, plot_importance
fig, ax = plt.subplots(figsize=(15, 8))
plot_importance(<xgb-classifier>, max_num_features = 15, xlabel='F-score', ylabel='Features', ax=ax)
plt.show()


These features would be labeled fxxx, fyyy etc where xxx and yyy are the indices of the features passed to xgboost.

Using the all_columns constructed in the first part, you could map the features to in indices in the plot encoding."
BERT : text classification and feature extractionn,"You'd need to apply a tagger, either a generic NE tagger or a custom-trained one. The tagger works with each token as an instance, so that you can extract a particular sequence of tokens, e.g.:

$15.00     Begin_Billing_rate
hour       Billing_rate
customer   _
service    _
,          _
open       _
to         _
industries _


Of course in order to train a custom tagger you will also have to annotate your data token by token."
Can you use two different datasets as train and test sets with countVectorizer and test_train_split?,"You're doing a big mistake in your code, which is applying the vectoriser before the train/test splitting. The vectoriser should be fit only on the training dataset, then the learned counts should be applied to the test set. Instead you applied the vectoriser to the whole data which you then splitter into train and test.

# THIS IS OK
# train_data = np.concatenate((train_data, validation_data))
X = cv.fit_transform(train_data_text).toarray()
y = pd.get_dummies(train_data_labels)
y = y.iloc[:, 1].values

# NOT OK 
# Train Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

# CORRECT WAY
X_train = X  # already good as it is
y_train = y  # also good

# I suggest to use pandas to read the txt files
X_test = cv.transform(test_data['text']) # <-- APPLY VECTORIZER TO TEST DATA USING TRANSFORM ONLY


Then you can procede as you did in the rest of your code."
Improve CoreNLP POS tagger and NER tagger?,Your best best is to train your own models on the kind of data you're going to be working with.
"Word2Vec, softmax function","Your definition is correct. For the reference you can compare it with the probabilistic model from Tensorflow ""Vector Representations of Words"" tutorial:

P(
w
t
|h)
=softmax(score(
w
t
,h))
=
exp{score(
w
t
,h)}
‚àë
Word w' in Vocab
exp{score(
w
‚Ä≤
,h)}
ùëÉ
(
ùë§
ùë°
|
‚Ñé
)
	
=
softmax
(
score
(
ùë§
ùë°
,
‚Ñé
)
)


	
=
exp
‚Å°
{
score
(
ùë§
ùë°
,
‚Ñé
)
}
‚àë
Word w' in Vocab
exp
‚Å°
{
score
(
ùë§
‚Ä≤
,
‚Ñé
)
}

It's the same as yours, but they generalize the condition 
h
‚Ñé
 as a history.

In case of skip-gram, the target word 
w
t
ùë§
ùë°
 is any context word (
w
‚àít
ùë§
‚àí
ùë°
 in your notation) and the history is the center word (
w
t
ùë§
ùë°
 in your notation). You specify 
Œ∏
ùúÉ
 explicitly in the condition, but that is usually omitted, because there is only one vocabulary in a given problem."
Best way to combine two similar document,"Your description could corresponds to the NLP task of summarization. This is an active field of research: https://scholar.google.com/scholar?q=text+summarization

A much simpler option is to only extract sentences from both: in this case the goal is not to produce a text which reads like a story, just an enumeration of sentences.

Even in this case you will have to define how to measure ""information value"", this is not easy afaik."
Understanding of naive bayes: computing the conditional probabilities,"Your formula is correct for one 
w
i
ùë§
ùëñ
, but if you want to classify a document, you need to compute 
P(c|
w
1
,‚Ä¶,
w
N
)
ùëÉ
(
ùëê
|
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
)
.

Then you have
P(c|
w
1
,‚Ä¶,
w
N
)=
P(c)‚ãÖP(
w
1
,‚Ä¶,
w
N
|c)
P(
w
1
,‚Ä¶,
w
N
)
=
P(c)‚ãÖ
‚àè
N
i=1
P(
w
i
|c)
P(
w
1
,‚Ä¶,
w
N
)
‚â†
‚àè
i=1
N
P(c|
w
i
)
ùëÉ
(
ùëê
|
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
)
=
ùëÉ
(
ùëê
)
‚ãÖ
ùëÉ
(
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
|
ùëê
)
ùëÉ
(
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
)
=
ùëÉ
(
ùëê
)
‚ãÖ
‚àè
ùëñ
=
1
ùëÅ
ùëÉ
(
ùë§
ùëñ
|
ùëê
)
ùëÉ
(
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
)
‚â†
‚àè
ùëñ
=
1
ùëÅ
ùëÉ
(
ùëê
|
ùë§
ùëñ
)

where the second equation holds because of the na√Øve Bayes assumption.

For classification purposes you can ignore 
P(
w
1
,‚Ä¶,
w
N
)
ùëÉ
(
ùë§
1
,
‚Ä¶
,
ùë§
ùëÅ
)
 because it is constant (given the data). The formula is still simple (""na√Øve"") but doesn't simplify quite as much.

The last part of the equation looks a bit suspicious to me as it seems way too simple to compute for a rather complex probability.

Keep in mind that while Na√Øve Bayes is a decent classifier for many applications, the generated probabilities are usually not very representative."
Complex Chunking with NLTK,"your grammar is correct!

grammar = """"""MEDIA: {<DT>?<JJ>*<NN.*>+}
           RELATION: {<V.*>}
                     {<DT>?<JJ>*<NN.*>+}
           ENTITY: {<NN.*>}""""""


by specifying

RELATION: {<V.*>}
          {<DT>?<JJ>*<NN.*>+}


you are indicating that there are two ways to generate the RELATION chunk i.e. {<V.*>} or {<DT>?<JJ>*<NN.*>+}

so

grammar = """"""MEDIA: {<DT>?<JJ>*<NN.*>+}
               RELATION: {<V.*>}
                         {<DT>?<JJ>*<NN.*>+}
               ENTITY: {<NN.*>}""""""
    chunkParser = nltk.RegexpParser(grammar)
    tagged = nltk.pos_tag(nltk.word_tokenize(""adventure movies between 2000 and 2015 featuring performances by daniel craig""))

    tree = chunkParser.parse(tagged)

    for subtree in tree.subtrees():
        if subtree.label() == ""RELATION"": 
            print(""RELATION: ""+str(subtree.leaves()))


gives

RELATION: [('featuring', 'VBG')]"
Overfitting with text classification using Transformers,"Your model is overfitting. You should try standard methods people use to prevent overfitting:

Larger dropout (up to 0.5), in low-resource setups word dropout (i.e., randomly masking input tokens) also sometimes help (0.1-0.3 might be reasonable values).
If you have many input classes, label smoothing can help.
You can try a smaller model dimension.

If you use a pre-trained Transformer (such as BERT), you, of course, cannot change the model dimension. In that case, you can try to set a much smaller learning rate for fine-tuning BERT than you use for training the actual classifier."
Best approach for text classification of phrases with little syntactic difference,"Your problem as you said is a high level of syntax overlapping between your sentences. take a look at these two sentences: Work to live versus live to work. The earlier that you can allow yourself to enjoy other things in life, aside from your job while the latter means obtaining resources so that you can be a functional member of society, and to permit yourself a good lifestyle. They are very different semantically. So when you vectorizing those sentences with techniques like Bag of words or Cosin similarity will be useless as both sentences contain the same corpus. The other problem you are dealing with (based on examples you provided) is dealing with the short text which makes it difficult to be vectorized by other simple but efficient techniques like TF-IDF. so regardless of what classification you are going to use, the performance of the classification model won't be high and that's because the input to the model is not correct.

On the other hand, deep learning methods like RNN or Transformers which solve sequence-to-sequence tasks like yours with ease can be very helpful. Named Entity Recognition models are what you need and given that your data is very domain-specific, you need to train your own model using your data. I recommend the Spacy Python package. So once you have your model, you will have two entities, CHANGE TALK, and NON-CHANGE TALK. Then you can simply count how many of them you have in your paragraph. Of course, that's the simplest way of dealing with your problem. You can add more entities and then they will act as features by which you can train any classification models. Hope this helps."
Sentence similarity prediction,"Your problem can be solved with Word2vec as well as Doc2vec. Doc2vec would give better results because it takes sentences into account while training the model.

Doc2vec solution
You can train your doc2vec model following this link. You may want to perform some pre-processing steps like removing all stop words (words like ""the"", ""an"", etc. that don't add much meaning to the sentence). Once you trained your model, you can find the similar sentences using following code.

import gensim  

model = gensim.models.Doc2Vec.load('saved_doc2vec_model')  

new_sentence = ""I opened a new mailbox"".split("" "")  
model.docvecs.most_similar(positive=[model.infer_vector(new_sentence)],topn=5)


Results:

[('TRAIN_29670', 0.6352514028549194),
 ('TRAIN_678', 0.6344441771507263),
 ('TRAIN_12792', 0.6202734708786011),
 ('TRAIN_12062', 0.6163255572319031),
 ('TRAIN_9710', 0.6056315898895264)]


The above results are list of tuples for (label,cosine_similarity_score). You can map outputs to sentences by doing train[29670].

Please note that the above approach will only give good results if your doc2vec model contains embeddings for words found in the new sentence. If you try to get similarity for some gibberish sentence like sdsf sdf f sdf sdfsdffg, it will give you few results, but those might not be the actual similar sentences as your trained model may haven't seen these gibberish words while training the model. So try to train your model on as many sentences as possible to incorporate as many words for better results.

Word2vec Solution
If you are using word2vec, you need to calculate the average vector for all words in every sentence and use cosine similarity between vectors.

def avg_sentence_vector(words, model, num_features, index2word_set):
    #function to average all words vectors in a given paragraph
    featureVec = np.zeros((num_features,), dtype=""float32"")
    nwords = 0

    for word in words:
        if word in index2word_set:
            nwords = nwords+1
            featureVec = np.add(featureVec, model[word])

    if nwords>0:
        featureVec = np.divide(featureVec, nwords)
    return featureVec


Calculate Similarity

from sklearn.metrics.pairwise import cosine_similarity

#get average vector for sentence 1
sentence_1 = ""this is sentence number one""
sentence_1_avg_vector = avg_sentence_vector(sentence_1.split(), model=word2vec_model, num_features=100)

#get average vector for sentence 2
sentence_2 = ""this is sentence number two""
sentence_2_avg_vector = avg_sentence_vector(sentence_2.split(), model=word2vec_model, num_features=100)

sen1_sen2_similarity =  cosine_similarity(sentence_1_avg_vector,sentence_2_avg_vector)"
Compare Books using book categories list NLP,"Your problem could be framed as multi-label classification, each instance can have multiple labels. For a given book, predict which labels are likely.

In Python, there scikit-multilearn is designed for the multi-label classification problem.

Additionally, you may want to consolidate labels that are similar (e.g., 'story' and 'stories'). The consolidation can be done with find and replace."
Classifying one particular class of documents from the rest,"Your problem is indeed a typical one-class classification problem, and as far as I know one-class SVM is usually a good option for that.

I think you should investigate what causes the poor performance:

Evaluating with accuracy is probably not informative enough, you would need to find out at least whether the errors tend to be mostly false positive or false negatives, thus using precision/recall.
You could look at what is happening at the level of features: I would expect some words specific to sports to be assigned a strong weight by SVM for instance. It could also be a problem with the dimensionality being too high, maybe you need to remove stop words or filter out rare words, etc."
How to train a Text Based data for a Machine Learning problem?,"Your problem looks more like a ranking problem than a classification problem to me. Have you tried a more naive method, like a 1-NN ""classifier"" with unigram text representation, Tf-Idf term weighting and a cosine similarity metric? It's far from the state of the art but it tends to give rather good results in retrieval and recommendation tasks."
Below text-classification model gives accuracy of 0.77 only on one dataset and 0.99 on spam-ham dataset? What should I do to increase with my dataset?,"Your question can be given a general answer along the lines of the comment from @Erwan . Scoring highly on a chosen metric (and as he mentioned accuracy is only one type and possibly not the best one) is affected by a myriad of considerations. Trying to compare the results of a metric on completely distinct types of problems does not have any meaning

The datasets are different
The objectives to be measured are different

There should be no expectation of results on the one hand having any carry-over meaning to the other. Even if you had 99% precisely on both datasets it would not indicate that the ham-spam results were indicative of likely statistical strength on the second problem."
Approaching a multi-class classification problem but without labels,"Your specific problem can be solved by Googling.

Here is a solution that

Searches the ""imdb [year] [movie name]"" in Google,
Finds its IMDb address and fetches the IMDb page, and then
Searches for the genres inside the IMDb page.

I have changed ""romantic"" to ""romance"", and ""emotional"" to ""drama"" to match the IMDb vocabulary.

from requests import get
import re

titles=[""2013+Conjuring"", ""2004+The+notebook""]
genres = ['horror', 'thriller', 'comedy', 'romance', 'drama']
matched_genres = {}
for title in titles:
  query = ""https://www.google.com/search?q=imdb+"" + title
  print(query)
  search_result = get(query).text.lower()
  imdb_id = re.findall(""https://www.imdb.com/title/(tt\d+)/"", search_result)[0]
  imdb_address = ""https://www.imdb.com/title/%s/"" % imdb_id
  print(imdb_address)
  imdb_result = get(imdb_address).text.lower()
  matched_genres[title] = []
  for genre in genres:
    # find "">genre<"" inside tags
    if imdb_result.find("">%s<"" % genre) > -1:  
      matched_genres[title].append(genre)

print(matched_genres)


Output

https://www.google.com/search?q=imdb+2013+Conjuring
https://www.imdb.com/title/tt1457767/
https://www.google.com/search?q=imdb+2004+The+notebook
https://www.imdb.com/title/tt0332280/
{'2013+Conjuring': ['horror', 'thriller'], '2004+The+notebook': ['romance', 'drama']}


This solution could be improved by

Querying movie titles in parallel,
Directly querying IMDb API,
Handling edge cases (for example when the first IMDb url is irrelevant or no IMDb page is found, etc.),

etc."
N-gram language model for preposition predition,"Your understanding is mostly correct, but don't forget that you can not only take into account the previous tokens, you can also consider:

p(this word | next words)
p(this word | N previous word,N next words)
etc.

Often a combination of these probabilities offers optimal results.

how is this machine learning I wonder?

Well, ML is very often a deterministic calculation, it doesn't have to be a complex approximation problem: decision trees, Naive Bayes, linear regression...

do I simply only compare the posterior probabilities of the few known prepositions and find the argmax as the prediction?

Yes, that would be the idea. Of course if you use multiple models as suggested above there must be some kind of combination additionally."
Variable input/output length for Transformer,"Your understanding is not correct: in the encoder-decoder attention, the Keys and Values come from the encoder (i.e. source sequence length) while the Query comes from the decoder itself (i.e. target sequence length).

The Query is what determines the output sequence length, therefore we obtain a sequence of the correct length (i.e. target sequence length).

In order to understand how the attention block works maybe this analogy helps: think of the attention block as a Python dictionary, e.g.

keys =   ['a', 'b', 'c']
values = [2, 7, 1]
attention = {keys[0]: values[0], keys[1]: values[1], keys[2]: values[2]}
queries = ['c', 'a']
result = [attention[queries[0]], attention[queries[1]]]


In the code above, result should have value [1, 2].

The attention from the transformer works in a similar way, but instead of having hard matches, it has soft maches: it gives you a combination of the values weighting them according to how similar their associated key is to the query.

While the number of values and keys has to match, the number of queries is independent."